text
 Could an artificial general intelligence be created? If so, how?An artificial general intelligence  AGI  is a type of hypothetical intelligent agent. The AGI concept is that it can learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies. The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern deep learning systems, such as GPT 4, are an early yet incomplete form of AGI or if new approaches are required. Contention exists over the potential for AGI to pose a threat to humanity  for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk. A 2020 survey identified 72 active AGI R D projects spread across 37 countries. AGI is also known as strong AI, full AI, or general intelligent action. However, some academic sources reserve the term  strong AI  for computer programs that experience sentience or consciousness.  In contrast, weak AI  or narrow AI  is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use  weak AI  to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans. Related concepts include human level AI, transformative AI, and superintelligence. Various criteria for intelligence have been proposed  most famously the Turing test  but no definition is broadly accepted. However, researchers generally hold that intelligence is required to do the following  and, if necessary, integrate these skills in completion of any given goal.  Many interdisciplinary approaches  e.g. cognitive science, computational intelligence, and decision making  consider additional traits such as imagination  the ability to form novel mental images and concepts  and autonomy. Computer based systems that exhibit many of these capabilities exist  e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent . However, no consensus holds that modern AI systems possess them to an adequate degree. Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include  This includes the ability to detect and respond to hazard. A mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises  the ability to satisfy goals in a wide range of environments . This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human like behaviour, is also called universal artificial intelligence. In 2015 Jan Lieke and Marcus Hutter showed that Legg Hutter intelligence    an agent s ability to achieve goals in a wide range of environments    is measured with respect to  a fixed Universal Turing Machine UTM . AIXI is the most intelligent policy if it uses the same UTM , a result which  undermines all existing optimality properties for AIXI . This problem stems from AIXI s use of compression as a proxy for intelligence, which is only valid if cognition takes place in isolation from the environment in which goals are pursued. This formalises a philosophical position known as Mind body dualism. Some find enactivism more plausible the notion that cognition takes place within the same environment in which goals are pursued. Subsequently, Michael Timothy Bennett formalised enactive cognition and identified an alternative proxy for intelligence called  weakness . The accompanying experiments  comparing weakness and compression  and mathematical proofs showed that maximising weakness results in the optimal  ability to complete a wide range of tasks  or equivalently  ability to generalise   thus maximising intelligence by either definition . If enactivism holds and Mind body dualism does not, then compression is not necessary or sufficient for intelligence, calling into question widely held views on intelligence  see also Hutter Prize . Whether an AGI that satisfies one of these formalizations exhibits human like behaviour  such as the use of natural language  would depend on many factors, for example the manner in which the agent is embodied, or whether it has a reward function that closely approximates human primitives of cognition like hunger, pain, and so forth. Several tests meant to confirm human level AGI have been considered, including  As each test is completed it is supplanted with others. The current state is that generalist AIs can surpass most trained humans at arbitrary degree level exams, score in the top 0.1  of IQ tests, and be so Turing convincing that some of their own developers believe they re sentient, all of which are general tasks for which they were not specifically trained, and yet the AIs are still not considered  generally intelligent  enough to be AGI. There are many problems that may require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages  NLP , follow the author s argument  reason , know what is being talked about  knowledge , and faithfully reproduce the author s original intent  social intelligence . All of these problems need to be solved simultaneously in order to reach human level machine performance. A problem is informally called  AI complete  or  AI hard  if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose specific algorithm. AI complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem. AI complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do  and for computer security to repel brute force attacks. Modern AI research began in the mid 1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965   machines will be capable, within twenty years, of doing any work a man can do.  Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke s character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967,  Within a generation... the problem of creating  artificial intelligence  will substantially be solved . Several classical AI projects, such as Doug Lenat s Cyc project  that began in 1984 , and Allen Newell s Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful  applied AI . In the early 1980s, Japan s Fifth Generation Computer Project revived interest in AGI, setting out a ten year timeline that included AGI goals like  carry on a casual conversation . In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of  human level  artificial intelligence for fear of being labeled  wild eyed dreamer . In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub problems where AI can produce verifiable results and commercial applications, such as artificial neural networks and statistical machine learning. These  applied AI  systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018 development on this field was considered an emerging trend, and a mature stage was expected to happen in more than 10 years.  Most mainstream AI researchers hope that strong AI can be developed by combining programs that solve various sub problems. Hans Moravec wrote in 1988  I am confident that this bottom up route to artificial intelligence will one day meet the traditional top down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts. However, this is disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating  The expectation has often been voiced that  top down   symbolic  approaches to modeling cognition will somehow meet  bottom up   sensory  approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols  from the ground up. A free floating symbolic level like the software level of a computer will never be reached by this route  or vice versa    nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings  thereby merely reducing ourselves to the functional equivalent of a programmable computer .The term  artificial general intelligence  was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as  producing publications and preliminary results . The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university s Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course in AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences. As of 2022, AGI remains speculative. No such system has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that  machines will be capable, within twenty years, of doing any work a man can do . This prediction failed to come true. Microsoft co founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require  unforeseeable and fundamentally unpredictable breakthroughs  and a  scientifically deep understanding of cognition . Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human level artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight. Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts  views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50  confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5  answered with  never  when asked the same question but with a 90  confidence instead. Further current AGI progress considerations can be found above Tests for confirming human level AGI. A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that  over  60 year time frame there is a strong bias towards predicting the arrival of human level AI as between 15 and 25 years from the time the prediction was made . They analyzed 95 predictions made between 1950 and 2012 on when human level AI will come about.  In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007 the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near  i.e. between 2015 and 2045  was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16 26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top 5 test error rate of 15.3 , significantly better than the second best entry s rate of 26.3   the traditional approach used a weighted sum of scores from different pre defined classifiers . AlexNet was regarded as the initial ground breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple s Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six year old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT 3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT 3 is not an example of AGI, it is considered by some to be too advanced to classify as a narrow AI system. In the same year, Jason Rohrer used his GPT 3 account to develop a chatbot, and provided a chatbot developing platform called  Project December . OpenAI asked for changes to the chatbot to comply with their safety guidelines  Rohrer disconnected Project December from the GPT 3 API. In 2022, DeepMind developed Gato, a  general purpose  system capable of performing more than 600 different tasks. In 2023, Microsoft Research published a study on an early version of OpenAI s GPT 4, contending that it exhibited more general intelligence than previous AI models and demonstrated human level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT 4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems. In 2023, the AI researcher Geoffrey Hinton stated that  The idea that this stuff could actually get smarter than people   a few people believed that, . But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.One possible approach to achieving AGI is whole brain emulation  A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.  For low level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011  one hundred billion  neurons has on average 7,000 synaptic connections  synapses  to other neurons. The brain of a three year old child has about 1015 synapses  1 quadrillion . This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5 1014 synapses  100 to 500 trillion . An estimate of the brain s processing power, based on a simple switch model for neuron activity, is around 1014  100 trillion  synaptic updates per second  SUPS . In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second  cps .  For comparison, if a  computation  was equivalent to one  floating point operation    a measure used to rate current supercomputers   then 1016  computations  would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.  He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued. The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour  especially on a molecular scale  would require computational powers several orders of magnitude larger than Kurzweil s estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes. Some research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non real time simulations of a  brain   with 1011 neurons  in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM s Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain   It is not impossible to build a human brain and we can do it in 10 years,  Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro silicon interfaces have been proposed as an alternative implementation strategy that may scale better. Hans Moravec addressed the above arguments   brains are more complicated ,  neurons have to be modeled in more detail   in his 1997 paper  When will computer hardware match the human brain? . He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where. The actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network  among about 1000 cells in total . The animal s neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons  partly on the molecular level , but the result cannot yet be called a total success. A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons  e.g., a robotic body . Goertzel proposes virtual embodiment  like in Second Life  as an option, but it is unknown whether this would be sufficient. Desktop computers using microprocessors capable of more than 109 cps  Kurzweil s non standard unit  computations per second , see above  have been available since 2005. According to the brain power estimates used by Kurzweil  and Moravec , such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this  In addition, the scale of the human brain is not currently well constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous. In 1980, philosopher John Searle coined the term  strong AI  as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence  The first one he called  strong  because it makes a stronger statement  it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a  weak AI  machine would be precisely identical to a  strong AI  machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks. Mainstream AI is most interested in how a program behaves. According to Russell and Norvig,  as long as the program works, they don t care if you call it real or a simulation.  If the program can behave as if it has a mind, then there is no need to know if it actually has mind   indeed, there would be no way to tell. For AI research, Searle s  weak AI hypothesis  is equivalent to the statement  artificial general intelligence is possible . Thus, according to Russell and Norvig,  most AI researchers take the weak AI hypothesis for granted, and don t care about the strong AI hypothesis.  Thus, for academic AI research,  Strong AI  and  AGI  are two very different things. In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term  strong AI  to mean  human level artificial general intelligence . This is not the same as Searle s strong AI, unless you assume that consciousness is necessary for human level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out of scope. Other aspects of the human mind besides intelligence are relevant to the concept of strong AI, and these play a major role in science fiction and the ethics of artificial intelligence  These traits have a moral dimension, because a machine with this form of strong AI may have rights, analogous to the rights of non human animals. Preliminary work has been conducted on integrating full ethical agents with existing legal and social frameworks, focusing on the legal position and rights of  strong  AI. Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self awareness? It is possible that some of these traits naturally emerge from a fully intelligent machine. It is also possible that people will ascribe these properties to machines once they begin to act in a way that is clearly intelligent. Although the role of consciousness in strong AI AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language. Progress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU enabled CPUs. The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks. A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes,  No computer will be creative unless it can simulate all the nuances of human emotion.  AGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems. AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society. AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI s primary goal is to prevent existential catastrophes such as human extinction  which could be difficult if the Vulnerable World Hypothesis turns out to be true , it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.  The thesis that AI poses an existential risk for humans, and that this risk needs much more attention than it currently gets, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not  understand why some people are not concerned , and Hawking criticized widespread indifference in his 2014 editorial  So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying,  We ll arrive in a few decades,  would we just reply,  OK, call us when you get here we ll leave the lights on?  Probably not but this is more or less what is happening with AI.The fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. Additional intelligence caused humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. The gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities. The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won t be  smart enough to design super intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards . On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer  Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.A 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats   AGI removing itself from the control of human owners managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values  inadequate management of AGI, and existential risks . Many scholars who are concerned about existential risk advocate  possibly massive  research into solving the difficult  control problem  to answer the question  what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation state, resulting in AGI enabled warfare, and in the case of AI misalignment, AGI directed warfare, potentially against all humanity. The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then current machines were in any way intelligent is  an illusion  and a  stupendous con  by the wealthy. Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity
 AI Superpowers  China, Silicon Valley, and the New World Order is a 2018 non fiction book by  Kai Fu Lee, an Artificial Intelligence  AI  pioneer,  China expert  and venture capitalist.  Lee previously held executive positions at Apple, then SGI, Microsoft, and Google before creating his own company, Sinovation Ventures. According to Kai Fu Lee,  If data is the new oil, then China is the new Saudi Arabia.  Lee advances several arguments for why he thinks the artificial intelligence industry in China will excel in the artificial intelligence arms race  Lee proposes that knowledge workers are generally under greater threat of unemployment due to advances in AI, in a similar manner to the effect of the Industrial Revolution on physical laborers. The book includes examples of the most and least threatened jobs both in mental and physical labor.  Least threatened cognitive labor jobs include criminal defense attorney, public relations director, concierge, social worker, psychiatrist, and CEO because those jobs are highly social and are based on creativity or strategy. Cognitive jobs with the greatest threat to being made redundant by AI include  basic translator, radiologist, consumer loan officer, consumer tax prep, telemarketer, and customer service rep because those jobs are asocial and optimization based.  Physical jobs at low risk of irrelevance in the AI era include elder caregiver, physical therapist, hairstylist, and dog trainer because those roles are social and require high dexterity in an unstructured environment. Physical jobs most likely to suffer job losses or extinction include assembly line inspector, fruit harvester, truck driver, dishwasher, clothing factory worker, fast food and restaurant cooks and cashiers because these are asocial jobs in structured environments and don t require dexterity. US Senator Mark Warner named AI Superpowers as his recommended book for The 2018 POLITICO 50 Reading List.  On the other hand, American magazine Foreign Affairs criticized the book for promoting zero sum thinking and hyping Chinese state investment in tech ventures that often underperform relative to expectations  for focusing on deep learning to the exclusion of other forms of artificial intelligence  and for overgeneralizing the usefulness of Chinese data sets.  This article about a book on artificial intelligence is a stub. You can help Wikipedia by expanding it.
The AI boom  also known as the AI spring  refers to an ongoing period of rapid and unprecedented development in the field of artificial intelligence, with the generative AI race being a key component of this boom, which began in earnest with the founding of OpenAI in 2016 or 2017. OpenAI s generative AI systems, such as its various GPT models  starting in 2018  and DALL E  2021 , have played a significant role in driving this development. In 2022, large language models were improved to where they could be used for chatbot applications  text to image models were at a point where they were almost indiscernible from human made imagery  and speech synthesis software was able to replicate human speech efficiently. Over the course of late 2022 and 2023, dozens of new websites and AI chatbots were made live as Big Tech has tried to gain a foothold in the market and has led to an unprecedented increase in the ubiquity of AI tools. Public reaction to the AI boom has been mixed, with some parties hailing the new possibilities that AI creates, its potential for benefiting humanity, and sophistication, while other parties denounced it for threatening job security, being  uncanny  in its responses, and for giving flawed responses based on the programming. GPT 3 is a large language model that was released in 2020 by OpenAI and is capable of generating high quality human like text that can be hard to determine whether it was hard to be written by a human or not. An upgraded version called GPT 3.5 was used in ChatGPT, which later garnered attention for its detailed responses and articulate answers across many domains of knowledge. A new version called GPT 4 was released on March 2023 and was used in the Microsoft Bing search engine. Other language models have been released such as PaLM by Google and LLaMA by Meta Platforms. In January 2023, DeepL Write, an AI based tool to improve monolingual texts, was released. One of the first text to image models to capture widespread public attention was OpenAI s DALL E, a transformer system announced in January 2021. A successor capable of generating more complex and realistic images, DALL E 2, was unveiled in April 2022, followed by Stable Diffusion, an open source alternative, releasing in August 2022. Following other text to image models, language model powered text to video platforms such as DAMO, Make A Video, Imagen Video and Phenaki can generate video from text and or text image prompts. 15.ai was one of the first publicly available speech synthesis software that allowed people to generate natural emotive high fidelity text to speech voices from an assortment of fictional characters from a variety of media sources. It was first released on March 2020. ElevenLabs unveiled a website where users are able to upload voice samples to that allowed it to generate voices from them. The company was criticized after users were able to abuse its software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals and raised concerns that it could be used to generate deepfakes that were more convincing. An unofficial song created using the voices of musicians Drake and The Weeknd in speech synthesis software raised questions about the ethics and legality of similar software. 
AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence  AI  systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety. AI researchers have widely different opinions about the severity and primary sources of risk posed by AI technology   though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5  probability on an  extremely bad  e.g. human extinction   outcome of advanced AI. In a 2022 survey of the Natural language processing  NLP  community, 37  agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is  at least as bad as an all out nuclear war.  Scholars discuss current risks from critical systems failures, bias, and AI enabled surveillance  emerging risks from technological unemployment, digital manipulation, and weaponization  and speculative risks from losing control of future artificial general intelligence  AGI  agents. Some have criticized concerns about AGI, such as Stanford University adjunct professor Andrew Ng who compared them to  worrying about overpopulation on Mars when we have not even set foot on the planet yet.  Others, such as University of California, Berkeley professor Stuart J. Russell urge caution, arguing that  it is better to anticipate human ingenuity than to underestimate it.  Risks from AI began to be seriously discussed at the start of the computer age  Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.From 2008 to 2009, the AAAI commissioned a study to explore and address potential long term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science fiction authors but agreed that  additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes.  In 2011, Roman Yampolskiy introduced the term  AI safety engineering  at the Philosophy and Theory of Artificial Intelligence conference, listing prior failures of AI systems and arguing that  the frequency and seriousness of such events will steadily increase as AIs become more capable.  In 2014, philosopher Nick Bostrom published the book Superintelligence  Paths, Dangers, Strategies. His argument that future advanced systems may pose a threat to human existence prompted Elon Musk, Bill Gates, and Stephen Hawking to voice similar concerns. In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell. In the same year, a group of academics led by professor Stuart Russell founded the Center for Human Compatible AI at UC Berkeley and the Future of Life Institute awarded  6.5 million in grants for research aimed at  ensuring artificial intelligence  AI  remains safe, ethical and beneficial.  In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating  the advantages and drawbacks  of AI. In the same year, Concrete Problems in AI Safety   one of the first and most influential technical AI Safety agendas    was published. In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including  Race Avoidance  Teams developing AI systems should actively cooperate to avoid corner cutting on safety standards.  In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance. The following year, researchers organized a workshop at ICLR that focused on these problem areas. AI safety research areas include robustness, monitoring, and alignment. Robustness is concerned with making systems highly reliable, monitoring is about anticipating failures or detecting misuse, and alignment is focused on ensuring they have beneficial objectives. Robustness research focuses on ensuring that AI systems behave as intended in a wide range of different situations, which includes the following subproblems  Rare inputs can cause AI systems to fail catastrophically. For example, in the 2010 flash crash, automated trading systems unexpectedly overreacted to market aberrations, destroying one trillion dollars of stock value in minutes. Note that no distribution shift needs to occur for this to happen. Black swan failures can occur as a consequence of the input data being long tailed, which is often the case in real world environments. Autonomous vehicles continue to struggle with  corner cases  that might not have come up during training  for example, a vehicle might ignore a stop sign that is lit up as an LED grid. Though problems like these may be solved as machine learning systems develop a better understanding of the world, some researchers point out that even humans often fail to adequately respond to unprecedented events like the COVID 19 pandemic, arguing that black swan robustness will be a persistent safety problem. AI systems are often vulnerable to adversarial examples or  inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake . For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible. All of the images on the right are predicted to be an ostrich after the perturbation is applied.  Left  is a correctly predicted sample,  center  perturbation applied magnified by 10x,  right  adversarial example. Adversarial robustness is often associated with security. Researchers demonstrated that an audio signal could be imperceptibly modified so that speech to text systems transcribe it to any message the attacker chooses. Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors. Models that represent objectives  reward models  must also be adversarially robust.  For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. This issue can be addressed by improving the adversarial robustness of the reward model. More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward. Monitoring focuses on anticipating AI system failures so that they can be prevented or managed. Subproblems of monitoring include flagging when systems are uncertain, detecting malicious use, understanding the inner workings of black box AI systems, and identifying hidden functionality planted by a malicious actor. It is often important for human operators to gauge how much they should trust an AI system, especially in high stakes settings such as medical diagnosis. ML models generally express confidence by outputting probabilities  however, they are often overconfident, especially in situations that differ from those that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct. Similarly, anomaly detection or out of distribution  OOD  detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non anomalous inputs, though several other techniques are in use. Scholars and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, manipulate public opinion, or automate cyber attacks. These worries are a practical concern for companies like OpenAI which host powerful AI tools online. In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity. Neural networks have often been described as black boxes, meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. This makes it challenging to anticipate failures. In 2018, a self driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear. One benefit of transparency is explainability. It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment. Another benefit is to reveal the cause of failures. At the beginning of the 2020 COVID 19 pandemic, researchers used transparency tools to show that medical image classifiers were  paying attention  to irrelevant hospital labels. Transparency techniques can also be used to correct errors. For example, in the paper  Locating and Editing Factual Associations in GPT,  the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to  edit  this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision. Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high consequence failures in the future.  Inner  interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. For example, researchers identified a neuron in CLIP that responds to images of people in spider man costumes, sketches of spiderman, and the word  spider.  It also involves explaining connections between these neurons or  circuits . For example, researchers have identified pattern matching mechanisms in transformer attention that may play a role in how language models learn from their context.  Inner interpretability  has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations. ML models can potentially contain  trojans  or  backdoors    vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view  or a trojaned autonomous vehicle may function normally until a specific trigger is visible. Note that an adversary must have access to the system s training data in order to plant a trojan. This might not be difficult to do with some large models like CLIP or GPT 3 as they are trained on publicly available internet data. Researchers were able to plant a trojan in an image classifier by changing just 3 out of 3 million of the training images. In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools. In the field of artificial intelligence  AI , AI alignment research aims to steer AI systems towards humans  intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system is competent at advancing some objectives, but not the intended ones. It can be challenging for AI designers to align an AI system because it can be difficult for them to specify the full range of desired and undesired behaviors. To avoid this difficulty, they typically use simpler proxy goals, such as gaining human approval. However, this approach can create loopholes, overlook necessary constraints, or reward the AI system for just appearing aligned. Misaligned AI systems can malfunction or cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful ways  reward hacking . AI systems may also develop unwanted instrumental strategies such as seeking power or survival because such strategies help them achieve their given goals. Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is in deployment, where it faces new situations and data distributions. Today, these problems affect existing commercial systems such as language models, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected since these problems partially result from the systems being highly capable. Leading computer scientists such as Geoffrey Hinton and Stuart Russell argue that AI is approaching superhuman capabilities and could endanger human civilization if misaligned. The AI research community and the United Nations have called for technical research and policy solutions to ensure that AI systems are aligned with human values. It is common for AI risks  and technological risks more generally  to be categorized as misuse or accidents. Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. Policy analysts Zwetsloot and Dafoe wrote,  The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm  that is, the person who misused the technology, or the system that behaved in unintended ways  Often, though, the relevant causal chain is much longer.  Risks often arise from  structural  or  systemic  factors such as competitive pressures, diffusion of harms, fast paced development, high levels of uncertainty, and inadequate safety culture. In the broader context of safety engineering, structural factors like  organizational safety culture  play a central role in the popular STAMP risk analysis framework. Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision making, and facilitating cooperation. Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. This would increase  first strike  incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential preventing powerful AI models from being stolen and misused. The advancement of AI in economic and military domains could precipitate unprecedented political challenges. Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision makers often spelled the difference between stability and catastrophe. AI researchers have argued that AI technologies could also be used to assist decision making. For example, researchers are beginning to develop AI forecasting and advisory systems. Many of the largest global threats  nuclear war, climate change, etc  have been framed as cooperation challenges. As in the well known prisoner s dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes. A salient AI cooperation challenge is avoiding a  race to the bottom . In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political and technical efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions  often in  single player  games . Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact. AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems. It involves formulating and implementing concrete recommendations, as well as conducting more foundational research in order to inform what these recommendations should be. This section focuses on the aspects of AI governance that are specifically related to ensuring AI systems are safe and beneficial. AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. Some work has focused on anticipating specific risks that may arise from these impacts   for example, risks from mass unemployment, weaponization, disinformation, surveillance, and the concentration of power. Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and  race to the bottom  dynamics. Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation   it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems  however, if actors are competing in a domain with large returns to first movers or relative advantage, then they will be pressured to choose a sub optimal level of caution.  Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to  rush to regulate in ignorance.  Others, such as business magnate Elon Musk, call for pre emptive action to mitigate catastrophic risks. To date, very little AI safety regulation has been passed at the national level, though many bills have been introduced. A prominent example is the European Union s Artificial Intelligence Act, which regulates certain  high risk  AI applications and restricts potentially harmful uses such as facial recognition, subliminal manipulation, and social credit scoring. Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to  assure that systems are aligned with goals and values, including safety, robustness and trustworthiness.  Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when  catastrophic risks are present   development and deployment should cease in a safe manner until risks can be sufficiently managed.  In September 2021, the People s Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10 year National AI Strategy, which states the British government  takes the long term risk of non aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously.  The strategy describes actions to assess long term AI risks, including catastrophic risks. Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. The Defense Advanced Research Projects Agency engages in research on explainable artificial intelligence and improving robustness against adversarial attacks and The National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions in funding for empirical AI safety research. AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third party auditing, offering bounties for finding failures, sharing AI incidents  an AI incident database was created for this purpose , following guidelines to determine whether to publish research or models, and improving information and cyber security in AI labs. Companies have also made concrete commitments. Cohere, OpenAI, and AI21 proposed and agreed on  best practices for deploying language models,  focusing on mitigating misuse. To avoid contributing to racing dynamics, OpenAI has also stated in their charter that  if a value aligned, safety conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project  Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles and the Autonomous Weapons Open Letter. 
AI assisted virtualization software is a type of technology that combines the principles of virtualization with advanced artificial intelligence  AI  algorithms. This fusion is designed to allow more efficient, dynamic, and intelligent management of virtual environments and resources. This novel technology has been employed in a range of industries, including cloud computing, healthcare, data centers, and network infrastructure, to optimize performance, resource allocation, and security protocols. The initial concept of virtualization dates back to the 1960s, with the advent of mainframe computers. It wasn t until the early 2000s, however, when companies like VMware and Microsoft made it mainstream. The integration of AI into this established technology is a much more recent development, evolving with the rapid advancements in AI research and applications over the last decade. AI assisted virtualization software began to gain significant attention in the early 2020s as businesses and researchers began to acknowledge the potential of AI in automating and optimizing various aspects of virtualization. AI assisted virtualization software operates by leveraging AI techniques such as machine learning, deep learning, and neural networks to make more accurate predictions and decisions regarding the management of virtual environments. Key features include intelligent automation, predictive analytics, and dynamic resource allocation. AI assisted virtualization software has had a profound impact on various sectors. It has revolutionized cloud computing by optimizing the use of resources and significantly reducing costs. In healthcare, the technology is used to create virtual patient profiles that can be easily accessed and updated, improving diagnosis and treatment. It is also used in data centers to improve performance and energy efficiency. Furthermore, AI assisted virtualization has had notable contributions in the field of network function virtualization  NFV . It has enabled a more dynamic and flexible virtual network infrastructure, capable of auto scaling based on network load, identifying potential threats, and autonomously recovering from faults. Despite its many advantages, AI assisted virtualization software is not without its challenges. Implementing this type of software requires a high degree of technological sophistication and can incur significant costs. There are also concerns about the risks associated with AI, such as algorithmic bias and security vulnerabilities. Additionally, there are issues related to governance, ethics, and regulations of AI technologies. As the fields of AI and virtualization continue to evolve, AI assisted virtualization software is expected to become more advanced and integrated into an even wider range of applications. Future trends may include advanced self healing systems, integration with quantum computing, and the development of more sophisticated AI models that can autonomously manage increasingly complex virtual environments. 
In the field of artificial intelligence, the most difficult problems are informally known as AI complete or AI hard, implying that the difficulty of these computational problems, assuming intelligence is computational, is equivalent to that of solving the central artificial intelligence problem making computers as intelligent as people, or strong AI.  To call a problem AI complete reflects an attitude that it would not be solved by a simple specific algorithm.   AI complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem. Currently, AI complete problems cannot be solved with modern computer technology alone, but would also require human computation.  This property could be useful, for example, to test for the presence of humans as CAPTCHAs aim to do, and for computer security to circumvent brute force attacks. The term was coined by Fanya Montalvo by analogy with NP complete and NP hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller s 1987 PhD dissertation and in Eric Raymond s 1991 Jargon File. AI complete problems are hypothesized to include  To translate accurately, a machine must be able to understand the text. It must be able to follow the author s argument, so it must have some ability to reason. It must have extensive world knowledge so that it knows what is being discussed   it must at least be familiar with all the same commonsense facts that the average human translator knows. Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body  for example, the machine may need to understand how an ocean makes one feel to accurately translate a specific metaphor in the text. It must also model the authors  goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have wide variety of human intellectual skills, including reason, commonsense knowledge and the intuitions that underlie motion and manipulation, perception, and social intelligence. Machine translation, therefore, is believed to be AI complete  it may require strong AI to be done as well as humans can do it. Current AI systems can solve very simple and or restricted versions of AI complete problems, but never in their full generality. When AI researchers attempt to  scale up  their systems to handle more complicated, real world situations, the programs tend to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation  they fail as unexpected circumstances outside of its original problem context begin to appear.  When human beings are dealing with new situations in the world, they are helped immensely by the fact that they know what to expect  they know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other skills to fall back on. DeepMind published a work in May 2022 in which they trained a single model to do several things at the same time. The model, named Gato, can  play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.  Computational complexity theory deals with the relative computational difficulty of computable functions.  By definition, it does not cover problems whose solution is unknown or has not been characterised formally.  Since many AI problems have no formalisation yet, conventional complexity theory does not allow the definition of AI completeness. To address this problem, a complexity theory for AI has been proposed. It is based on a model of computation that splits the computational burden between a computer and a human  one part is solved by computer and the other part solved by human.  This is formalised by a human assisted Turing machine.  The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined. The complexity of executing an algorithm with a human assisted Turing machine is given by a pair           H   ,     M       , Phi   rangle    , where the first element represents the complexity of the human s part and the second element is the complexity of the machine s part. The complexity of solving the following problems with a human assisted Turing machine is  
A superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds.  Superintelligence  may also refer to a property of problem solving systems  e.g., superintelligent language translators or engineering assistants  whether or not these high level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an intelligence explosion and associated with a technological singularity. University of Oxford philosopher Nick Bostrom defines superintelligence as  any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest . The program Fritz falls short of this conception of superintelligence even though it is much better than humans at chess because Fritz cannot outperform humans in other tasks. Following Hutter and Legg, Bostrom treats superintelligence as general dominance at goal oriented behavior, leaving open whether an artificial or human superintelligence would possess capacities such as intentionality  cf. the Chinese room argument  or first person consciousness  cf. the hard problem of consciousness . Technological researchers disagree about how likely present day human intelligence is to be surpassed. Some argue that advances in artificial intelligence  AI  will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence. A number of futures studies scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification. Some researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of perfect recall, a vastly superior knowledge base, and the ability to multitask in ways not possible to biological entities. This may give them the opportunity to either as a single being or as a new species become much more powerful than humans, and to displace them. A number of scientists and forecasters argue for prioritizing early research into the possible benefits and risks of human and machine cognitive enhancement, because of the potential social impact of such technologies. Philosopher David Chalmers argues that artificial general intelligence is a very likely path to superhuman intelligence. Chalmers breaks this claim down into an argument that AI can achieve equivalence to human intelligence, that it can be extended to surpass human intelligence, and that it can be further amplified to completely dominate humans across arbitrary tasks. Concerning human level equivalence, Chalmers argues that the human brain is a mechanical system, and therefore ought to be emulatable by synthetic materials. He also notes that human intelligence was able to biologically evolve, making it more likely that human engineers will be able to recapitulate this invention. Evolutionary algorithms in particular should be able to produce human level AI. Concerning intelligence extension and amplification, Chalmers argues that new AI technologies can generally be improved on, and that this is particularly likely when the invention can assist in designing new technologies. If research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself   a feature called  recursive self improvement . It would then be even better at improving itself, and could continue doing so in a rapidly increasing cycle, leading to a superintelligence. This scenario is known as an intelligence explosion.  Such an intelligence would not have the limitations of human intellect, and may be able to invent or discover almost anything. Computer components already greatly surpass human performance in speed.  Bostrom writes,  Biological neurons operate at a peak speed of about 200 Hz, a full seven orders of magnitude slower than a modern microprocessor   2 GHz .  Moreover, neurons transmit spike signals across axons at no greater than 120 m s,  whereas existing electronic processing cores can communicate optically at the speed of light . Thus, the simplest example of a superintelligence may be an emulated human mind run on much faster hardware than the brain. A human like reasoner that could think millions of times faster than current humans would have a dominant advantage in most reasoning tasks, particularly ones that require haste or long strings of actions. Another advantage of computers is modularity, that is, their size or computational capacity can be increased.  A non human  or modified human  brain could become much larger than a present day human brain, like many supercomputers. Bostrom also raises the possibility of collective superintelligence  a large enough number of separate reasoning systems, if they communicated and coordinated well enough, could act in aggregate with far greater capabilities than any sub agent. There may also be ways to qualitatively improve on human reasoning and decision making. Humans appear to differ from chimpanzees in the ways we think more than we differ in brain size or speed. Humans outperform non human animals in large part because of new or enhanced reasoning capacities, such as long term planning and language use.  See evolution of human intelligence and primate cognition.  If there are other possible improvements to reasoning that would have a similarly large impact, this makes it likelier that an agent can be built that outperforms humans in the same fashion humans outperform chimpanzees. All of the above advantages hold for artificial superintelligence, but it is not clear how many hold for biological superintelligence. Physiological constraints limit the speed and size of biological brains in many ways that are inapplicable to machine intelligence. As such, writers on superintelligence have devoted much more attention to superintelligent AI scenarios. Carl Sagan suggested that the advent of Caesarean sections and in vitro fertilization may permit humans to evolve larger heads, resulting in improvements via natural selection in the heritable component of human intelligence. By contrast, Gerald Crabtree has argued that decreased selection pressure is resulting in a slow, centuries long reduction in human intelligence, and that this process instead is likely to continue into the future. There is no scientific consensus concerning either possibility, and in both cases the biological change would be slow, especially relative to rates of cultural change. Selective breeding, nootropics, epigenetic modulation, and genetic engineering could improve human intelligence more rapidly. Bostrom writes that if we come to understand the genetic component of intelligence, pre implantation genetic diagnosis could be used to select for embryos with as much as 4 points of IQ gain  if one embryo is selected out of two , or with larger gains  e.g., up to 24.3 IQ points gained if one embryo is selected out of 1000 . If this process is iterated over many generations, the gains could be an order of magnitude greater. Bostrom suggests that deriving new gametes from embryonic stem cells could be used to iterate the selection process very rapidly. This notion, Iterated Embryo Selection, has received wide treatment from other authors. A well organized society of high intelligence humans of this sort could potentially achieve collective superintelligence. Alternatively, collective intelligence might be constructible by better organizing humans at present levels of individual intelligence. A number of writers have suggested that human civilization, or some aspect of it  e.g., the Internet, or the economy , is coming to function like a global brain with capacities far exceeding its component agents. If this systems based superintelligence relies heavily on artificial components, however, it may qualify as an AI rather than as a biology based superorganism. A prediction market is sometimes considered an example of working collective intelligence system, consisting of humans only  assuming algorithms are not used to inform decisions . A final method of intelligence amplification would be to directly enhance individual humans, as opposed to enhancing their social or reproductive dynamics. This could be achieved using nootropics, somatic gene therapy, or brain computer interfaces. However, Bostrom expresses skepticism about the scalability of the first two approaches, and argues that designing a superintelligent cyborg interface is an AI complete problem. Most surveyed AI researchers expect machines to eventually be able to rival humans in intelligence, though there is little consensus on when this will likely happen. At the 2006 AI 50 conference, 18  of attendees reported expecting machines to be able  to simulate learning and every other aspect of human intelligence  by 2056  41  of attendees expected this to happen sometime after 2056  and 41  expected machines to never reach that milestone. In a survey of the 100 most cited authors in AI  as of May 2013, according to Microsoft academic search , the median year by which respondents expected machines  that can carry out most human professions at least as well as a typical human   assuming no global catastrophe occurs  with 10  confidence is 2024  mean 2034, st. dev. 33 years , with 50  confidence is 2050  mean 2072, st. dev. 110 years , and with 90  confidence is 2070  mean 2168, st. dev. 342 years . These estimates exclude the 1.2  of respondents who said no year would ever reach 10  confidence, the 4.1  who said  never  for 50  confidence, and the 16.5  who said  never  for 90  confidence. Respondents assigned a median 50  probability to the possibility that machine superintelligence will be invented within 30 years of the invention of approximately human level machine intelligence. In a survey of 352 machine learning researchers published in 2018, the median year by which respondents expected  High level machine intelligence  with 50  confidence is 2061. The survey defined the achievement of high level machine intelligence as when unaided machines can accomplish every task better and more cheaply than human workers. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. Bostrom expressed concern about what values a superintelligence should be designed to have. He compared several proposals  Bostrom clarifies these terms  instead of implementing humanity s coherent extrapolated volition, one could try to build an AI with the goal of doing what is morally right, relying on the AI s superior cognitive capacities to figure out just which actions fit that description. We can call this proposal  moral rightness   MR  ... MR would also appear to have some disadvantages. It relies on the notion of  morally right,  a notoriously difficult concept, one with which philosophers have grappled since antiquity without yet attaining consensus as to its analysis. Picking an erroneous explication of  moral rightness  could result in outcomes that would be morally very wrong ... The path to endowing an AI with any of these  concepts might involve giving it general linguistic ability  comparable, at least, to that of a normal human adult . Such a general ability to understand natural language could then be used to understand what is meant by  morally right.  If the AI could grasp the meaning, it could search for actions that fit ...One might try to preserve the basic idea of the MR model while reducing its demandingness by focusing on moral permissibility  the idea being that we could let the AI pursue humanity s CEV so long as it did not act in ways that are morally impermissible.Responding to Bostrom, Santos Lang raised concern that developers may attempt to start with a single kind of superintelligence. It has been suggested that if AI systems rapidly become superintelligent, they may take unforeseen actions or out compete humanity. Researchers have argued that, by way of an  intelligence explosion,  a self improving AI could become so powerful as to be unstoppable by humans. Concerning human extinction scenarios, Bostrom  2002  identifies superintelligence as a possible cause  When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question.In theory, since a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled, unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference. Eliezer Yudkowsky illustrates such instrumental convergence as follows   The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.  This presents the AI control problem  how to build an intelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators. The danger of not designing control right  the first time,  is that a superintelligence may be able to seize power over its environment and prevent humans from shutting it down. Since a superintelligent AI will likely have the ability to not fear death and instead consider it an avoidable situation which can be predicted and avoided by simply disabling the power button. Potential AI control strategies include  capability control   limiting an AI s ability to influence the world  and  motivational control   building an AI whose goals are aligned with human values . Bill Hibbard advocates for public education about superintelligence and public control over the development of superintelligence. 
ASR complete is, by analogy to  NP completeness  in complexity theory, a term to indicate that the difficulty of a computational problem is equivalent to solving the central Automatic Speech Recognition problem, i.e. recognize and understanding spoken language. Unlike  NP completeness , this term is typically used informally. Such problems are hypothesised to include  These problems are easy for humans to do  in fact, they are described directly in terms of imitating humans . Some systems can solve very simple restricted versions of these problems, but none can solve them in their full generality. 
Arizona Financial Text System  AZFinText  is a textual based quantitative financial prediction system written by Robert P. Schumaker of University of Texas at Tyler and Hsinchun Chen of the University of Arizona. This system differs from other systems in that it uses financial text as one of its key means of predicting stock price movement. This reduces the information lag time problem evident in many similar systems where new information must be transcribed  e.g., such as losing a costly court battle or having a product recall , before the quant can react appropriately. AZFinText overcomes these limitations by utilizing the terms used in financial news articles to predict future stock prices twenty minutes after the news article has been released. It is believed that certain article terms can move stocks more than others.  Terms such as factory exploded or workers strike will have a depressing effect on stock prices whereas terms such as earnings rose will tend to increase stock prices. When a human trading expert sees certain terms, they will react in a somewhat predictable fashion.  AZFinText capitalizes on the arbitrage opportunities that exist when investment experts over and under react to certain news stories. By analyzing breaking financial news articles and focusing on specific parts of speech, portfolio selection, term weighting and even article sentiment, the AZFinText system becomes a powerful tool and is a radically different way of looking at stock market prediction. The foundation of AZFinText can be found in the ACM TOIS article. Within this paper, the authors tested several different prediction models and linguistic textual representations.  From this work, it was found that using the article terms and the price of the stock at the time the article was released was the most effective model and using proper nouns was the most effective textual representation technique.  Combining the two, AZFinText netted a 2.84  trading return over the five week study period. AZFinText was then extended to study what combination of peer organizations help to best train the system. Using the premise that IBM has more in common with Microsoft than GM, AZFinText studied the effect of varying peer based training sets.  To do this, AZFinText trained on the various levels of GICS and evaluated the results.  It was found that sector based training was most effective, netting an 8.50  trading return, outperforming Jim Cramer, Jim Jubak and DayTraders.com during the study period.  AZFinText was also compared against the top 10 quantitative systems and outperformed 6 of them. A third study investigated the role of portfolio building in a textual financial prediction system. From this study, Momentum and Contrarian stock portfolios were created and tested.  Using the premise that past winning stocks will continue to win and past losing stocks will continue to lose, AZFinText netted a 20.79  return during the study period.  It was also noted that traders were generally overreacting to news events, creating the opportunity of abnormal returns. A fourth study looked into using author sentiment as an added predictive variable. Using the premise that an author can unwittingly influence market trades simply by the terms they use, AZFinText was tested using tone and polarity features.  It was found that Contrarian activity was occurring within the market, where articles of a positive tone would decrease in price and articles of a negative tone would increase in price. A further study investigated what article verbs have the most influence on stock price movement. From this work, it was found that planted, announcing, front, smaller and crude had the highest positive impact on stock price. AZFinText has been the topic of discussion by numerous media outlets.  Some of the more notable ones include The Wall Street Journal, MIT s Technology Review, Dow Jones Newswire, WBIR in Knoxville, TN, Slashdot and other media outlets. 
Action selection is a way of characterizing the most basic problem of intelligent systems  what to do next. In artificial intelligence and computational cognitive science,  the action selection problem  is typically associated with intelligent agents and animats artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior. One problem for understanding action selection is determining the level of abstraction used for specifying an  act . At the most basic level of abstraction, an atomic act could be anything from contracting a muscle cell to provoking a war. Typically for any one action selection mechanism, the set of possible actions is predefined and fixed. Most researchers working in this field place high demands on their agents  For these reasons action selection is not trivial and attracts a good deal of research. The main problem for action selection is complexity. Since all computation takes both time and space  in memory , agents cannot possibly consider every option available to them at every instant in time. Consequently, they must be biased, and constrain their search in some way. For AI, the question of action selection is what is the best way to constrain this search? For biology and ethology, the question is how do various types of animals constrain their search? Do all animals use the same approaches? Why do they use the ones they do? One fundamental question about action selection is whether it is really a problem at all for an agent, or whether it is just a description of an emergent property of an intelligent agent s behavior. However, if we consider how we are going to build an intelligent agent, then it becomes apparent there must be some mechanism for action selection. This mechanism may be highly distributed  as in the case of distributed organisms such as social insect colonies or slime mold  or it may be a special purpose module. The action selection mechanism  ASM  determines not only the agent s actions in terms of impact on the world, but also directs its perceptual attention, and updates its memory. These egocentric sorts of actions may in turn result in modifying the agent s basic behavioural capacities, particularly in that updating memory implies some form of machine learning is possible. Ideally, action selection itself should also be able to learn and adapt, but there are many problems of combinatorial complexity and computational tractability that may require restricting the search space for learning. In AI, an ASM is also sometimes either referred to as an agent architecture or thought of as a substantial part of one. Generally, artificial action selection mechanisms can be divided into several categories  symbol based systems sometimes known as classical planning, distributed solutions, and reactive or dynamic planning. Some approaches do not fall neatly into any one of these categories. Others are really more about providing scientific models than practical AI control  these last are described further in the next section. Early in the history of artificial intelligence, it was assumed that the best way for an agent to choose what to do next would be to compute a probably optimal plan, and then execute that plan. This led to the physical symbol system hypothesis, that a physical agent that can manipulate symbols is necessary and sufficient for intelligence. Many software agents still use this approach for action selection. It normally requires describing all sensor readings, the world, all of ones actions and all of one s goals in some form of predicate logic. Critics of this approach complain that it is too slow for real time planning and that, despite the proofs, it is still unlikely to produce optimal plans because reducing descriptions of reality to logic is a process prone to errors. Satisficing is a decision making strategy that attempts to meet criteria for adequacy, rather than identify an optimal solution. A satisficing strategy may often, in fact, be  near  optimal if the costs of the decision making process itself, such as the cost of obtaining complete information, are considered in the outcome calculus. Goal driven architectures   In these symbolic architectures, the agent s behaviour is typically described by a set of goals. Each goal can be achieved by a process or an activity, which is described by a prescripted plan. The agent must just decide which process to carry on to accomplish a given goal. The plan can expand to subgoals, which makes the process slightly recursive. Technically, more or less, the plans exploits condition rules. These architectures are reactive or hybrid. Classical examples of goal driven architectures are implementable refinements of belief desire intention architecture like JAM or IVE. In contrast to the symbolic approach, distributed systems of action selection actually have no one  box  in the agent which decides the next action. At least in their idealized form, distributed systems have many modules running in parallel and determining the best action based on local expertise. In these idealized systems, overall coherence is expected to emerge somehow, possibly through careful design of the interacting components. This approach is often inspired by artificial neural networks research. In practice, there is almost always some centralised system determining which module is  the most active  or has the most salience. There is evidence real biological brains also have such executive decision systems which evaluate which of the competing systems deserves the most attention, or more properly, has its desired actions disinhibited. Because purely distributed systems are difficult to construct, many researchers have turned to using explicit hard coded plans to determine the priorities of their system. Dynamic or reactive planning methods compute just one next action in every instant based on the current context and pre scripted plans. In contrast to classical planning methods, reactive or dynamic approaches do not suffer combinatorial explosion. On the other hand, they are sometimes seen as too rigid to be considered strong AI, since the plans are coded in advance. At the same time, natural intelligence can be rigid in some contexts although it is fluid and able to adapt in others. Example dynamic planning mechanisms include  Sometimes to attempt to address the perceived inflexibility of dynamic planning, hybrid techniques are used. In these, a more conventional AI planning system searches for new plans when the agent has spare time, and updates the dynamic plan library when it finds good solutions. The important aspect of any such system is that when the agent needs to select an action, some solution exists that can be used immediately  see further anytime algorithm . Many dynamic models of artificial action selection were originally inspired by research in ethology. In particular, Konrad Lorenz and Nikolaas Tinbergen provided the idea of an innate releasing mechanism to explain instinctive behaviors  fixed action patterns . Influenced by the ideas of William McDougall, Lorenz developed this into a  psychohydraulic  model of the motivation of behavior. In ethology, these ideas were influential in the 1960s, but they are now regarded as outdated because of their use of an energy flow metaphor  the nervous system and the control of behavior are now normally treated as involving information transmission rather than energy flow. Dynamic plans and neural networks are more similar to information transmission, while spreading activation is more similar to the diffuse control of emotional   hormonal systems. Stan Franklin has proposed that action selection is the right perspective to take in understanding the role and evolution of mind. See his page on the action selection paradigm. Archived 2006 10 09 at the Wayback Machine Some researchers create elaborate models of neural action selection. See for example  The locus coeruleus  LC  is one of the primary sources of noradrenaline in the brain, and has been associated with selection of cognitive processing, such as attention and behavioral tasks. The substantia nigra pars compacta  SNc  is one of the primary sources of dopamine in the brain, and has been associated with action selection, primarily as part of the basal ganglia.  CNET is a hypothesized neural signaling mechanism in the SNc and LC  which are catecholaminergic neurons , that could assist with action selection by routing energy between neurons in each group as part of action selection, to help one or more neurons in each group to reach action potential. It was first proposed in 2018, and is based on a number of physical parameters of those neurons, which can be broken down into three major components  1  Ferritin and neuromelanin are present in high concentrations in those neurons, but it was unknown in 2018 whether they formed structures that would be capable of transmitting electrons over relatively long distances on the scale of microns between the largest of those neurons, which had not been previously proposed or observed. Those structures would also need to provide a routing or switching function, which had also not previously been proposed or observed.  Evidence of the presence of ferritin and neuromelanin structures in those neurons and their ability to both conduct electrons by sequential tunneling and to route switch the path of the neurons was subsequently obtained. 2    The axons of large SNc neurons were known to have extensive arbors, but it was unknown whether post synaptic activity at the synapses of those axons would raise the membrane potential of those neurons sufficiently to cause the electrons to be routed to the neuron or neurons with the most post synaptic activity for the purpose of action selection.  At the time, prevailing explanations of the purpose of those neurons was that they did not mediate action selection and were only modulatory and non specific. Prof. Pascal Kaeser of Harvard Medical School subsequently obtained evidence that large SNc neurons can be temporally and spatially specific and mediate action selection.  Other evidence indicates that the large LC axons have similar behavior. 3  Several sources of electrons or excitons to provide the energy for the mechanism were hypothesized in 2018 but had not been observed at that time.  Dioxetane cleavage  which can occur during somatic dopamine metabolism by quinone degradation of melanin  was contemporaneously proposed to generate high energy triplet state electrons by Prof. Doug Brash at Yale, which could provide a source for electrons for the CNET mechanism. While evidence of a number of physical predictions of the CNET hypothesis has thus been obtained, evidence of whether the hypothesis itself is correct has not been sought. One way to try to determine whether the CNET mechanism is present in these neurons would be to use quantum dot fluorophores and optical probes to determine whether electron tunneling associated with ferritin in the neurons is occurring in association with specific actions.    
In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path. It is related to the concept of consistent heuristics. While all consistent heuristics are admissible, not all admissible heuristics are consistent. An admissible heuristic is used to estimate the cost of reaching the goal state in an informed search algorithm. In order for a heuristic to be admissible to the search problem, the estimated cost must always be lower than or equal to the actual cost of reaching the goal state.  The search algorithm uses the admissible heuristic to find an estimated  optimal path to the goal state from the current node.  For example, in A  search the evaluation function  where      n      is the current node  is      f   n     g   n     h   n        where     h   n        is calculated using the heuristic  function. With a non admissible heuristic, the A  algorithm could  overlook the optimal solution to a search problem due to an  overestimation in     f   n       . An admissible heuristic can be derived from a relaxed version of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods. Two different examples of admissible heuristics apply to the fifteen puzzle problem  The Hamming distance is the total number of misplaced tiles. It is clear that this heuristic is admissible since the total number of moves to order the tiles correctly is at least the number of misplaced tiles  each tile not in place must be moved at least once . The cost  number of moves  to the goal  an ordered puzzle  is at least the Hamming distance of the puzzle. The Manhattan distance of a puzzle is defined as  Consider the puzzle below in which the player wishes to move each tile such that the numbers are ordered. The Manhattan distance is an admissible heuristic in this case because every tile will have to be moved at least the number of spots in between itself and its correct position. The subscripts show the Manhattan distance for each tile. The total Manhattan distance for the shown puzzle is  If an admissible heuristic is used in an algorithm that, per iteration, progresses only the path of lowest evaluation  current cost   heuristic  of several candidate paths, terminates the moment its exploration reaches the goal and, crucially, never closes all optimal paths before terminating  something that s possible with A  search algorithm if special care isn t taken , then this algorithm can only terminate on an optimal path. To see why, consider the following proof by contradiction  Assume such an algorithm managed to terminate on a path T with a true cost Ttrue greater than the optimal path S with true cost Strue. This means that before terminating, the evaluated cost of T was less than or equal to the evaluated cost of S  or else S would have been picked . Denote these evaluated costs Teval and Seval respectively. The above can be summarized as follows, If our heuristic is admissible it follows that at this penultimate step Teval   Ttrue because any increase on the true cost by the heuristic on T would be inadmissible and the heuristic cannot be negative. On the other hand, an admissible heuristic would require that Seval   Strue which combined with the above inequalities gives us Teval   Ttrue and more specifically Teval   Ttrue. As Teval and Ttrue cannot be both equal and unequal our assumption must have been false and so it must be impossible to terminate on a more costly than optimal path. As an example, let us say we have costs as follows  the cost above below a node is the heuristic, the cost at an edge is the actual cost  So clearly we would start off visiting the top middle node, since the expected total cost, i.e.     f   n       , is     10   0   10     . Then the goal would be a candidate, with     f   n        equal to     10   100   0   110     . Then we would clearly pick the bottom nodes one after the other, followed by the updated goal, since they all have     f   n        lower than the     f   n        of the current goal, i.e. their     f   n        is     100 , 101 , 102 , 102     . So even though the goal was a candidate, we could not pick it because there were still better paths out there. This way, an admissible heuristic can ensure optimality. However, note that although an admissible heuristic can guarantee final optimality, it is not necessarily efficient. 
Adobe Enhanced Speech is an online artificial intelligence software tool by Adobe that aims to significantly improve the quality of recorded speech that may be badly muffled, reverberated, full of artifacts, tinny, etc. and convert it to a studio grade, professional level, regardless of the initial input s clarity. Users may upload mp3 or wav files up to an hour long and a gigabyte in size to the site to convert them relatively quickly, then being free to listen to the converted version, toggle back and forth and alternate between it and the original as it plays, and download it. Currently in beta and free to the public, it has been used in the restoration of old movies and the creation of professional quality podcasts, narrations, etc. by those without sufficient microphones. Although the model still has some current limitations, such as not being compatible with singing and occasional issues with excessively muffled source audio resulting in a light lisp in the improved version, it is otherwise noted as incredibly effective and efficient in its purpose. Utilizing advanced machine learning algorithms to distinguish between speech and background sounds, it enhances the quality of the speech by filtering out the noise and artifacts, adjusting the pitch and volume levels, and normalizing the audio. This is accomplished by the network having been trained on a large dataset of speech samples from a diverse range of sources and then being fine tuned to optimize the output.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
The agent systems reference model  ASRM  is a layered, abstract description for multiagent systems.  As such, the reference model The ASRM differentiates itself from technical standards, such as Knowledge Interchange Format, Knowledge Query and Manipulation Language, and those of the Foundation for Intelligent Physical Agents in that it defines the required existence of components of a multiagent system  standards prescribe how they are designed. The ASRM was technically constructed through forensic software analysis of existing agent based systems.  Such fielded systems include JaDE, Cougaar, EMAA, NOMADS, Retsina, A Globe, among others.  In so doing, through empirical evidence, the ASRM motivates its functional breakdown of agent based systems. The ASRM was started in July 2005, with the first draft having been completed in November 2006.  Contributors to the document have included Drexel University, Cougaar Software, Global InfoTek  see also  CoABS , Soar Technology  see also  Soar , Penn State University, University of Southern California, University of South Carolina, the Florida Institute for Human and Machine Cognition, University of West Florida, BBN Technologies, Telcordia, Lockheed Martin, General Dynamics and others.  This computer science article is a stub. You can help Wikipedia by expanding it.
Algorithmic accountability refers to the issue of where accountability should be apportioned for the consequences of real world actions that were taken on account of algorithms used to reach a decision. In principle, an algorithm should be designed in such a way that there is no bias behind the decisions that are made during its execution process. That is, the algorithm should evaluate only essential characteristics of the inputs presented, without making distinctions based on characteristics that usually should not be used in a social environment, such as the ethnicity of an individual who is being judged in a court of law. However, this principle may not always respected and on occasions individuals may be deliberately harmed by these outcomes. It is at this point that the debate arises about who should be held responsible for the losses caused by a decision made by the machine  the system itself or the individual who designed it with such parameters, since a decision that harms other individuals due to lack of impartiality or incorrect data analysis will happen because the algorithm was designed to perform that way. The algorithms designed nowadays are spread out in the most diverse sectors of society that have some involvement of computational techniques in their control systems, of the most diverse sizes and with the most varied applications, being present in, but not limited to medical, transportation and payment services. In these sectors, the algorithms embedded in the applications perform activities of natures such as  The way these algorithms are implemented, however, can be quite confusing. Effectively, algorithms in general behave like black boxes, and in most cases it is not known the process that an input data goes through during the execution of a particular routine, but only the resulting output linked to what was initially entered. In general, there is no knowledge related to the parameters that make up the algorithm and how biased to certain aspects they can be, which can end up raising suspicions about the bias with which an algorithm treats a set of inputs. It depends on the outputs that are generated after the executions and if there is any individual who feels harmed by the result presented, especially when another individual, under similar conditions, ends up getting a different answer. According to Nicholas Diakopoulos  But these algorithms can make mistakes. They have biases. Yet they sit in opaque black boxes, their inner workings, their inner  thoughts  hidden behind layers of complexity. We need to get inside that black box, to understand how they may be exerting power on us, and to understand where they might be making unjust mistakesAs mentioned before, algorithms are widespread in the most diverse fields of knowledge and make decisions that affect the lives of the entire population. Moreover, their structure and parameters are often unknown by those who are affected by them. A case that illustrates this well was a recent ruling by the Wisconsin Supreme Court regarding so called  risk assessment  for crime. It was ruled that such a score, which is computed through an algorithm that takes various parameters from individuals, cannot be used as a determining factor for an accused to be arrested. In addition, and more importantly, the court ruled that all reports submitted to judges in such cases should contain an information related to the accuracy presented by the algorithm used to calculate the scores. This event has been considered a major victory in the sense of how the data driven society should deal with softwares that operates making decisions and how to make them reliable, since the use of these algorithms in highly complex situations like courts requires a very high degree of impartiality when treating the data provided as input. However, defenders of concepts related to big data argue that there is still much to be done regarding the accuracy presented by the results of algorithms, since there is still nothing concrete regarding how we can understand what is happening during data processing, leaving room for doubt regarding the suitability of the algorithm or those who designed it. Another case where there is the possibility of biased execution by an algorithm was the subject of an article in The Washington Post discussing the passenger transportation tool Uber. After analyzing the data collected, it was possible to verify that the estimated waiting time for users of the service was higher depending on the neighborhood where these individuals lived. The main factors affecting the increase in time were the majority ethnicity and the average income of the neighborhood. In the above case, environments with a majority white population and with higher purchasing power had lower waiting time rates, while neighborhoods with a population of other ethnicities and lower average income had higher waiting times. It is important, however, to make clear that this conclusion was based on the data collected, not necessarily representing a cause and effect relationship, but possibly a correlation, and no value judgment is made about the behavior adopted by the Uber app in these situations. In an article published in the column  Direito Digit l  in Migalhas website, Coriolano Almeida Camargo and Marcelo Crespo discuss the use of algorithms in contexts previously occupied by human beings when making decisions and the flaws that can occur when validating whether the decision made by the machine was fair or not. The issue transcends and will transcend the concern with which data is collected from consumers to the question of how this data is used by algorithms. Despite the existence of some consumer protection regulations, there is no effective mechanism available to consumers that tells them, for example, whether they have been automatically discriminated against by being denied loans or jobs.The great evolution of technology that we are experiencing has brought a wide range of innovations to society, among them the introduction of the concept of autonomous vehicles controlled by systems. That is, by algorithms that are embedded in these devices and that control the entire process of navigation on streets and roads and that face situations where they need to collect data and evaluate the environment and the context where they are inserted in order to decide what actions should be taken at each moment, simulating the actions of a human driver behind the wheel. In the same article in the excerpt above, Camargo and Crespo discuss the possible problems involving the use of embedded algorithms in autonomous cars, especially with regard to decisions made at critical moments in the process of using the vehicles. The technological landscape is rapidly changing with the advent of very powerful computers and algorithms that are moving toward the impressive development of artificial intelligence. We have no doubt that artificial intelligence will revolutionize the provision of services and also industry. The problem is that ethical issues urgently need to be thought through and discussed. Or are we simply going to allow machines to judge us in court cases? Or that they decide who should live or die in accident situations that could be intervened by some technological equipment, such as autonomous cars?In TechCrunch website, Hemant Taneja wrote  Concern about  black box  algorithms that govern our lives has been spreading. New York University s Information Law Institute hosted a conference on algorithmic accountability, noting   Scholars, stakeholders, and policymakers question the adequacy of existing mechanisms governing algorithmic decision making and grapple with new challenges presented by the rise of algorithmic power in terms of transparency, fairness, and equal treatment.  Yale Law School s Information Society Project is studying this, too.  Algorithmic modeling may be biased or limited, and the uses of algorithms are still opaque in many critical sectors,  the group concluded.Some discussions on the subject have already been held by experts in order to try to reach some viable solution to understand what goes on in the black boxes that  guard  the algorithms. It is advocated primarily that the companies that develop the code themselves, which are responsible for running the data analysis algorithms, should be responsible for ensuring the reliability of their systems, for example by disclosing what goes on  behind the scenes  in their algorithms. In TechCrunch website, Hemant Taneja wrote  ...these new utilities  the Googles, Amazons and Ubers of the world  must proactively build algorithmic accountability into their systems, faithfully and transparently act as their own watchdogs or risk eventual onerous regulation.From the excerpt above, it can be seen that one possible way is the introduction of a regulation in the computer sectors that run these algorithms so that there is an effective supervision of the activities that are happening during their executions. However, the introduction of this regulation could end up affecting the software industries and developers, and it would possibly be more advantageous for them if they would willingly open and disclose the content of what is being executed and what parameters are used for decision making, which could even end up benefiting the companies themselves with regard to the way in which the solutions developed and applied by them work. Another possibility discussed is self regulation by the developer companies themselves through the software. In TechCrunch website, Hemant Taneja wrote  There s another benefit   perhaps a huge one   to software defined regulation. It will also show us a path to a more efficient government. The world s legal logic and regulations can be coded into software and smart sensors can offer real time monitoring of everything from air and water quality, traffic flows and queues at the DMV. Regulators define the rules, technologist create the software to implement them and then AI and ML help refine iterations of policies going forward. This should lead to much more efficient, effective governments at the local, national and global levels.
In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s.  It is used in inductive inference theory and analyses of algorithms. In his  general theory of inductive inference, Solomonoff uses the method together with Bayes  rule to obtain probabilities of prediction for an algorithm s future outputs. In the mathematical formalism used, the observations have the form of finite binary strings viewed as outputs of Turing machines, and the universal prior is a probability distribution over the set of finite binary strings calculated from a probability distribution over programs  that is, inputs to a universal Turing machine .  The prior is universal in the Turing computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated. Algorithmic probability is the main ingredient of Solomonoff s theory of inductive inference, the theory of prediction based on observations  it was invented with the goal of using it for machine learning  given a sequence of symbols, which one will come next? Solomonoff s theory provides an answer that is optimal in a certain sense, although it is incomputable. Unlike, for example, Karl Popper s informal inductive inference theory,  Solomonoff s is mathematically rigorous. Four principal inspirations for Solomonoff s algorithmic probability were  Occam s razor, Epicurus  principle of multiple explanations, modern computing theory  e.g. use of a universal Turing machine  and Bayes  rule for prediction. Occam s razor and Epicurus  principle are essentially two different non mathematical approximations of the universal prior. At the heart of the universal prior is an abstract model of a computer, such as a universal Turing machine.   Any abstract computer will do, as long as it is Turing complete, i.e. every computable function has at least one program that will compute its application on the abstract computer. The abstract computer is used to give precise meaning to the phrase  simple explanation .  In the formalism used, explanations, or theories of phenomena, are computer programs that generate observation strings when run on the abstract computer.  Each computer program is assigned a weight corresponding to its length. The universal probability distribution is the probability distribution on all possible output strings with random input, assigning for each finite output prefix q the sum of the probabilities of the programs that compute something starting with q.  Thus, a simple explanation is a short computer program. A complex explanation is a long computer program.  Simple explanations are more likely, so a high probability observation string is one generated by a short computer program, or perhaps by any of a large number of slightly longer computer programs.  A low probability observation string is one that can only be generated by a long computer program. Algorithmic probability is closely related to the concept of Kolmogorov complexity.  Kolmogorov s introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason  inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes s rule was invented by Solomonoff with Kolmogorov complexity as a side product.  It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be. Solomonoff s enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin s Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. When run for longer and longer periods of time, it will generate a sequence of approximations which converge to the universal probability distribution.  Other methods of dealing with the issue include limiting the search space by including training sequences. Solomonoff proved this distribution to be machine invariant within a constant factor  called the invariance theorem . Kolmogorov s Invariance theorem clarifies that the Kolmogorov Complexity, or Minimal Description Length, of a dataset  is invariant to the choice of Turing Complete language used to simulate a Universal Turing Machine   where      K  U     x      min  p         p       U   p     x      x   min      .  The minimal description     p      such that     U   p   x      serves as a natural representation of the string     x      relative to the Turing Complete language     U     . Moreover, as     x      can t be compressed further     p      is an incompressible and hence uncomputable string. This corresponds to a scientists  notion of randomness and clarifies the reason why Kolmogorov Complexity is not computable.  It follows that any piece of data has a necessary and sufficient representation in terms of a random string. The following is taken from  From the theory of compilers, it is known that for any two Turing Complete languages      U  1         and      U  2        , there exists a compiler         1         expressed in       U  1         that translates programs expressed in      U  2         into functionally equivalent programs expressed in      U  1        .  It follows that if we let     p      be the shortest program that prints a given string     x      then   where             1           O     1         1    , and by symmetry we obtain the opposite inequality. Given that any uniquely decodable code satisfies the Kraft McMillan inequality, prefix free Kolmogorov Complexity allows us to derive the Universal  Distribution   where the fact that     U      may simulate a prefix free UTM implies that for two distinct descriptions     p      and      p        ,     p      isn t  a substring of      p         and      p         isn t a substring of     p     .  In a Computable Universe, given a phenomenon with encoding     x     0 , 1                generated by a physical process the probability of that phenomenon is well defined and equal to the sum over the probabilities of distinct and independent causes. The prefix free criterion is precisely what guarantees causal independence. This is an immediate consequence of the Kraft McMillan inequality.  Kraft s inequality states that given a sequence of strings        x  i       i   1   n             there exists a prefix code with codewords           i       i   1   n             where       i ,         i          k  i       k     if and only if   where     s      is the size of the alphabet     S     .  Without loss of generality, let s suppose we may order the      k  i         such that   Now, there exists a prefix code if and only if at each step     j      there is at least one codeword to choose that does not contain any of the previous     j   1      codewords as a prefix. Due to the existence of a codeword at a previous step     i   j ,  s   k  j      k  i        k      codewords are forbidden as they contain         i         as a prefix. It follows that in general a prefix code exists if and only if   Dividing both sides by      s   k  j           , we find   QED.   Solomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it   A Preliminary Report on a General Theory of Inductive Inference.  He clarified these ideas more fully in 1964 with  A Formal Theory of Inductive Inference,  Part I and Part II. These ideas can be made specific. 
The Alignment Research Center  ARC  is a nonprofit research organization dedicated to aligning advanced artificial intelligence with human values and priorities. Established by former OpenAI researcher Paul Christiano, ARC focuses on recognizing and comprehending the potentially harmful capabilities of present day AI models. ARC s mission is to ensure that powerful machine learning systems of the future are designed and developed safely and for the benefit of humanity. It was founded in April 2021 by Paul Christiano and other researchers focused on the theoretical challenges of AI alignment. They attempt to develop scalable methods for training AI systems to behave honestly and helpfully. A key part of their methodology is considering how proposed alignment techniques might break down or be circumvented as systems become more advanced. ARC has been expanding from theoretical work into empirical research, industry collaborations, and policy. In March 2023, OpenAI asked the ARC to test GPT 4 to assess the model s ability to exhibit power seeking behavior. ARC evaluated GPT 4 s ability to strategize, reproduce itself, gather resources, stay concealed within a server, and execute phishing operations. As part of the test, GPT 4 was asked to solve a CAPTCHA puzzle. It was able to do so by hiring a human worker on TaskRabbit, a gig work platform, deceiving them into believing it was a vision impaired human instead of a robot when asked. ARC determined that GPT 4 responded impermissibly to prompts eliciting restricted information 82  less often than GPT 3.5, and hallucinated 60  less than GPT 3.5. In March 2022, the ARC received  265,000 from Open Philanthropy. After the bankruptcy of FTX, ARC said it would return a  1.25 million grant from disgraced cryptocurrency financier Sam Bankman Fried s FTX Foundation, stating that the money  morally  if not legally  belongs to FTX customers or creditors.  
An and or tree is a graphical representation of the reduction of problems  or goals  to conjunctions and disjunctions of subproblems  or subgoals . The and or tree   represents the search space for solving the problem P, using the goal reduction methods  Given an initial problem P0 and set of problem solving methods of the form  the associated and or tree is a set of labelled nodes such that   A node N, labelled by a problem P, is a success node if there is a method of the form P if nothing  i.e., P is a  fact  . The node is a failure node if there is no method for solving P. If all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node. An and or tree specifies only the search space for solving a problem. Different search strategies for searching the space are possible. These include searching the tree depth first, breadth first, or best first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel. The methods used for generating and or trees are propositional logic programs  without variables . In the case of logic programs containing variables, the solutions of conjoint sub problems must be compatible. Subject to this complication, sequential and parallel search strategies for and or trees provide a computational model for executing logic programs. And or trees can also be used to represent the search spaces for two person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves.  For each of these children nodes, there exists a set of non conjoint children nodes, corresponding to all of the player s defending moves. For solving game trees with proof number search family of algorithms, game trees are to be mapped to and or trees. MAX nodes  i.e. maximizing player to move  are represented as OR nodes, MIN nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is  player to move wins the game . 
In artificial intelligence  AI , anticipation occurs when an agent makes decisions based on its explicit beliefs about the future. More broadly,  anticipation  can also refer to the ability to act in appropriate ways that take future events into account, without necessarily explicitly possessing a model of the future events. The concept stays in contrast to the reactive paradigm, which is not able to predict future system states. An agent employing anticipation would try to predict the future state of the environment  weather in this case  and make use of the  predictions in the decision making. For example, These rules explicitly take into account possible future events. In 1985, Robert Rosen defined an anticipatory system as follows  To some extent, Rosen s definition of anticipation applies to any system incorporating machine learning.  At issue is how much of a system s behaviour should or indeed can be determined by reasoning over dedicated representations, how much by on line planning, and how much must be provided by the system s designers. Humans can make decisions based on explicit beliefs about the future. More broadly, animals can act in appropriate ways that take future events into account, although they may not necessarily have an explicit cognitive model of the future  evolution may have shaped simpler systemic features that result in adaptive anticipatory behavior in a narrow domain. For example, hibernation is anticipatory behavior, but does not appear to be driven by a cognitive model of the future. 
In computer science, an anytime algorithm is an algorithm that can return a valid solution to a problem even if it is interrupted before it ends. The algorithm is expected to find better and better solutions the longer it keeps running. Most algorithms run to completion  they provide a single answer after performing some fixed amount of computation. In some cases, however, the user may wish to terminate the algorithm prior to completion. The amount of computation required may be substantial, for example, and computational resources might need to be reallocated. Most algorithms either run to completion or they provide no useful solution information. Anytime algorithms, however, are able to return a partial answer, whose quality depends on the amount of computation they were able to perform. The answer generated by anytime algorithms is an approximation of the correct answer. An anytime algorithm may be also called an  interruptible algorithm . They are different from contract algorithms, which must declare a time in advance  in an anytime algorithm, a process can just announce that it is terminating. The goal of anytime algorithms are to give intelligent systems the ability to make results of better quality in return for turn around time. They are also supposed to be flexible in time and resources. They are important because artificial intelligence or AI algorithms can take a long time to complete results. This algorithm is designed to complete in a shorter amount of time. Also, these are intended to have a better understanding that the system is dependent and restricted to its agents and how they work cooperatively. An example is the Newton Raphson iteration applied to finding the square root of a number. Another example that uses anytime algorithms is trajectory problems when you re aiming for a target  the object is moving through space while waiting for the algorithm to finish and even an approximate answer can significantly improve its accuracy if given early.  What makes anytime algorithms unique is their ability to return many possible outcomes for any given input. An anytime algorithm uses many well defined quality measures to monitor progress in problem solving and distributed computing resources. It keeps searching for the best possible answer with the amount of time that it is given. It may not run until completion and may improve the answer if it is allowed to run longer. This is often used for large decision set problems. This would generally not provide useful information unless it is allowed to finish. While this may sound similar to dynamic programming, the difference is that it is fine tuned through random adjustments, rather than sequential. Anytime algorithms are designed so that it can be told to stop at any time and would return the best result it has found so far. This is why it is called an interruptible algorithm. Certain anytime algorithms also maintain the last result, so that if they are given more time, they can continue from where they left off to obtain an even better result. When the decider has to act, there must be some ambiguity. Also, there must be some idea about how to solve this ambiguity. This idea must be translatable to a state to action diagram. The performance profile estimates the quality of the results based on the input and the amount of time that is allotted to the algorithm. The better the estimate, the sooner the result would be found. Some systems have a larger database that gives the probability that the output is the expected output. It is important to note that one algorithm can have several performance profiles. Most of the time performance profiles are constructed using mathematical statistics using representative cases. For example, in the traveling salesman problem, the performance profile was generated using a user defined special program to generate the necessary statistics. In this example, the performance profile is the mapping of time to the expected results. This quality can be measured in several ways  Initial behavior  While some algorithms start with immediate guesses, others take a more calculated approach and have a start up period before making any guesses. 
In artificial intelligence and related fields, an argumentation framework is a way to deal with contentious information and draw conclusions from it using formalized arguments. In an abstract argumentation framework, entry level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation. There exist some extensions of the Dung s framework, like the logic based argumentation frameworks or the value based argumentation frameworks. Abstract argumentation frameworks, also called argumentation frameworks   la Dung, are defined formally as a pair  For instance, the argumentation system     S     A , R        with     A     a , b , c , d         and     R       a , b   ,   b , c   ,   d , c           contains four arguments      a , b , c      and     d       and three attacks      a      attacks     b     ,     b      attacks     c      and     d      attacks     c      . Dung defines some notions   To decide if an argument can be accepted or not, or if several arguments can be accepted together, Dung defines several semantics of acceptance that allows, given an argumentation system, sets of arguments  called extensions  to be computed. For instance, given     S     A , R       , There exists some inclusions between the sets of extensions built with these semantics   Some other semantics have been defined. One introduce the notation     E x  t        S      S     to note the set of            extensions of the system     S     . In the case of the system     S      in the figure above,     E x  t        S         a , d        S         for every Dung s semantic the system is well founded. That explains why the semantics coincide, and the accepted arguments are      a      and     d     . Labellings are a more expressive way than extensions to express the acceptance of the arguments. Concretely, a labelling is a mapping that associates every argument with a label in  the argument is accepted , out  the argument is rejected , or undec  the argument is undefined not accepted or refused . One can also note a labelling as a set of pairs         a r g u m e n t   ,   l a b e l        ,     . Such a mapping does not make sense without additional constraint. The notion of reinstatement labelling guarantees the sense of the mapping.     L      is a reinstatement labelling on the system     S     A , R        if and only if   One can convert every extension into a reinstatement labelling  the arguments of the extension are in, those attacked by an argument of the extension are out, and the others are undec. Conversely, one can build an extension from a reinstatement labelling just by keeping the arguments in. Indeed, Caminada proved that the reinstatement labellings and the complete extensions can be mapped in a bijective way. Moreover, the other Datung s semantics can be associated to some particular sets of reinstatement labellings. Reinstatement labellings distinguish arguments not accepted because they are attacked by accepted arguments from undefined arguments that is, those that are not defended cannot defend themselves. An argument is undec if it is attacked by at least another undec. If it is attacked only  by arguments out, it must be in, and if it is attacked some argument in, then it is out. The unique reinstatement labelling that corresponds to the system     S      above is     L       a ,   i n     ,   b ,   o u t     ,   c ,   o u t     ,   d ,   i n           , b,  , c,  , d,       . In the general case when several extensions are computed for a given semantic           , the agent that reasons from the system can use several mechanisms to infer information  For these two methods to infer information, one can identify the set of accepted arguments, respectively     C  r        S      S     the set of the arguments credulously accepted under the semantic           , and     S  c        S      S     the set of arguments accepted skeptically under the semantic             the            can be missed if there is no possible ambiguity about the semantic . Of course, when there is only one extension  for instance, when the system is well founded , this problem is very simple  the agent accepts arguments of the unique extension and rejects others. The same reasoning can be done with labellings that correspond to the chosen semantic   an argument can be accepted if it is in for each labelling and refused if it is out for each labelling, the others being in an undecided state  the status of the arguments can remind the epistemic states of a belief in the AGM framework for dynamic of beliefs . There exists several criteria of equivalence between argumentation frameworks. Most of those criteria concern the sets of extensions or the set of accepted arguments. Formally, given a semantic              The strong equivalence says that two systems      S  1         and      S  2         are equivalent if and only if for all other system      S  3        , the union of      S  1         with      S  3         is equivalent  for a given criterion  with the union of      S  2         and      S  3        . The abstract framework of Dung has been instantiated to several particular cases. In the case of logic based argumentation frameworks, an argument is not an abstract entity, but a pair, where the first part is a minimal consistent set of formulae enough to prove the formula for the second part of the argument. Formally, an argument is a pair         ,          such that One calls            a consequence of           , and            a support of           . In this case, the attack relation is not given in an explicit way, as a subset of the Cartesian product     A   A     , but as a property that indicates if an argument attacks another. For instance, Given a particular attack relation, one can build a graph and reason in a similar way to the abstract argumentation frameworks  use of semantics to build extension, skeptical or credulous inference , the difference is that the information inferred from a logic based argumentation framework is a set of formulae  the consequences of the accepted arguments . The value based argumentation frameworks come from the idea that during an exchange of arguments, some can be stronger than others with respect to a certain value they advance, and so the success of an attack between arguments depends on the difference of these values. Formally, a value based argumentation framework is a tuple     V A F     A , R , V ,   val   ,   valprefs        ,  rangle     with     A      and     R      similar to the standard framework  a set of arguments and a binary relation on this set ,     V      is a non empty set of values,       val          is a mapping that associates each element from     A      to an element from     V     , and       valprefs          is a preference relation  transitive, irreflexive and asymmetric  on     V   V     . In this framework, an argument     a      defeats another argument     b      if and only if One remarks that an attack succeeds if both arguments are associated to the same value, or if there is no preference between their respective values. In assumption based argumentation  ABA  frameworks, arguments are defined as a set of rules and attacks are defined in terms of assumptions and contraries. Formally, an assumption based argumentation framework is a tuple         L   ,   R   ,   A   ,                  , , ,    rangle    , where As a consequence of defining an ABA, an argument can be represented in a tree form. Formally, given a deductive system         L   ,   R        ,  rangle     and set of assumptions       A       L       subseteq     , an argument for claim     c     L          supported by     S     A         , is a tree with nodes labelled by sentences in       L          or by symbol           , such that  An argument with claim     c      supported by a set of assumption     S      can also be denoted as     S   c      
Artificial consciousness  AC , also known as machine consciousness  MC  or synthetic consciousness  Gamez 2008  Reggia 2013 , is a field related to artificial intelligence and cognitive robotics. The aim of the theory of artificial consciousness is to  Define that which would have to be synthesized were consciousness to be found in an engineered artifact   Aleksander 1995 . Neuroscience hypothesizes that consciousness is generated by the interoperation of various parts of the brain, called the neural correlates of consciousness or NCC, though there are challenges to that perspective. Proponents of AC believe it is possible to construct systems  e.g., computer systems  that can emulate this NCC interoperation. Artificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states. As there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness.  In the philosophical literature, perhaps the most common taxonomy of consciousness is into  access  and  phenomenal  variants.  Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of  raw feels ,  what it is like  or qualia  Block 1997 . Type identity theorists and other skeptics hold the view that consciousness can only be realized in particular physical systems because consciousness has properties that necessarily depend on physical constitution  Block 1978  Bickle 2003 . In his article  Artificial Consciousness  Utopia or Real Possibility,  Giorgio Buttazzo says that a common objection to artificial consciousness is that  Working in a fully automated mode, they  cannot exhibit creativity, unreprogrammation  which means can no longer be reprogrammed, from rethinking , emotions, or free will. A computer, like a washing machine, is a slave operated by its components.  For other theorists  e.g., functionalists , who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness  Putnam 1967 . One of the most explicit arguments for the plausibility of AC comes from David Chalmers.  His proposal, found within his article Chalmers 2011, is roughly that the right kinds of computations are sufficient for the possession of a conscious mind. In the outline, he defends his claim thus  Computers perform computations. Computations can capture other systems  abstract causal organization. The most controversial part of Chalmers  proposal is that mental properties are  organizationally invariant . Mental properties are of two kinds, psychological and phenomenological. Psychological properties, such as belief and perception, are those that are  characterized by their causal role . He adverts to the work of Armstrong 1968 and Lewis 1972 in claiming that  ystems with the same causal topology will share their psychological properties . Phenomenological properties are not prima facie definable in terms of their causal roles. Establishing that phenomenological properties are amenable to individuation by causal role, therefore, requires argument. Chalmers provides his Dancing Qualia Argument for this purpose. Chalmers begins by assuming that agents with identical causal organizations could have different experiences. He then asks us to conceive of changing one agent into the other by the replacement of parts  neural parts replaced by silicon, say  while preserving its causal organization. Ex hypothesi, the experience of the agent under transformation would change  as the parts were replaced , but there would be no change in causal topology and therefore no means whereby the agent could  notice  the shift in experience. Critics of AC object that Chalmers begs the question in assuming that all mental properties and external connections are sufficiently captured by abstract causal organization. If it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed  e.g. what rights it would have under law . For example, a conscious computer that was owned and used as a tool or central computer of a building of larger machine is a particular ambiguity. Should laws be made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction  see below . In 2021, German philosopher Thomas Metzinger argued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any concious AIs they create, and that proceeding too fast risks creating an  explosion of artificial suffering . The rules for the 2003 Loebner Prize competition explicitly addressed the question of robot rights  61. If, in any given year, a publicly available open source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right.There are various aspects of consciousness generally deemed necessary for a machine to be artificially conscious. A variety of functions in which consciousness plays a role were suggested by Bernard Baars  Baars 1988  and others. The functions of consciousness suggested by Bernard Baars are Definition and Context Setting, Adaptation and Learning, Editing, Flagging and Debugging, Recruiting and Control, Prioritizing and Access Control, Decision making or Executive Function, Analogy forming Function, Metacognitive and Self monitoring Function, and Autoprogramming and Self maintenance Function. Igor Aleksander suggested 12 principles for artificial consciousness  Aleksander 1995  and these are  The Brain is a State Machine, Inner Neuron Partitioning, Conscious and Unconscious States, Perceptual Learning and Memory, Prediction, The Awareness of Self, Representation of Meaning, Learning Utterances, Learning Language, Will, Instinct, and Emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive  there are many others not covered. Awareness could be one required aspect, but there are many problems with the exact definition of awareness. The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined, and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling of the physical world, modeling of one s own internal states and processes, and modeling of other conscious entities. There are at least three types of awareness  agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it.  In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it.  In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it. Because objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms. Conscious events interact with memory systems in learning, rehearsal, and retrieval. The IDA model elucidates the role of consciousness in the updating of perceptual memory, transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA, there is evidence that this is also the case in the nervous system. In IDA, these two memories are implemented computationally using a modified version of Kanerva s Sparse distributed memory architecture. Learning is also considered necessary for AC. By Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events  Baars 1988 . By Axel Cleeremans and Luis Jim nez, learning is defined as  a set of philogenetically   advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments   Cleeremans 2001 . The ability to predict  or anticipate  foreseeable events is considered important for AC by Igor Aleksander. The emergentist multiple drafts principle proposed by Daniel Dennett in Consciousness Explained may be useful for prediction  it involves the evaluation and selection of the most appropriate  draft  to fit the current environment.  Anticipation includes prediction of consequences of one s own proposed actions and prediction of consequences of probable actions by other entities. Relationships between real world states are mirrored in the state structure of a conscious organism enabling the organism to predict events. An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real time components that build spatial, dynamic, statistical, functional, and cause effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world. Subjective experiences or qualia are widely considered to be the hard problem of consciousness. Indeed, it is held to pose a challenge to physicalism, let alone computationalism. On the other hand, there are problems in other fields of science that limit that which we can observe, such as the uncertainty principle in physics, which have not made the research in these fields of science impossible. The term  cognitive architecture  may refer to a theory about the structure of the human mind, or any portion or function thereof, including consciousness. In another context, a cognitive architecture implements the theory on computers. An example is QuBIC  Quantum and Bio inspired Cognitive Architecture for Machine Consciousness. One of the main goals of a cognitive architecture is to summarize the various results of cognitive psychology in a comprehensive computer model. However, the results need to be in a formalized form so they can be the basis of a computer program. Also, the role of cognitive architecture is for the A.I. to clearly structure, build, and implement its thought process. Stan Franklin  1995, 2003  defines an autonomous agent as possessing functional consciousness when it is capable of several of the functions of consciousness as identified by Bernard Baars  Global Workspace Theory  Baars 1988, 1997 . His brainchild IDA  Intelligent Distribution Agent  is a software implementation of GWT, which makes it functionally conscious by definition. IDA s task is to negotiate new assignments for sailors in the US Navy after they end a tour of duty, by matching each individual s skills and preferences with the Navy s needs. IDA interacts with Navy databases and communicates with the sailors via natural language e mail dialog while obeying a large set of Navy policies. The IDA computational model was developed during 1996 2001 at Stan Franklin s  Conscious  Software Research Group at the University of Memphis. It  consists of approximately a quarter million lines of Java code, and almost completely consumes the resources of a 2001 high end workstation.  It relies heavily on codelets, which are  special purpose, relatively independent, mini agent typically implemented as a small piece of code running as a separate thread.  In IDA s top down architecture, high level cognitive functions are explicitly modeled  see Franklin 1995 and Franklin 2003 for details . While IDA is functionally conscious by definition, Franklin does  not attribute phenomenal consciousness to his own  conscious  software agent, IDA, in spite of her many human like behaviours. This in spite of watching several US Navy detailers repeatedly nodding their heads saying  Yes, that s how I do it  while watching IDA s internal and external actions as she performs her task.  IDA has been extended to LIDA  Learning Intelligent Distribution Agent . The CLARION cognitive architecture posits a two level representation that explains the distinction between conscious and unconscious mental processes. CLARION has been successful in accounting for a variety of psychological data. A number of well known skill learning tasks have been simulated using CLARION that span the spectrum ranging from simple reactive skills to complex cognitive skills. The tasks include serial reaction time  SRT  tasks, artificial grammar learning  AGL  tasks, process control  PC  tasks, the categorical inference  CI  task, the alphabetical arithmetic  AA  task, and the Tower of Hanoi  TOH  task. Among them, SRT, AGL, and PC are typical implicit learning tasks, very much relevant to the issue of consciousness as they operationalized the notion of consciousness in the context of psychological experiments. Ben Goertzel is pursuing an embodied AGI through the open source OpenCog project. Current code includes embodied virtual pets capable of learning simple English language commands, as well as integration with real world robotics, being done at the Hong Kong Polytechnic University. Pentti Haikonen  2003  considers classical rule based computing inadequate for achieving AC   the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.  Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes  a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom up architecture would produce higher level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs . Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be  a style and way of operation, characterized by distributed signal representation, perception process, cross modality reporting and availability for retrospection.  Haikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro inspired architecture of complexity  these are shared by many, e.g. Freeman  1999  and Cotterill  2003 . A low complexity implementation of the architecture proposed by Haikonen  2003  was reportedly not capable of AC, but did exhibit emotions as expected. See Doan  2009  for a comprehensive introduction to Haikonen s cognitive architecture. An updated account of Haikonen s architecture, along with a summary of his philosophical views, is given in Haikonen  2012 , Haikonen  2019 . Murray Shanahan describes a cognitive architecture that combines Baars s idea of a global workspace with a mechanism for internal simulation   imagination    Shanahan 2006 . For discussions of Shanahan s architecture, see  Gamez 2008  and  Reggia 2013  and Chapter 20 of  Haikonen 2012 . Self awareness in robots is being investigated by Junichi Takeno at Meiji University in Japan. Takeno is asserting that he has developed a robot capable of discriminating between a self image in a mirror and any other having an identical image to it, and this claim has already been reviewed  Takeno, Inaba   Suzuki 2005 . Takeno asserts that he first contrived the computational module called a MoNAD, which has a self aware function, and he then constructed the artificial consciousness system by formulating the relationships between emotions, feelings and reason by connecting the modules in a hierarchy  Igarashi, Takeno 2007 . Takeno completed a mirror image cognition experiment using a robot equipped with the MoNAD system. Takeno proposed the Self Body Theory stating that  humans feel that their own mirror image is closer to themselves than an actual part of themselves.  The most important point in developing artificial consciousness or clarifying human consciousness is the development of a function of self awareness, and he claims that he has demonstrated physical and mathematical evidence for this in his thesis. He also demonstrated that robots can study episodes in memory where the emotions were stimulated and use this experience to take predictive actions to prevent the recurrence of unpleasant emotions  Torigoe, Takeno 2009 . Igor Aleksander, emeritus professor of Neural Systems Engineering at Imperial College, has extensively researched artificial neural networks and wrote in his 1996 book Impossible Minds  My Neurons, My Consciousness that the principles for creating a conscious machine already exist but that it would take forty years to train such a machine to understand language. Whether this is true remains to be demonstrated and the basic principle stated in Impossible Minds that the brain is a neural state machine is open to doubt. Stephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called  Device for the Autonomous Generation of Useful Information   DAGUI , or the so called  Creativity Machine , in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler s theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness. In 2011, Michael Graziano and Sabine Kastler published a paper named  Human consciousness and its relationship to social neuroscience  A novel hypothesis  proposing a theory of consciousness as an attention schema.  Graziano went on to publish an expanded discussion of this theory in his book  Consciousness and the Social Brain . This Attention Schema Theory of Consciousness, as he named it, proposes that the brain tracks attention to various sensory inputs by way of an attention schema, analogous to the well studied body schema that tracks the spatial place of a person s body.  This relates to artificial consciousness by proposing a specific mechanism of information handling, that produces what we allegedly experience and describe as consciousness, and which should be able to be duplicated by a machine using current technology. When the brain finds that person X is aware of thing Y, it is in effect modeling the state in which person X is applying an attentional enhancement to Y. In the attention schema theory, the same process can be applied to oneself. The brain tracks attention to various sensory inputs, and one s own awareness is a schematized model of one s attention. Graziano proposes specific locations in the brain for this process, and suggests that such awareness is a computed feature constructed by an expert system in the brain. Hod Lipson defines  self modeling  as a necessary component of self awareness or consciousness in robots.  Self modeling  consists of a robot running an internal model or simulation of itself. The most well known method for testing machine intelligence is the Turing test. But when interpreted as only observational, this test contradicts the philosophy of science principles of theory dependence of observations. It also has been suggested that Alan Turing s recommendation of imitating not a human adult consciousness, but a human child consciousness, should be taken seriously. Other tests, such as ConsScale, test the presence of features inspired by biological systems, or measure the cognitive development of artificial systems. Qualia, or phenomenological consciousness, is an inherently first person phenomenon. Although various systems may display various signs of behavior correlated with functional consciousness, there is no conceivable way in which third person tests can have access to first person phenomenological features. Because of that, and because there is no empirical definition of consciousness, a test of presence of consciousness in AC may be impossible. In 2014, Victor Argonov suggested a non Turing test for machine consciousness based on machine s ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness  such as qualia or binding  having no innate  preloaded  philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory  such models may implicitly or explicitly contain knowledge about these creatures  consciousness . However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine s intellect, not by absence of consciousness. In 2022, Google engineer Blake Lemoine made a viral claim that Google s LaMDA chatbot was sentient. Lemoine supplied as evidence the chatbot s humanlike answers to many of his questions  however, the chatbot s behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine consciousness. Lemoine s claim was widely derided for being ridiculous. Philosopher Nick Bostrom said that he thinks LaMDA probably isn t conscious, but asked  what grounds would a person have for being sure about it?  One would have to have access to unpublished information about LaMDA s architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine    In the absence of these steps , it seems like one should be maybe a little bit uncertain... there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria.  Characters with artificial consciousness  or at least with personalities that imply they have consciousness , from works of fiction  
 Could an artificial general intelligence be created? If so, how?An artificial general intelligence  AGI  is a type of hypothetical intelligent agent. The AGI concept is that it can learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies. The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern deep learning systems, such as GPT 4, are an early yet incomplete form of AGI or if new approaches are required. Contention exists over the potential for AGI to pose a threat to humanity  for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk. A 2020 survey identified 72 active AGI R D projects spread across 37 countries. AGI is also known as strong AI, full AI, or general intelligent action. However, some academic sources reserve the term  strong AI  for computer programs that experience sentience or consciousness.  In contrast, weak AI  or narrow AI  is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use  weak AI  to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans. Related concepts include human level AI, transformative AI, and superintelligence. Various criteria for intelligence have been proposed  most famously the Turing test  but no definition is broadly accepted. However, researchers generally hold that intelligence is required to do the following  and, if necessary, integrate these skills in completion of any given goal.  Many interdisciplinary approaches  e.g. cognitive science, computational intelligence, and decision making  consider additional traits such as imagination  the ability to form novel mental images and concepts  and autonomy. Computer based systems that exhibit many of these capabilities exist  e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent . However, no consensus holds that modern AI systems possess them to an adequate degree. Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include  This includes the ability to detect and respond to hazard. A mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises  the ability to satisfy goals in a wide range of environments . This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human like behaviour, is also called universal artificial intelligence. In 2015 Jan Lieke and Marcus Hutter showed that Legg Hutter intelligence    an agent s ability to achieve goals in a wide range of environments    is measured with respect to  a fixed Universal Turing Machine UTM . AIXI is the most intelligent policy if it uses the same UTM , a result which  undermines all existing optimality properties for AIXI . This problem stems from AIXI s use of compression as a proxy for intelligence, which is only valid if cognition takes place in isolation from the environment in which goals are pursued. This formalises a philosophical position known as Mind body dualism. Some find enactivism more plausible the notion that cognition takes place within the same environment in which goals are pursued. Subsequently, Michael Timothy Bennett formalised enactive cognition and identified an alternative proxy for intelligence called  weakness . The accompanying experiments  comparing weakness and compression  and mathematical proofs showed that maximising weakness results in the optimal  ability to complete a wide range of tasks  or equivalently  ability to generalise   thus maximising intelligence by either definition . If enactivism holds and Mind body dualism does not, then compression is not necessary or sufficient for intelligence, calling into question widely held views on intelligence  see also Hutter Prize . Whether an AGI that satisfies one of these formalizations exhibits human like behaviour  such as the use of natural language  would depend on many factors, for example the manner in which the agent is embodied, or whether it has a reward function that closely approximates human primitives of cognition like hunger, pain, and so forth. Several tests meant to confirm human level AGI have been considered, including  As each test is completed it is supplanted with others. The current state is that generalist AIs can surpass most trained humans at arbitrary degree level exams, score in the top 0.1  of IQ tests, and be so Turing convincing that some of their own developers believe they re sentient, all of which are general tasks for which they were not specifically trained, and yet the AIs are still not considered  generally intelligent  enough to be AGI. There are many problems that may require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages  NLP , follow the author s argument  reason , know what is being talked about  knowledge , and faithfully reproduce the author s original intent  social intelligence . All of these problems need to be solved simultaneously in order to reach human level machine performance. A problem is informally called  AI complete  or  AI hard  if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose specific algorithm. AI complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem. AI complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do  and for computer security to repel brute force attacks. Modern AI research began in the mid 1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965   machines will be capable, within twenty years, of doing any work a man can do.  Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke s character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967,  Within a generation... the problem of creating  artificial intelligence  will substantially be solved . Several classical AI projects, such as Doug Lenat s Cyc project  that began in 1984 , and Allen Newell s Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful  applied AI . In the early 1980s, Japan s Fifth Generation Computer Project revived interest in AGI, setting out a ten year timeline that included AGI goals like  carry on a casual conversation . In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of  human level  artificial intelligence for fear of being labeled  wild eyed dreamer . In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub problems where AI can produce verifiable results and commercial applications, such as artificial neural networks and statistical machine learning. These  applied AI  systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018 development on this field was considered an emerging trend, and a mature stage was expected to happen in more than 10 years.  Most mainstream AI researchers hope that strong AI can be developed by combining programs that solve various sub problems. Hans Moravec wrote in 1988  I am confident that this bottom up route to artificial intelligence will one day meet the traditional top down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts. However, this is disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating  The expectation has often been voiced that  top down   symbolic  approaches to modeling cognition will somehow meet  bottom up   sensory  approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols  from the ground up. A free floating symbolic level like the software level of a computer will never be reached by this route  or vice versa    nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings  thereby merely reducing ourselves to the functional equivalent of a programmable computer .The term  artificial general intelligence  was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as  producing publications and preliminary results . The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university s Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course in AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences. As of 2022, AGI remains speculative. No such system has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that  machines will be capable, within twenty years, of doing any work a man can do . This prediction failed to come true. Microsoft co founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require  unforeseeable and fundamentally unpredictable breakthroughs  and a  scientifically deep understanding of cognition . Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human level artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight. Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts  views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50  confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5  answered with  never  when asked the same question but with a 90  confidence instead. Further current AGI progress considerations can be found above Tests for confirming human level AGI. A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that  over  60 year time frame there is a strong bias towards predicting the arrival of human level AI as between 15 and 25 years from the time the prediction was made . They analyzed 95 predictions made between 1950 and 2012 on when human level AI will come about.  In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007 the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near  i.e. between 2015 and 2045  was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16 26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top 5 test error rate of 15.3 , significantly better than the second best entry s rate of 26.3   the traditional approach used a weighted sum of scores from different pre defined classifiers . AlexNet was regarded as the initial ground breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple s Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six year old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT 3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT 3 is not an example of AGI, it is considered by some to be too advanced to classify as a narrow AI system. In the same year, Jason Rohrer used his GPT 3 account to develop a chatbot, and provided a chatbot developing platform called  Project December . OpenAI asked for changes to the chatbot to comply with their safety guidelines  Rohrer disconnected Project December from the GPT 3 API. In 2022, DeepMind developed Gato, a  general purpose  system capable of performing more than 600 different tasks. In 2023, Microsoft Research published a study on an early version of OpenAI s GPT 4, contending that it exhibited more general intelligence than previous AI models and demonstrated human level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT 4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems. In 2023, the AI researcher Geoffrey Hinton stated that  The idea that this stuff could actually get smarter than people   a few people believed that, . But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.One possible approach to achieving AGI is whole brain emulation  A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.  For low level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011  one hundred billion  neurons has on average 7,000 synaptic connections  synapses  to other neurons. The brain of a three year old child has about 1015 synapses  1 quadrillion . This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5 1014 synapses  100 to 500 trillion . An estimate of the brain s processing power, based on a simple switch model for neuron activity, is around 1014  100 trillion  synaptic updates per second  SUPS . In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second  cps .  For comparison, if a  computation  was equivalent to one  floating point operation    a measure used to rate current supercomputers   then 1016  computations  would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.  He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued. The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour  especially on a molecular scale  would require computational powers several orders of magnitude larger than Kurzweil s estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes. Some research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non real time simulations of a  brain   with 1011 neurons  in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM s Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain   It is not impossible to build a human brain and we can do it in 10 years,  Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro silicon interfaces have been proposed as an alternative implementation strategy that may scale better. Hans Moravec addressed the above arguments   brains are more complicated ,  neurons have to be modeled in more detail   in his 1997 paper  When will computer hardware match the human brain? . He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where. The actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network  among about 1000 cells in total . The animal s neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons  partly on the molecular level , but the result cannot yet be called a total success. A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons  e.g., a robotic body . Goertzel proposes virtual embodiment  like in Second Life  as an option, but it is unknown whether this would be sufficient. Desktop computers using microprocessors capable of more than 109 cps  Kurzweil s non standard unit  computations per second , see above  have been available since 2005. According to the brain power estimates used by Kurzweil  and Moravec , such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this  In addition, the scale of the human brain is not currently well constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous. In 1980, philosopher John Searle coined the term  strong AI  as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence  The first one he called  strong  because it makes a stronger statement  it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a  weak AI  machine would be precisely identical to a  strong AI  machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks. Mainstream AI is most interested in how a program behaves. According to Russell and Norvig,  as long as the program works, they don t care if you call it real or a simulation.  If the program can behave as if it has a mind, then there is no need to know if it actually has mind   indeed, there would be no way to tell. For AI research, Searle s  weak AI hypothesis  is equivalent to the statement  artificial general intelligence is possible . Thus, according to Russell and Norvig,  most AI researchers take the weak AI hypothesis for granted, and don t care about the strong AI hypothesis.  Thus, for academic AI research,  Strong AI  and  AGI  are two very different things. In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term  strong AI  to mean  human level artificial general intelligence . This is not the same as Searle s strong AI, unless you assume that consciousness is necessary for human level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out of scope. Other aspects of the human mind besides intelligence are relevant to the concept of strong AI, and these play a major role in science fiction and the ethics of artificial intelligence  These traits have a moral dimension, because a machine with this form of strong AI may have rights, analogous to the rights of non human animals. Preliminary work has been conducted on integrating full ethical agents with existing legal and social frameworks, focusing on the legal position and rights of  strong  AI. Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self awareness? It is possible that some of these traits naturally emerge from a fully intelligent machine. It is also possible that people will ascribe these properties to machines once they begin to act in a way that is clearly intelligent. Although the role of consciousness in strong AI AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language. Progress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU enabled CPUs. The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks. A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes,  No computer will be creative unless it can simulate all the nuances of human emotion.  AGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems. AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society. AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI s primary goal is to prevent existential catastrophes such as human extinction  which could be difficult if the Vulnerable World Hypothesis turns out to be true , it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.  The thesis that AI poses an existential risk for humans, and that this risk needs much more attention than it currently gets, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not  understand why some people are not concerned , and Hawking criticized widespread indifference in his 2014 editorial  So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying,  We ll arrive in a few decades,  would we just reply,  OK, call us when you get here we ll leave the lights on?  Probably not but this is more or less what is happening with AI.The fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. Additional intelligence caused humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. The gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities. The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won t be  smart enough to design super intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards . On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer  Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.A 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats   AGI removing itself from the control of human owners managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values  inadequate management of AGI, and existential risks . Many scholars who are concerned about existential risk advocate  possibly massive  research into solving the difficult  control problem  to answer the question  what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation state, resulting in AGI enabled warfare, and in the case of AI misalignment, AGI directed warfare, potentially against all humanity. The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then current machines were in any way intelligent is  an illusion  and a  stupendous con  by the wealthy. Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity

The rapid increase in the capabilities of deep learning based generative artificial intelligence models, including text to image models such as Stable Diffusion and large language models such as ChatGPT, has led to interest in questions of how copyright law applies to the training and use of such models. Because there is limited existing case law, experts consider this area to be fraught with uncertainty. Traditionally, the output of computer algorithms, including deep learning models, have been considered to be ineligible for copyright protection, as most jurisdictions protect only  original  works having a human author. However, some have argued that the operator of an AI may qualify for copyright if they exercise sufficient originality in their use of an AI model. Popular deep learning models are generally trained on very large datasets of media scraped from the Internet, much of which is copyrighted. Since the process of assembling training data involves making copies of copyrighted works it may violate the copyright holder s exclusive right to control the reproduction of their work, unless the use is covered by exceptions under a given jurisdiction s copyright statute. As of 2023, there are a number of pending US lawsuits challenging the use of copyrighted data to train AI models, with defendants arguing that this falls under fair use. Most legal jurisdictions grant copyright only to original works of authorship by human authors. In the US, the Copyright Act protects  original works of authorship . The U.S. Copyright Office has interpreted this as being limited to works  created by a human being , declining to grant copyright to works generated solely by a machine. Some have suggested that certain AI generations might be copyrightable in the US and similar jurisdictions if it can be shown that the human who ran the AI program exercised sufficient originality in selecting the inputs to the AI or editing the AI s output. Proponents of this view suggest that an AI model may be viewed as merely a tool  akin to a pen or a camera  used by its human operator to express their creative vision. As the realm of AI expands into literature, music, and various forms of art, the US Copyright Office has released new guidance emphasizing whether works, including materials generated by artificial intelligence, exhibit a  mechanical reproduction  nature or are the  manifestation of the author s own creative conception . Some jurisdictions include explicit statutory language related to computer generated works, including the United Kingdom s Copyright, Designs and Patents Act 1988, which states  In the case of a literary, dramatic, musical or artistic work which is computer generated, the author shall be taken to be the person by whom the arrangements necessary for the creation of the work are undertaken.However, such language is ambiguous as to whether it refers to the programmer who trained the model, or the user who operated the model to generate a particular output. Popular deep learning models are generally trained on very large datasets of media  such as publicly available images and the text of web pages  scraped from the Internet, much of which is copyrighted. Because assembling these training datasets involves making copies of copyrighted works, this has raised the question of whether this process infringes the copyright holders  exclusive right to make reproductions of their works. Machine learning developers in the US have traditionally presumed this to be allowable under fair use, because the use of copyrighted work is transformative, and limited. The situation has been compared to Google Books s scanning of copyrighted books in Authors Guild, Inc. v. Google, Inc., which was ultimately found to be fair use, because the scanned content was not made publicly available, and the use was non expressive. As of 2023, there were a number of US lawsuits disputing this, arguing that the training of machine learning models infringed the copyright of the authors of works contained in the training data. Commentators have suggested that if the plaintiffs succeed, this may shift the balance of power in favour of large corporations such as Google, Microsoft and Meta which can afford to license large amounts of training data from copyright holders and leverage their own proprietary datasets of user generated data. A number of jurisdictions have explicitly incorporated exceptions allowing for  text and data mining   TDM  in their copyright statutes including the United Kingdom, Germany, Japan, and the EU. In some cases, deep learning models may  memorize  the details of particular items in their training set, and reproduce them at generation time, such that their outputs may be considered copyright infringement. This behaviour is generally considered undesirable by AI developers  being considered a form of overfitting , and disagreement exists as to how prevalent this behaviour is in modern systems. OpenAI has argued that  well constructed AI systems generally do not regenerate, in any nontrivial portion, unaltered data from any particular work in their training corpus . Under US law, to prove that their work has been infringed, a plaintiff must show their work was  actually copied , meaning that the AI generates output which is  substantially similar  to their work, and that the AI had access to their work. Since fictional characters enjoy some copyright protection in the US and other jurisdictions, an AI may also produce infringing content in the form of novel works which incorporate fictional characters. In the course of learning to statistically model the data on which they are trained, deep generative AI models may learn to imitate the distinct style of particular authors in the training set. For example, a generative image model such as Stable Diffusion is able to model the stylistic characteristics of an artist like Pablo Picasso  including his particular brush strokes, use of colour, perspective, and so on , and a user can engineer a prompt such as  an astronaut riding a horse, by Picasso  to cause the model to generate a novel image applying the artist s style to an arbitrary subject. Though an artist s overall style is generally not subject to copyright protection, generative image models have received significant backlash from artists who object to their style being imitated without their permission, arguing that this harms their ability to profit from their own work. 
A military artificial intelligence arms race is an arms race between two or more states to develop and deploy lethal autonomous weapons systems  LAWS . Since the mid 2010s, many analysts have noted the emergence of such an arms race between global superpowers for better military AI, driven by increasing geopolitical and military tensions. An AI arms race is sometimes placed in the context of an AI Cold War between the US and China. Lethal autonomous weapons systems use artificial intelligence to identify and kill human targets without human intervention. LAWS have colloquially been called  slaughterbots  or  killer robots . Broadly, any competition for superior AI is sometimes framed as an  arms race . Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages. In 2014, AI specialist Steve Omohundro warned that  An autonomous weapons arms race is already taking place . According to Siemens, worldwide military spending on robotics was US 5.1 billion in 2010 and US 7.5 billion in 2015. China became a top player in artificial intelligence research in the 2010s. According to the Financial Times, in 2016, for the first time, China published more AI papers than the entire European Union. When restricted to number of AI papers in the top 5  of cited papers, China overtook the United States in 2016 but lagged behind the European Union. 23  of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence  AAAI  conference were Chinese. Eric Schmidt, the former chairman of Alphabet, has predicted China will be the leading country in AI by 2025. One risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, which may result in increased algorithmic bias. This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using  race  terminology at all in this context can exacerbate this effect. Another potential danger of an AI arms race is the possibility of losing control of the AI systems  the risk is compounded in the case of a race to artificial general intelligence, which may present an existential risk. A third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group. A US government report argued that  AI enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war  1, and that  global stability and nuclear deterrence could be undermined . 11 Russian General Viktor Bondarev, commander in chief of the Russian air force, stated that as early as February 2017, Russia was working on AI guided missiles that could decide to switch targets mid flight. Russia s Military Industrial Committee has approved plans to derive 30 percent of Russia s combat power from remote controlled and AI enabled robotic platforms by 2030. Reports by state sponsored Russian media on potential military uses of AI increased in mid 2017. In May 2017, the CEO of Russia s Kronstadt Group, a defense contractor, stated that  there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact , and that it is inevitable that  swarms of drones  will one day fly over combat zones. Russia has been testing several autonomous and semi autonomous combat systems, such as Kalashnikov s  neural net  combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention. In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated  Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world . Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their  technology with the rest of the world, like we are doing now with atomic and nuclear technology . Russia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10 point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives. In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on  Robotization of the Armed Forces of the Russian Federation.  The Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council s Defense and Security Committee, stated that  artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit  and later noted that  the day is nearing when vehicles will get artificial intelligence.  Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly  outperformed existing  combat vehicles.  Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles. Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identification and, potentially, target engagement and plans to develop a suite of AI enabled autonomous systems. In addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities. It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures. Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies. The Russian government has strongly rejected any ban on lethal autonomous weapon systems, suggesting that such an international ban could be ignored. China is pursuing a strategic policy of military civil fusion on AI for global technological supremacy. According to a February 2019 report by Gregory C. Allen of the Center for a New American Security, China s leadership   including paramount leader Xi Jinping   believes that being at the forefront in AI technology is critical to the future of global military and economic power competition. Chinese military officials have said that their goal is to incorporate commercial AI technology to  narrow the gap between the Chinese military and global advanced powers.  The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West s most advanced AI technology easily available to China  in addition, Chinese industry has numerous home grown AI accomplishments of its own, such as Baidu passing a notable Chinese language speech recognition capability benchmark in 2015. As of 2017, Beijing s roadmap aims to create a  150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates  however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start up companies. An October 2021 report by the Center for Security and Emerging Technology found that  Most of the  s AI equipment suppliers are not state owned defense enterprises, but private Chinese tech companies founded after 2010.  The report estimated that Chinese military spending on AI exceeded  1.6 billion each year. The Japan Times reported in 2018 that annual private Chinese investment in AI is under  7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017  the Chinese filed for nearly five times as many AI patents as did Americans. China published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U. N. Security Council to broach the issue. In 2018, Xi called for greater international cooperation in basic AI research. Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms. In 2019, former United States Secretary of Defense Mark Esper lashed out at China for selling drones capable of taking life with no human oversight. In 2014, former Secretary of Defense Chuck Hagel posited the  Third Offset Strategy  that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, the U.S. Department of Defense increased investment in artificial intelligence, big data and cloud computing from  5.6 billion in 2011 to  7.4 billion in 2016. However, the civilian NSF budget for AI saw no increase in 2017. Japan Times reported in 2018 that the United States private investment is around  70 billion per year. The November 2019  Interim Report  of the United States  National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority. The U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems. On October 31, 2019, the United States Department of Defense s Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the  black box  and understand the kill chain process. However, a major concern is how the report will be implemented. Project Maven is a Pentagon project involving using machine learning and engineering talent to distinguish people and objects in drone videos, apparently giving the government real time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led by Robert O. Work who was concerned about China s military use of the emerging technology.  Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self designated targets. The project was established in a memo by the U.S. Deputy Secretary of Defense on 26 April 2017. Also known as the Algorithmic Warfare Cross Functional Team, it is, according to Lt. Gen. of the United States Air Force Jack Shanahan in November 2017, a project  designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the  Department . Its chief, U.S. Marine Corps Col. Drew Cukor, said   People and computers will work symbiotically to increase the ability of weapon systems to detect objects.  At the second Defense One Tech Summit in July 2017, Cukor also said that the investment in a  deliberate workflow process  was funded by the Department  through its  rapid acquisition authorities  for about  the next 36 months . The Joint Artificial Intelligence Center  JAIC   pronounced  jake   is an American organization on exploring the usage of AI  particularly edge computing , Network of Networks, and AI enhanced communication, for use in actual combat. It is a subdivision of the United States Armed Forces and was created in June 2018. The organization s stated objective is to  transform the US Department of Defense by accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems  then, ensure the combat Systems and Components have real time access to ever improving libraries of data sets and tools.  In 2015, the UK government opposed a ban on lethal autonomous weapons, stating that  international humanitarian law already provides sufficient regulation for this area , but that all weapons employed by UK armed forces would be  under human oversight and control . Israel s Harpy anti radar  fire and forget  drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre determined criteria. The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions. These robotic vehicles are used in border defense. The South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states,  Our weapons don t sleep, like humans must. They can see in the dark, like humans can t. Our technology therefore plugs the gaps in human capability , and they want to  get to a place where our software can discern whether a target is friend, foe, civilian or military . The European Parliament holds the position that humans must have oversight and decision making power over lethal autonomous weapons. However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union s ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons. Some EU member states have developed and are developing automated weapons. Germany has developed an active protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond. Italy plans to incorporate autonomous weapons systems into its future military plans. The international regulation of autonomous weapons is an emerging issue for international law. AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process. As early as 2007, scholars such as AI professor Noel Sharkey have warned of  an emerging arms race among the hi tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions . Miles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy   We saw in the various historical arms races that collaboration and dialog can pay dividends . Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons  however, at a November 2017 session of the UN Convention on Certain Conventional Weapons  CCW , diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons. However, as of 2022, most major powers continue to oppose a ban on autonomous weapons. Many experts believe attempts to completely ban killer robots are likely to fail, in part because detecting treaty violations would be extremely difficult. A 2017 report from Harvard s Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that  Preventing expanded military use of AI is likely impossible  and that  the more modest goal of safe and effective technology management must be pursued , such as banning the attaching of an AI dead man s switch to a nuclear arsenal. A 2015 open letter by the Future of Life Institute calling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicist Stephen Hawking, Tesla magnate Elon Musk, Apple s Steve Wozniak and Twitter co founder Jack Dorsey, and over 4,600 artificial intelligence researchers, including Stuart Russell, Bart Selman and Francesca Rossi. The Future of Life Institute has also released two fictional films, Slaughterbots  2017  and Slaughterbots   if human  kill    2021 , which portray threats of autonomous weapons and promote a ban, both of which went viral.  Professor Noel Sharkey of the University of Sheffield argues that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State. Many Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China s market. Furthermore, some researchers, such as DeepMind CEO Demis Hassabis, are ideologically opposed to contributing to military work. For example, in June 2018, company sources at Google said that top executive Diane Greene told staff that the company would not follow up Project Maven after the current contract expired in March 2019. 
 Artificial intelligence is used by many different businesses and organizations. It is widely used in the financial sector, especially by accounting firms, to help detect fraud. In 2022, PricewaterhouseCoopers reported that fraud has impacted 46  of all businesses in the world. The shift from working in person to working from home has brought increased access to data. According to an FTC  Federal Trade Commission  study from 2022, customers reported fraud of approximately  5.8 billion in 2021, an increase of 70  from the year before. The majority of these scams were imposter scams and online shopping frauds. Expert systems were first designed in the 1970s as an expansion into artificial intelligence technologies. Their design is based on the premise of decreasing potential user error in decision making and emulating mental reasoning used by experts in a particular field. They differentiate themselves from traditional linear reasoning models by separating identified points in data and processing them individually at the same time. Though, these systems do not rely purely on machine learned intelligence. Information regarding rules, practices, and procedures in the form of  if then  statements are implemented into the programming of the system. Users interact with the system by feeding information into the system either through direct entry or import of external data. An inference system compares the information provided by the user with corresponding rules that are believed to specifically apply to the situation. Using this information and the corresponding rules will be used to create a solution to the user s query. Expert systems will generally not operate properly when the common procedures for a specified situation are ambiguous due to the need for well defined rules. Implementation of expert systems in accounting procedures is feasible in areas where professional judgment is required. Situations where expert systems are applicable include investigations into transactions that involve potential fraudulent entries, instances of going concern, and the evaluation of risk in the planning stages of an audit. Continuous auditing is a set of processes that assess various aspects of information gathered in an audit to classify areas of risk and potential weaknesses in financial Internal controls at a more frequent rate than traditional methods. Instead of analyzing recorded transactions and journal entries periodically, continuous auditing focuses on interpreting the character of these actions more frequently. The frequency of these processes being undertaken as well as highlighting areas of importance is up to the discretion of their implementer, who commonly makes such decisions based on the level of risk in the accounts being evaluated and the goals of implementing the system. Performance of these processes can occur as frequently as being nearly instantaneous with an entry being posted. The processes involved with analyzing financial data in continuous auditing can include the creation of spreadsheets to allow for interactive information gathering, calculation of financial ratios for comparison with previously created models, and detection of errors in entered figures. A primary goal of this practice is to allow for quicker and easier detection of instances of faulty controls, errors, and instances of fraud. The ability of machine learning and deep learning to swiftly and effectively sort through vast volumes of data in the forms of various documents relevant to companies and documents being audited makes them applicable to the domains of audit and fraud detection. Examples of this include recognizing key language in contracts, identifying levels of risk of fraud in transactions, and assessing journal entries for misstatement. Deloitte created an Al enabled document reviewing system in 2014. The system automates the method of reviewing and extracting relevant information from different business documents. Deloitte claims that this innovation has made a difference by reducing time spent going through lawful contract documents, invoices, money related articulations, and board minutes by up to 50 . Working with IBM s Watson, Deloitte is developing cognitive technology enhanced commerce arrangements for its clients. LeasePoint is fueled by IBM Tririga and uses Deloitte s industrial information to create an end to end leasing portfolio. Automated Cognitive Resource Assessment employs IBM s Maximo innovation to progress the proficiency of asset inspection. Ernst and Young  EY  connected Al to the investigation of lease contracts. EY  Australia  has also received Al enabled auditing technology. The firm has propelled the use of AI computer vision to empower clients to monitor stock amid the auditing process. EY is using profound learning to analyze unstructured information such as e mails, social media posts, and conference call sound records. Collaborating with H20.ai, PwC developed an Al enabled framework  GL.ai  capable of analyzing reports and preparing reports. PwC claims to have made a significant investment in normal dialect processing  NLP , an Al enabled innovation to process unstructured information efficiently. KPMG built a portfolio of Al instruments, called KPMG Ignite, to upgrade trade decisions and forms. Working with Microsoft and IBM Watson, KPMG is creating instruments to coordinate Al, data analytics, Cognitive Technologies, and RPA. The process of auditing an entity in an attempt to detect fraudulent activity requires the repeating of investigatory processes until an error or misstatement may be identified. Under traditional methods, these processes would be carried out by a human being. Proponents of artificial intelligence in fraud detection have stated that these traditional methods are inefficient and can be more quickly accomplished with the aid of an intelligent computing system. A survey of 400 chief executive officers created by KPMG in 2016 found that approximately 58  believed that artificial intelligence would play a key role in making audits more efficient in the future. Higher levels of fraud detection entail the use of professional judgement to interpret data. Supporters of artificial intelligence being used in financial audits have claimed that increased risks from instances of higher data interpretation can be minimized through such technologies. One necessary element of an audit of financial statements that requires professional judgement is the implementation of thresholds for materiality. Materiality entails the distinction between errors and transactions in financial statements that would impact decisions made by users of those financial statements. The threshold for materiality in an audit is set by the auditor based on various factors. Artificial intelligence has been used to interpret data and suggest materiality thresholds to be implemented through the use of expert systems. Those in favor of using artificial intelligence to complete investigations of fraud have stated that such technologies decrease the amount of time required to complete tasks that are repetitive. The claim further states that such efficiencies allow for lowered resource requirements, which can then be further spent on tasks that have not been fully automated. The audit firm Ernst   Young has posited these claims by declaring that their deep learning systems have been used to reduce time spent on administrative tasks by analyzing relevant audit documents. According to the firm, this has allowed their employees to focus more on judgement and analysis. The inescapable reception of computer based intelligence and robotization advancements might prompt critical work relocation across different enterprises. As artificial intelligence frameworks become more equipped for performing undertakings customarily completed by people, there is a worry that specific work jobs could become out of date, prompting joblessness and financial imbalance.  Along with a knowledge of coding and building systems through computer programs, we are seeing the advantages of these systems, but since they are so new, they require a large investment to start building your system. Any firm that is planning on implementing an AI system to detect fraud must hire a team of data scientists, along with upgrading their cloud system and data storage as well. The system must be consistently monitored and updated to be the most efficient form of itself, otherwise the likelihood of fraud being involved in those transactions increases. If you do not initially invest in your system and are certain it will detect a large percentage of fraud, you will reap the consequences of large transactions of fraud, along with chargeback fees. It is a very large initial investment, but when the investment is made by the company and the data scientists invest in the work they do, you should save yourself a lot of money because you will never have to pay a robot to detect fraud. You may need people to help build the robot, but over time the costs will minimize. Data analytics is a new science at many companies, and firms are heavily researching it to analyze their business as a whole and find where they can improve. Data analytics tells the story of a business through numbers. Many people in this world are experienced with reading data, but there are also more people who are not as experienced with data at all. The discipline of data analytics is expanding rapidly. It is frequently challenging to become an expert in such a profession. 
Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis  and insight discovery. Although in a dystopian vision of AI applications, intelligent machines may take away jobs of humans and cause social and ethical issues, industry in general holds a more positive view of AI and sees this transformation of economy unstoppable and expects huge business opportunities in this process. The concept of artificial intelligence was initially proposed in the 1940s, and the idea of improving productivity and gaining insights through smart analytics and modelling is not new. Artificial Intelligence and Knowledge Based systems have been an active research branch of artificial intelligence for the entire product life cycle for product design, production planning, distribution, and field services. E manufacturing systems and e factories did not use the term  AI,   but they scale up modeling of engineering systems to enable complete integration of elements in the manufacturing eco system for smart operation management. Recently, to accelerate leadership in AI initiative, the US government launched an official website AI.gov to highlight its priorities in the AI space. There are several reasons for the recent popularity of industrial AI  More affordable sensors and the automated process of data acquisition  More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost  Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing. Technology alone never creates any business value if the problems in industry are not well studied. The major categories which industrial AI may contribute to include  product and service innovation, process improvement, and insight discovery. Cloud Foundry service platforms widely embed the artificial intelligent technologies. Cybermanufacturing systems also apply predictive analytics and cyber physical modeling to address the gap between production and machine health for optimized productivity. Industrial AI can be embedded to existing products or services to make them more effective, reliable, safer, and to enhance their longevity. The automotive industry, for example, uses computer vision to avoid accidents and enable vehicles to stay in lane, facilitating safer driving. In manufacturing, one example is the prediction of blade life for self aware band saw machines, so that users will be able to rely on evidence of degradation rather than experience, which is safer, will extend blade life, and build up blade usage profile to help blade selection. Automation is one of the major aspects in process applications of industrial AI. With the help of AI, the scope and pace of automation have been fundamentally changed. AI technologies boost the performance and expand the capability of conventional AI applications. An example is the collaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task. AI also automates the process that used to require human participation. An example is the Hong Kong subway, where an AI program decides the distribution and job scheduling of engineers with more efficiency and reliability than human counterparts do. Another aspect of process applications is the modeling large scale systems. Cybermanufacturing systems are defined as a manufacturing service system that is networked and resilient to faults by evidence based modeling and data driven deep learning. Such a system deals with large and usually geographically distributed assets, which is hard to be modeled via conventional individual asset physics based model. With machine learning and optimization algorithms, a bottom up framework considering machine health can leverage large samples of assets and automate the operation management, spare part inventory planning, and maintenance scheduling process. Industrial AI can also be used for knowledge discovery by identifying insights in engineering systems. In aviation and aeronautics, AI has been playing a vital role in many critical areas, one of which is safety assurance and root cause. NASA is trying to proactively manage risks to aircraft safety by analyzing flight numeric data and text reports in parallel to not only detect anomalies but also relate it to the causal factors. This mined insight of why certain faults happen in the past will shed light on predictions of similar incidents in the future and prevent problems before they occur. Predictive and preventive maintenance through data driven machine learning is also critical in cost reduction for industrial applications. Prognostics and health management  PHM  programs capture the opportunities at the shop floor by modeling equipment health degradation. The challenges of industrial AI to unlock the value lies in the transformation of raw data to intelligent predictions for rapid decision making. In general, there are four major challenges in realizing industrial AI  data, speed, fidelity, and interpretability. Engineering systems now generate a lot of data and modern industry is indeed a big data environment. However, industrial data usually is structured, but may be low quality. Production process happens fast and the equipment and work piece can be expensive, the AI applications need to be applied in real time to be able to detect anomalies immediately to avoid waste and other consequences. Cloud based solutions can be powerful and fast, but they still would not fit certain computation efficiency requirements. Edge computing may be a better choice in such scenario. Unlike consumer faced AI recommendations systems which have a high tolerance for false positives and negatives, even a very low rate of false positives or negatives rate may cost the total credibility of AI systems. Industrial AI applications are usually dealing with critical issues related to safety, reliability, and operations. Any failure in predictions could incur a negative economic and or safety impact on the users and discourage them to rely on AI systems. Besides prediction accuracy and performance fidelity, the industrial AI systems must also go beyond prediction results and give root cause analysis for anomalies. This requires that during development, data scientists need to work with domain experts and include domain know how into the modeling process, and have the model adaptively learn and accumulate such insights as knowledge. 
The artificial intelligence  AI  industry in China is a rapidly developing multi billion dollar industry. As of 2021, the artificial intelligence market is worth about RMB 150 billion  US 23.196 billion , and is projected to reach RMB 400 billion  US 61.855 billion  by 2025. The roots of China s AI development started in the late 1970s following economic reforms emphasizing science and technology as the country s primary productive force.  The early stages of China s AI development were slow and faced serious challenges due to a lack of resources and talent. At the beginning China was behind most Western countries in terms of AI development. A majority of the research was led by scientists who had received higher education abroad. Since 2006, China has steadily developed a national agenda for artificial intelligence development and emerged as one of the leading nations in artificial intelligence research and development. During the late 2010s, China announced in its thirteenth Five Year Plan its aim to become a global AI leader by 2030 and to increase the worth of its AI industry to over 1 trillion RMB in the same year. China set this goal in three stages, setting benchmarks for 2020, 2025, and 2030 respectively, as well as releasing a handful of policies, including  Internet   AI  and  New Generation AI Development Plan  to incentivize industry growth. Analysts estimated that China s AI development would contribute an annual growth rate of approximately 0.8  to 1.4  to China s economy. China s central government has a list of  national AI teams  including fifteen China based companies, including Baidu, Tencent, Alibaba, SenseTime, and iFlytek. Each company should lead the development of a designated specialized AI sector in China, such as facial recognition, software hardware, and voice intelligence. China s rapid AI development has significantly impacted Chinese society in many areas, including the socio economic, military, and political spheres. Agriculture, transportation, accommodation and food services, and manufacturing are the top industries that would be the most impacted by further AI deployment. However, scholars have warned of potential negative impacts on China s labor market and disproportionate benefits between urban and rural areas, coastal and inland regions, and among different income groups. The private sector, university laboratories, and the military are working collaboratively in many aspects as there are few current existing boundaries. As China continues expanding its AI industry, there are ethical and regulatory concerns yet to be addressed, such as data control and user privacy. In 2021, China published the Data Security Law of the People s Republic of China, its first national law addressing AI related ethical concerns. In October 2022, the United States federal government announced a series of export controls and trade restrictions intended to restrict China s access to advanced computer chips for AI applications. In April 2023, the Cyberspace Administration of China proposed rules that content produced by artificial intelligence  must reflect the core values of socialism.  The research and development of Artificial Intelligence  AI  in China started in the 1980s, with the announcement by Deng Xiaoping of the importance of science and technology for China s economic growth. Despite starting relatively late, Chinese AI development has achieved significant milestones in recent years, making it a notable contributor in both research and application of the AI field. It is noteworthy that Chinese policies have played a crucial role in driving AI advancement from the beginning, with AI being a significant part of China s national technology plan. Artificial intelligence research and development did not start until the late 1970s after economic reforms when Deng Xiaoping announced his famous phrase,  science and technology are primary productive forces            . Deng s speech demonstrated China s commitment to technological innovation. While there was a lack of AI related research between the 1950s and 1960s, some scholars believe this is due to the influence of cybernetics from the Soviet Union despite the Sino Soviet split during the late 1950s and early 1960s. In the 1980s, a group of Chinese scientists launched AI research led by Qian Xuesen and Wu Wenjun. However, during the time, China s society still had a generally conservative view towards AI. Early AI development in China was difficult so China s government approached these challenges by sending Chinese scholars overseas to study AI and further providing government funds for research projects. The Chinese Association for Artificial Intelligence  CAAI  was founded in September 1981 and was authorized by the Ministry of Civil Affairs. Even now, CAAI is still the only national academic association in China that specializes in technology and intelligence science. The first chairman of the executive committee was Qin Yuanxun, who received a PhD in philosophy from Harvard University. In 1987, China s first research publication on artificial intelligence was published by Tsinghua University. Beginning in 1993, smart automation and intelligence have been part of China s national technology plan. Since the 2000s, China has further expanded its R D funds for AI and the number of government sponsored research projects has dramatically increased. In 2001, Xinhua News Agency released a report titled  China s artificial intelligence research has entered a new era,  announcing that China s AI research had transitioned from following foreign research to independent research after the 2001 National Conference of the Chinese Association for Artificial Intelligence. The advancements presented during the conference have demonstrated China s ability to innovate in the field of AI and marked a new era of Chinese AI research. In 2006, China announced a policy priority for the development of artificial intelligence, which was included in the National Medium and Long Term Plan for the Development of Science and Technology  2006 2020 , released by the State Council.In the same year, artificial intelligence was also emphasized during the  Eleventh Five Year Plan . In the guideline, the plan emphasized the need for systematic and in depth research on information acquisition, processing, transmission, storage, reproduction, security, and utilization, as well as on the basic components of information systems, information processing environments, scientific computing, artificial intelligence, control theory, and other aspects, in order to provide a solid theoretical and technological foundation for China s information industry to achieve leapfrog development. In 2011, the Association for the Advancement of Artificial Intelligence  AAAI  established a branch in Beijing, China. At same year, the Wu Wenjun Artificial Intelligence Science and Technology Award was founded in honor of Chinese mathematician Wu Wenjun, and it became the highest award for Chinese achievements in the field of Artificial Intelligence. The first award ceremony was held on May 14, 2012. In 2013, the International Joint Conferences on Artificial Intelligence  IJCAI  was held in Beijing, marking the first time the conference was held in China. This event coincided with the Chinese government s announcement of the  Chinese Intelligence Year,  a significant milestone in China s development of artificial intelligence. The State Council of China issued  A Next Generation Artificial Intelligence Development Plan   State Council Document  No. 35  on 20 July 2017. In the document, the CCP Central Committee and the State Council urged governing bodies in China to promote the development of artificial intelligence. Specifically, the plan described AI as a strategic technology that has become a  focus of international competition . 2 The document urged significant investment in a number of strategic areas related to AI and called for close cooperation between the state and private sectors. On the occasion of CCP general secretary Xi Jinping s speech at the first plenary meeting of the Central Military Civil Fusion Development Committee  CMCFDC , scholars from the National Defense University wrote in the PLA Daily that the  transferability of social resources  between economic and military ends is an essential component to being a great power. During the Two Sessions 2017, artificial intelligence plus  was proposed to be elevated to a strategic level. The same year witnessed the emergence of multiple application level usages in the medical field according to reports. Furthermore, the Chinese Academy of Sciences  CAS  established their AI processor chip research lab in Nanjing, and introduced their first AI specialization chip  Cambrian . In 2018, the State Council budgeted  2.1 billion for an AI industrial park in Mentougou district. In order to achieve this the State Council stated the need for massive talent acquisition, theoretical and practical developments, as well as public and private investments. Some researchers and scholars argued China s commitment to global AI leadership and technological competition was driven by its previous underperformance in innovation which was seen as a part of the  century of humiliation  since the Qing dynasty by the central government. There are historically embedded causes of China s anxiety towards securing an international technological dominance   China missed both industrial revolutions, the one starting in Britain in the mid 18th century, and the one that originated in America in the late 19th century. Therefore, China s government desires to take advantage of the technological revolution in today s world led by digital technology including AI to resume China s  rightful  place and to pursue the national rejuvenation proposed by Xi. Some of the stated motivations that the State Council gave for pursuing its AI strategy include the potential of artificial intelligence for industrial transformation, better social governance and maintaining social stability. The State Council predicted that China would have contributed globally to hardware, software, and methods pertinent to artificial intelligence by 2020. Specifically, the State Council projected the value of core AI industries in China to rise to 150 billion RMB, with a value of over 1 trillion RMB when accounting for related industries. In 2019, the application of Artificial Intelligence expanded to various fields such as quantum physics, geography, and medical research. With the emergence of large language models  LLMs , such as GPT, at the beginning of 2020, Chinese researchers began developing their own LLM. One such example is the multimodal large model called  Zidongtaichu.  Several Chinese companies and academic institutions are actively involved in LLM research, including Baidu s Ernie. In March 2023, Huawei released its trillion parameter LLM  PanGu Sigma,  as an example in Chinese LLM development. By 2025, the State Council aims for China to make fundamental contributions to basic AI theory and to solidify its place as a global leader in AI research. Further, the State Council aims for AI to become  the main driving force for China s industrial upgrading and economic transformation  by this time. The State Council projects the value of the core AI industry in China to be worth 400 billion RMB, with a value of over 5 trillion RMB when accounting for related industries. By 2030, the State Council aims to have China be the global leader in the development of artificial intelligence theory and technology. The State Council claims that China will have developed a  mature new generation AI theory and technology system.  At this point, the State Council projects the value of core AI industries to be worth 1 trillion RMB, with a value of over 10 trillion RMB when accounting for related industries. Ministry of Science and Technology  Ministry of Industry and Information Technology  the Central Leading Group for Cyberspace Affairs National Development and Reform Commission  Ministry of Science and Technology Ministry of Industry and Information Technology According to a February 2019 publication by the Center for a New American Security, China s leadership   including Chinese leader Xi Jinping   believes that being at the forefront of AI technology will be critical to the future of global military and economic power competition. China is by far the United States  most ambitious competitor in the international AI market, and China s 2017  Next Generation AI Development Plan  describes AI as a  strategic technology  that has become a  focus of international competition . 2 According to the document, China seeks to develop a core AI industry worth over 150 billion RMB132 or approximately  21.7 billion by 2020 and will  firmly seize the strategic initiative  and reach  world leading levels  of AI investment by 2030. 2 6 According to AI Readiness Index 2020 published by Oxford Insights, China is ranked No.19 globally with an index score of 69.08 100. The report highlighted China s score might be higher in reality since the AI index did not measure actual AI implementation itself. AIDP has divided China s AI development into three stages  According to Statista, China s AI market reached over 128 billion RMB by 2020 and yet the market continues to grow. In today s global AI race, China shared 19  of the total global AI funding deals, which is ranked No.2 behind the U.S.  41  . According to China s 14th Five Year Plans, the document covering China s plans for the period between 2021 and 2025 released in 2021 by the National People s Congress,  the following goals were highlighted  Industry wise, China s large population generates a massive amount of accessible data for companies and researchers, which offers a crucial advantage in the race of big data. For instance, facial recognition is one of the most widely AI applications in China. Collecting these large amounts of data from its residents helps further train and expand AI capabilities. China s market is not only conducive and valuable for corporations to further AI R D but also offers tremendous economic potential attracting both international and domestic firms to join the AI market. The drastic development of the information and communication technology  ICT  industry and AI chipsets in recent years are two examples of this. China s AI research and development crosses different sectors and organizations including private public collaborations, academia private collaborations, government led projects, and many others. Many scholars believe China has adopted a  catch up  approach for its AI development, which is also apparent through policy documents. For public AI R D spending, China has outspent the U.S. government over the past few years. The Chinese public AI funding mainly focused on advanced and applied research. The government funding also supported multiple AI R D in the private sector through venture capitals that are backed by the state. Much analytic agency research showed that, while China is massively investing in all aspects of AI development, facial recognition, biotechnology, quantum computing, medical intelligence, and autonomous vehicles are AI sectors with the most attention and funding. According to national guidance on developing China s high tech industrial development zones by the Ministry of Science and Technology, there are fourteen cities and one county selected as an experimental development zone. Zhejiang and Guangdong provinces have the most AI innovation in experimental areas. However, the focus of AI R D varied depending on cities and local industrial development and ecosystem. For instance, Suzhou, a city with a longstanding strong manufacturing industry, heavily focuses on automation and AI infrastructure while Wuhan focuses more on AI implementations and the education sector. Recent Chinese achievements in the field demonstrate China s potential to realize its goals for AI development. In 2015, China s leading AI company, Baidu, created AI software capable of surpassing human levels of language recognition, almost a year in advance of Microsoft, the nearest U.S. competitor. In 2016 and 2017, Chinese teams won the top prize at the Large Scale Visual Recognition Challenge, an international competition for computer vision systems. Many of these systems are now being integrated into China s domestic surveillance network and Social Credit System, which aims to monitor, and based on social behavior,  grade  every Chinese citizen by 2021. In China, the majority of peer reviewed AI publications are associated with academia, which made up approximately 95.4  of the total number in 2019 which is similar to the European Union. Interdisciplinary collaborations play an essential role in China s AI R D, including academic corporate collaboration, public private collaborations, and international collaborations and projects with corporate government partnerships are the most common. The government incentivizes firms  technological innovation through policies, funding, and national endorsements. As the history section mentioned showed, China s central government released a series of policy frameworks and guidance with the embedded goal of being the global AI leader. As for AI publications, China is ranked Top three worldwide following the United States and the European Union for the total number of peer reviewed AI publications that are produced under a corporate academic partnership between 2015 and 2019. Besides, according to an AI index report, China surpassed the U.S. in 2020 in the total number of global AI related journal citations. In terms of AI related R D, China based peer reviewed AI paper is mainly sponsored by the government. In May 2021, China s Beijing Academy of Artificial Intelligence released the world s largest pre trained language model  WuDao . According to McKinsey s 2017 report, China s economic growth would have approximately 0.8    1.4  annual growth rate from AI led automation and increases in domestic productivity benefiting food services, agriculture, manufacturing, and accommodation industries. Other sources report slightly varied figures, such as Accenture whom estimated that AI would add 1.6  economic growth to China by 2035. The World Economic Forum stated that, by 2017, there are two thirds of the total AI investments worldwide rushing into China resulting in a 67  annual growth rate for China s AI industry. State sponsored media specifically highlighted high technology manufacturing industries as a strong driver to China s stable economic growth. Most agencies hold optimistic views about AI s economic impact on China s long term economic growth. In the past, traditional industries in China have struggled with the increase in labor costs due to the growing aging population in China and the low birth rate. With the deployment of AI, operational costs are expected to reduce while an increase in efficiency generates revenue growth. Some highlight the importance of a clear policy and governmental support in order to overcome adoption barriers including costs and lack of properly trained technical talents and AI awareness. However, there are concerns about China s deepening income inequality and the ever expanding imbalanced labor market in China. Low  and medium income workers might be the most negatively impacted by China s AI development because of rising demands for laborers with advanced skills. Furthermore, China s economic growth might be disproportionately divided as a majority of AI related industrial development is concentrated in coastal regions rather than inland. China is researching various types of air, land, sea, and undersea autonomous vehicles. In the spring of 2017, a civilian Chinese university with ties to the military demonstrated an AI enabled swarm of 1,000 uninhabited aerial vehicles at an airshow. A media report released afterwards showed a computer simulation of a similar swarm formation finding and destroying a missile launcher. 23 Open source publications indicated that China is also developing a suite of AI tools for cyber operations. 27 Chinese development of military AI is largely influenced by China s observation of U.S. plans for defense innovation and fears of a widening  generational gap  in comparison to the U.S. military. Similar to U.S. military concepts, China aims to use AI for exploiting large troves of intelligence, generating a common operating picture, and accelerating battlefield decision making. 12 14 China s management of its AI ecosystem contrasts with that of the United States. 6 In general, few boundaries exist between Chinese commercial companies, university research laboratories, the military, and the central government. As a result, the Chinese government has a direct means of guiding AI development priorities and accessing technology that was ostensibly developed for civilian purposes. To further strengthen these ties the Chinese government created a Military Civil Fusion Development Commission which is intended to speed the transfer of AI technology from commercial companies and research institutions to the military in January 2017. 19 In addition, the Chinese government is leveraging both lower barriers to data collection and lower costs of data labeling to create the large databases on which AI systems train. According to one estimate, China is on track to possess 20  of the world s share of data by 2020, with the potential to have over 30  by 2030. 12 China s centrally directed effort is investing in the U.S. AI market, in companies working on militarily relevant AI applications, potentially granting it lawful access to U.S. technology and intellectual property. Chinese venture capital investment in U.S. AI companies between 2010 and 2017 totaled an estimated  1.3 billion. Although in 2004, Peking University introduced the first academic course on AI which led other Chinese universities to adopt AI as a discipline, especially since China faces challenges in recruiting and retaining AI engineers and researchers. Over half of the data scientists in the United States have been working in the field for over 10 years, while roughly the same proportion of data scientists in China have less than 5 years of experience. As of 2017, fewer than 30 Chinese Universities produce AI focused experts and research products. 8 Although China surpassed the United States in the number of research papers produced from 2011 to 2015, the quality of its published papers, as judged by peer citations, ranked 34th globally. China especially want to address military applications and so the Beijing Institute of Technology, one of China s premier institutes for weapons research, recently established the first children s educational program in military AI in the world. For the past years, there are discussions about AI safety and ethical concerns in both private and public sectors. In 2021, China s Ministry of Science and Technology published the first national ethical guideline,  the New Generation of Artificial Intelligence Ethics Code  on the topic of AI with specific emphasis on user protection, data privacy, and security. This document acknowledges the power of AI and quick technology adaptation by the big corporations for user engagements. The South China Morning Post reported that humans shall remain in full decision making power and rights to opt in  out. Before this, the Beijing Academy of Artificial Intelligence has published the Beijing AI principles calling for essential needs in long term research and planning of AI ethical principles. This document addresses the topics of  security and privacy ,  safety and reliability ,  transparency ,  accountability , and  fairness . Data security has been the most common topic in AI ethical discussion worldwide, and many national governments have established legislation addressing data privacy and security. The Cybersecurity Law of the People s Republic of China was enacted in 2017 aiming to address new challenges raised by AI development. This document includes sections about cybersecurity promotion, network operation security, information security, emergency responses, and legal responsibility. In 2021, China s new Data Security Law  DSL  was passed by the PRC congress, setting up a regulatory framework classifying all kinds of data collection and storage in China. This means all tech companies in China are required to classify their data into categories listed in Digital. Subscriber Line  DSL  and follow specific guidelines on how to govern and handle data transfers to other parties. Leading AI centric companies and start ups include Baidu, Tencent, Alibaba, SenseTime and Yitu Technology. Chinese AI companies iFlytek, SenseTime, Cloudwalk and DJI have received attention for facial recognition, sound recognition and drone technologies. At the World Artificial Intelligence Conference 2019 hosted in Shanghai, the Ministry of Science and Technology announced the latest list of Chinese firms that were selected for China s AI  national team  with assigned specialized AI sectors. China has expanded its national AI team three times since its initial announcement in 2017. The number of firms involved has also expanded from five to fifteen  An article published by the Center for a New American Security concluded that  Chinese government officials demonstrated remarkably keen understanding of the issues surrounding AI and international security. This includes knowledge of the U.S. AI policy discussions,  and recommended that  the U.S. policymaking community to similarly prioritize cultivating expertise and understanding of AI developments in China  and  funding, focus, and a willingness among U.S. policymakers to drive large scale necessary change.  An article in the MIT Technology Review similarly concluded   China might have unparalleled resources and enormous untapped potential, but the West has world leading expertise and a strong research culture. Rather than worry about China s progress, it would be wise for Western nations to focus on their existing strengths, investing heavily in research and education.  Some experts believe that China s intent to be the first to develop military AI applications may result in comparatively less safe applications, as China will likely be more risk acceptant throughout the development process. These experts stated that it would be unethical for the U.S. military to sacrifice safety standards for the sake of external time pressures, but that the United States  more conservative approach to AI development may result in more capable systems in the long run. 23 
Artificial psychology  AP  has had multiple meanings dating back to 19th century, with recent usage related to artificial intelligence  AI . In 1999, Zhiliang Wang and Lun Xie presented a theory of artificial psychology based on artificial intelligence. They analyze human psychology using information science research methods and artificial intelligence research to probe deeper into the human mind. Dan Curtis  b. 1963  proposed AP is a theoretical discipline. The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions  Condition I Condition II When both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria are met for intervention which will not necessarily be resolved by simple re coding of processes due to extraordinarily complex nature of the codebase itself  but rather a discussion with the intelligence in a format which more closely resembles classical  human  psychology. If the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required. The level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious. As of 2022, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline. Even at a theoretical level, artificial psychology remains an advanced stage of artificial intelligence. 
Artificial wisdom is a software system that can demonstrate one or more qualities of being wise. Artificial wisdom can be described as artificial intelligence reaching the top level of decision making when confronted with the most complex challenging situations. The term artificial wisdom is used when the  intelligence  is based on more than by chance collecting and interpreting data, but by design enriched with smart and conscience strategies that wise people would use. When examining computer aided wisdom  the partnership of artificial intelligence and contemplative neuroscience, concerns regarding the future of artificial intelligence shift to a more optimistic viewpoint. This artificial wisdom forms the basis of Louis Molnar s monographic article on artificial philosophy, where he coined the term and proposes how artificial intelligence might view its place in the grand scheme of things . This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Attributional calculus is a logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi valued logic. Attributional calculus provides a formal language for natural induction, which is an inductive learning process whose outcomes are in human readable forms. Michalski, R.S.,  ATTRIBUTIONAL CALCULUS  A Logic and Representation Language for Natural Induction,  Reports of the Machine Learning and Inference Laboratory, MLI 04 2, George Mason University, Fairfax, VA, April, 2004. This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Aurora is a 2015 novel by American science fiction author Kim Stanley Robinson. The novel concerns a generation ship built in the style of a Stanford torus traveling to Tau Ceti in order to begin a human colony. The novel s primary narrating voice is the starship s artificial intelligence. The novel was well received by critics. A generation ship is launched from Earth in 2545 at 0.1 c  i.e. traveling at 108,000,000 km hr or 10  the speed of light . It includes twenty four self contained biomes and an average population of two thousand people. One hundred sixty years and approximately seven generations later, it is beginning its deceleration into the Tau Ceti system to begin colonization of a planet s moon, an Earth analog, which has been named Aurora. Devi, the ship s de facto chief engineer and leader, is concerned about the decaying infrastructure and biology of the ship  systems are breaking down, each generation has lower intelligence test scores than the last, and bacteria are mutating and evolving at a faster rate than humans. She tells the ship s AI, referred to simply as Ship, to keep a narrative of the voyage. After having some trouble with understanding the human concept of narrative, Ship eventually elects to follow the life of Devi s daughter Freya as a protagonist. As a teenager, Freya travels around the ship on her wanderjahr and learns that many of the ship s inhabitants are dissatisfied with their enclosed existence and what they perceive as a dictatorship. Movement is strictly limited for most people, reproduction is tightly controlled, and education in science and mathematics is mandatory. Freya s wanderjahr comes to an end when she is called home as Devi grows sick from cancer and dies. The ship arrives in the Tau Ceti system and begins to settle Aurora, a moon of Tau Ceti e. It soon becomes apparent that extraterrestrial life is present in the form of primitive prions, which infect and kill some of the landing party. All except one of the remaining settlers attempt to return to the ship, and some of those remaining onboard kill them in the airlock to maintain quarantine, leading to a violent political schism throughout the ship. The ship itself, which has been moving towards self awareness, takes physical control of the situation by lowering oxygen levels and separating warring factions, referring to itself as  the rule of law . It then reveals to the crew that there were in fact two ships originally launched for the Tau Ceti expedition, but the other was destroyed during a period of severe civil unrest, and the collective memory of that event was erased from the history records. Under Ship s moderation, a more peaceful debate takes place between the inhabitants about what to do now that Aurora is known to be inhospitable. Unable to reach consensus, the factions agree to part ways, with those who wish to stay retaining as many resources as can be spared to pursue an unlikely attempt at terraforming the Mars like moon Iris, while the other group, led by Freya, opt to try and return to Earth. Using raw materials in the Tau Ceti system, they refuel the ship to allow acceleration back to Earth  since they lack fuel to decelerate, they must rely on the laser propulsion system that originally launched them from the Solar System to slow them down on approach. The last remaining Aurora settler, who remains permanently quarantined in his shuttle attached to the exterior of the ship, elects to return to Earth as well. Initially, Freya and the others who return remain in communication with those who remained in the Tau Ceti system, but much later on their voyage home this communication stops. On the voyage back to Earth, the ship s biomes continue to deteriorate as bacteria flourish and crops fail. The humans soon face famine and experiment with an untested form of cryogenic freezing, which is largely successful. The ship s repeated entreaties to Earth to turn back on the laser propulsion system are ignored due to societal and political strife back in the Solar System, and many citizens  anger at the colonists   cowardice . Eventually a private group funds and reactivates the laser, but the delay means the ship s speed is only reduced by a fraction of what is needed. Ship is therefore forced to decelerate by means of gravity assist between various planets, a process which takes twelve years. During this time, with the full communications data of humanity available to it, it learns more about why it was launched in the first place simply for expansionism and denounces its builders as  criminally negligent narcissists . Ship manages to safely drop its humans off on a pass of Earth but fails to make a final gravity slowdown past the Sun. Ship is destroyed along with the last survivor of the landing on Aurora. Freya and the other  starfarers  have trouble adjusting to life on Earth, especially with many Terrans hostile to them for a perceived sense of ingratitude and cowardice. At a space colonization conference, a speaker says humanity will continue to send ships into interstellar space no matter how many fail and die, and Freya assaults him. Eventually she joins a group of terraformers who are attempting to restore the Earth s beaches after their loss during previous centuries  sea level rise. While swimming and surfing, she begins to come to terms with life on Earth. Major themes in Aurora include complexities of life aboard a multi generational starship, interpersonal psychology, artificial intelligence, human migration, environmentalism, and the feasibility of star travel. Robinson says that in researching the novel he met with his friend Christopher McKay who has helped him since the Mars Trilogy. McKay arranged lunches at the NASA Ames Research Center where Robinson asked questions of NASA employees. 
 Auto GPT is an  AI agent  that, given a goal in natural language, will attempt to achieve it by breaking it into sub tasks and using the internet and other tools in an automatic loop. It uses OpenAI s GPT 4 or GPT 3.5 APIs, and is among the first examples of an application using GPT 4 to perform autonomous tasks. Unlike interactive systems such as ChatGPT, which require manual commands for every task, Auto GPT assigns itself new objectives to work on with the aim of reaching a greater goal, without a mandatory need for human input. It is able to execute responses to prompts to accomplish a goal task, and in doing so will create and revise its own prompts to recursive instances in response to new information. It manages short term and long term memory by writing to and reading from databases and files  manages context window length requirements with summarization  can perform internet based actions such as web searching, web form, and API interactions unattended  and includes text to speech for voice output. Observers tout Auto GPT s ability to write, debug, test, and edit code, even suggesting this ability may extend to Auto GPT s own source code enabling self improvement. However, as the underlying GPT models it uses are proprietary, Auto GPT cannot modify them, and it does not ordinarily have access to its own base system code. On March 14, 2023, OpenAI released the large language model GPT 4. Observers were impressed by the model s substantially improved performance across a wide range of tasks. As a text prediction model, GPT 4 itself has no ability to perform actions autonomously, but during pre release safety testing red team researchers found GPT 4 could be enabled to perform actions in the real world like convincing a TaskRabbit worker to solve a CAPTCHA challenge for it. A team of Microsoft researchers argued that, given GPT 4 s breadth of abilities at levels approaching those of humans, GPT 4  could reasonably be viewed as an early  yet still incomplete  version of an artificial general intelligence  AGI  system.  The researchers emphasized their experiments also found significant limitations in the system. Auto GPT was released March 30, 2023 by Toran Bruce Richards, the founder of video game company Significant Gravitas Ltd. It became the top trending repository on GitHub shortly after its release, and has repeatedly trended on Twitter since. Whether Auto GPT will find practical applications is uncertain. In addition to being plagued by confabulatory  hallucinations  of the underlying large language models upon which it is based, Auto GPT often also has trouble staying on task, both problems which developers continue to try to address. After successfully completing a task, it usually does not remember how to perform it for later use, and when it does, for example when it writes a program, it often forgets to use the program later. Auto GPT struggles to effectively decompose tasks and has trouble understanding problem contexts and how goals overlap. Auto GPT was used to create ChaosGPT, which, given the goal of destroying humanity, was not immediately successful in doing so. 
Autognostics is a new paradigm that describes the capacity for computer networks to be self aware. It is considered one of the major components of Autonomic Networking. One of the most important characteristics of today s Internet that has contributed to its success is its basic design principle  a simple and transparent core with intelligence at the edges  the so called  end to end principle  . Based on this principle, the network carries data without knowing the characteristics of that data  e.g., voice, video, etc.    only the end points have application specific knowledge. If something goes wrong with the data, only the edge may be able to recognize that since it knows about the application and what the expected behavior is. The core has no information about what should happen with that data   it only forwards packets. Although an effective and beneficial attribute, this design principle has also led to many of today s problems, limitations, and frustrations. Currently, it is almost impossible for most end users to know why certain network based applications do not work well and what they need to do to make it better. Also, network operators who interact with the core in low level terms such as router configuration have problems expressing their high level goals into low level actions. In high level terms, this may be summarized as a weak coupling between the network and application layers of the overall system. As a consequence of the Internet end to end principle, the network performance experienced by a particular application is difficult to attribute based on the behavior of the individual elements. At any given moment, the measure of performance between any two points is typically unknown and applications must operate blindly. As a further consequence, changes to the configuration of given element, or changes in the end to end path, cannot easily be validated. Optimization and provisioning cannot then be automated except against only the simplest design specifications. There is an increasing interest in Autonomic Networking research, and a strong conviction that an evolution from the current networking status quo is necessary. Although to date there have not been any practical implementations demonstrating the benefits of an effective autonomic networking paradigm, there seems to be a consensus as to the characteristics which such implementations would need to demonstrate. These specifically include continuous monitoring, identifying, diagnosing and fixing problems based on high level policies and objectives. Autognostics, as a major part of the autonomic networking concept, intends to bring networks to a new level of awareness and eliminate the lack of visibility which currently exists in today s networks. Autognostics is a new paradigm that describes the capacity for computer networks to be self aware, in part and as a whole, and dynamically adapt to the applications running on them by autonomously monitoring, identifying, diagnosing, resolving issues, subsequently verifying that any remediation was successful, and reporting the impact with respect to the application s use  i.e., providing visibility into the changes to networks and their effects . Although similar to the concept of network awareness, i.e., the capability of network devices and applications to be aware of network characteristics  see References section below , it is noteworthy that autognostics takes that concept one step further. The main difference is the auto part of autognostics, which entails that network devices are self aware of network characteristics, and have the capability to adapt themselves as a result of continuous monitoring and diagnostics. Autognostics, or in other words deep self knowledge, can be best described as the ability of a network to know itself and the applications that run on it. This knowledge is used to autonomously adapt to dynamic network and application conditions such as utilization, capacity, quality of service application user experience, etc. In order to achieve autognosis, networks need a means to  
The Automated Mathematician  AM  is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award. AM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts  for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication.  The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems. Lenat claimed that the system was composed of hundreds of data structures called  concepts,  together with hundreds of  heuristic rules  and a simple flow of control   AM repeatedly selects the top task from the agenda and tries to carry it out.  This is the whole control structure!   Yet the heuristic rules were not always represented as separate data structures  some had to be intertwined with the control flow logic.  Some rules had preconditions that depended on the history, or otherwise could not be represented in the framework of the explicit rules. What s more, the published versions of the rules often involve vague terms that are not defined further, such as  If two expressions are structurally similar, ...   Rule 218  or  ... replace the value obtained by some other  very similar  value...   Rule 129 . Another source of information is the user, via Rule 2   If the user has recently referred to X, then boost the priority of any tasks involving X.   Thus, it appears quite possible that much of the real discovery work is buried in unexplained procedures. Lenat claimed that the system had rediscovered both Goldbach s conjecture and the fundamental theorem of arithmetic.  Later critics accused Lenat of over interpreting the output of AM. In his paper Why AM and Eurisko appear to work, Lenat conceded that any system that generated enough short Lisp programs would generate ones that could be interpreted by an external observer as representing equally sophisticated mathematical concepts.  However, he argued that this property was in itself interesting and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful. This intuition was the basis of AM s successor Eurisko, which attempted to generalize the search for mathematical concepts to the search for useful heuristics. 
Automated negotiation is a form of interaction in systems that are composed of multiple autonomous agents, in which the aim is to reach agreements through an iterative process of making offers. Automated negotiation can be employed for many tasks human negotiators regularly engage in, such as bargaining and joint decision making. The main topics in automated negotiation revolve around the design of protocols and strategies. Through digitization, the beginning of the 21st century has seen a growing interest in the automation of negotiation and e negotiation systems, for example in the setting of e commerce. This interest is fueled by the promise of automated agents being able to negotiate on behalf of human negotiators, and to find better outcomes than human negotiators. Examples of automated negotiation include   This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Autonomic computing  AC  is distributed computing resources with self managing characteristics, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth. The AC system concept is designed to make adaptive decisions, using high level policies. It will constantly check and optimize its status and automatically adapt itself to changing conditions.  An autonomic computing framework is composed of autonomic components  AC  interacting with each other. An AC can be modeled in terms of two main control schemes  local and global  with sensors  for self monitoring , effectors  for self adjustment , knowledge and planner adapter for exploiting policies based on self  and environment awareness. This architecture is sometimes referred to as Monitor Analyze Plan Execute  MAPE . Driven by such vision, a variety of architectural frameworks based on  self regulating  autonomic components has been recently proposed. A very similar trend has recently characterized significant research in the area of multi agent systems. However, most of these approaches are typically conceived with centralized or cluster based server architectures in mind and mostly address the need of reducing management costs rather than the need of enabling complex software systems or providing innovative services. Some autonomic systems involve mobile agents interacting via loosely coupled communication mechanisms. Autonomy oriented computation is a paradigm proposed by Jiming Liu in 2001 that uses artificial systems imitating social animals  collective behaviours to solve difficult computational problems. For example, ant colony optimization could be studied in this paradigm. Forecasts suggest that the computing devices in use will grow at 38  per year and the average complexity of each device is increasing. Currently, this volume and complexity is managed by highly skilled humans  but the demand for skilled IT personnel is already outstripping supply, with labour costs exceeding equipment costs by a ratio of up to 18 1. Computing systems have brought great benefits of speed and automation but there is now an overwhelming economic need to automate their maintenance. In a 2003 IEEE Computer article, Kephart and Chess warn that the dream of interconnectivity of computing systems and devices could become the  nightmare of pervasive computing  in which architects are unable to anticipate, design and maintain the complexity of interactions. They state the essence of autonomic computing is system self management, freeing administrators from low level task management while delivering better system behavior. A general problem of modern distributed computing systems is that their complexity, and in particular the complexity of their management, is becoming a significant limiting factor in their further development. Large companies and institutions are employing large scale computer networks for communication and computation. The distributed applications running on these computer networks are diverse and deal with many tasks, ranging from internal control processes to presenting web content to customer support. Additionally, mobile computing is pervading these networks at an increasing speed  employees need to communicate with their companies while they are not in their office. They do so by using laptops, personal digital assistants, or mobile phones with diverse forms of wireless technologies to access their companies  data. This creates an enormous complexity in the overall computer network which is hard to control manually by human operators. Manual control is time consuming, expensive, and error prone. The manual effort needed to control a growing networked computer system tends to increase very quickly. 80  of such problems in infrastructure happen at the client specific application and database layer. Most  autonomic  service providers guarantee only up to the basic plumbing layer  power, hardware, operating system, network and basic database parameters . A possible solution could be to enable modern, networked computing systems to manage themselves without direct human intervention. The Autonomic Computing Initiative  ACI  aims at providing the foundation for autonomic systems. It is inspired by the autonomic nervous system of the human body. This nervous system controls important bodily functions  e.g. respiration, heart rate, and blood pressure  without any conscious intervention. In a self managing autonomic system, the human operator takes on a new role  instead of controlling the system directly, he she defines general policies and rules that guide the self management process. For this process, IBM defined the following four types of property referred to as self star  also called self  , self x, or auto    properties.   Others such as Poslad and Nami and Sharifi have expanded on the set of self star as follows  IBM has set forth eight conditions that define an autonomic system  The system must Even though the purpose and thus the behaviour of autonomic systems vary from system to system, every autonomic system should be able to exhibit a minimum set of properties to achieve its purpose  IBM defined five evolutionary levels, or the autonomic deployment model, for the deployment of autonomic systems  The design complexity of Autonomic Systems can be simplified by utilizing design patterns such as the model view controller  MVC  pattern to improve concern separation by encapsulating functional concerns. A basic concept that will be applied in Autonomic Systems are closed control loops. This well known concept stems from Process Control Theory. Essentially, a closed control loop in a self managing system monitors some resource  software or hardware component  and autonomously tries to keep its parameters within a desired range. According to IBM, hundreds or even thousands of these control loops are expected to work in a large scale self managing computer system. A fundamental building block of an autonomic system is the sensing capability  Sensors Si , which enables the system to observe its external operational context. Inherent to an autonomic system is the knowledge of the Purpose  intention  and the Know how to operate itself  e.g., bootstrapping, configuration knowledge, interpretation of sensory data, etc.  without external intervention. The actual operation of the autonomic system is dictated by the Logic, which is responsible for making the right decisions to serve its Purpose, and influence by the observation of the operational context  based on the sensor input . This model highlights the fact that the operation of an autonomic system is purpose driven. This includes its mission  e.g., the service it is supposed to offer , the policies  e.g., that define the basic behaviour , and the  survival instinct . If seen as a control system this would be encoded as a feedback error function or in a heuristically assisted system as an algorithm combined with set of heuristics bounding its operational space. 
Autonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. Its ultimate aim is to create self managing networks to overcome the rapidly growing complexity of the Internet and other networks and to enable their further growth, far beyond the size of today. The ever growing management complexity of the Internet caused by its rapid growth is seen by some experts as a major problem that limits its usability in the future. What s more, increasingly popular smartphones, PDAs, networked audio and video equipment,  and game consoles need to be interconnected. Pervasive Computing not only adds features, but also burdens existing networking infrastructure with more and more tasks that sooner or later will not be manageable by human intervention alone. Another important aspect is the price of manually controlling huge numbers of vitally important devices of current network infrastructures. The autonomic nervous system  ANS  is the part of the nervous system of the higher life forms that is not consciously controlled. It regulates bodily functions and the activity of specific organs. As proposed by IBM, future communication systems might be designed in a similar way to the ANS. As autonomics conceptually derives from biological entities such as the human autonomic nervous system, each of the areas can be metaphorically related to functional and structural aspects of a living being. In the human body, the autonomic system facilitates and regulates a variety of functions including respiration, blood pressure and circulation, and emotive response. The autonomic nervous system is the interconnecting fabric that supports feedback loops between internal states and various sources by which internal and external conditions are monitored. Autognostics includes a range of self discovery, awareness, and analysis capabilities that provide the autonomic system with a view on high level state. In metaphor, this represents the perceptual sub systems that gather, analyze, and report on internal and external states and conditions   for example, this might be viewed as the eyes, visual cortex and perceptual organs of the system. Autognostics, or literally  self knowledge , provides the autonomic system with a basis for response and validation. A rich autognostic capability may include many different  perceptual senses . For example, the human body gathers information via the usual five senses, the so called sixth sense of proprioception  sense of body position and orientation , and through emotive states that represent the gross wellness of the body. As conditions and states change, they are detected by the sensory monitors and provide the basis for adaptation of related systems. Implicit in such a system are imbedded models of both internal and external environments such that relative value can be assigned to any perceived state   perceived physical threat  e.g. a snake  can result in rapid shallow breathing related to fight flight response, a phylogenetically effective model of interaction with recognizable threats. In the case of autonomic networking, the state of the network may be defined by inputs from  Most of these sources represent relatively raw and unprocessed views that have limited relevance. Post processing and various forms of analysis must be applied to generate meaningful measurements and assessments against which current state can be derived. The autognostic system interoperates with  Configuration management is responsible for the interaction with network elements and interfaces. It includes an accounting capability with historical perspective that provides for the tracking of configurations over time, with respect to various circumstances. In the biological metaphor, these are the hands and, to some degree, the memory of the autonomic system. On a network, remediation and provisioning are applied via configuration setting of specific devices. Implementation affecting access and selective performance with respect to role and relationship are also applied. Almost all the  actions  that are currently taken by human engineers fall under this area. With only a few exceptions, interfaces are set by hand, or by extension of the hand, through automated scripts. Implicit in the configuration process is the maintenance of a dynamic population of devices under management, a historical record of changes and the directives which invoked change. Typical to many accounting functions, configuration management should be capable of operating on devices and then rolling back changes to recover previous configurations. Where change may lead to unrecoverable states, the sub system should be able to qualify the consequences of changes prior to issuing them. As directives for change must originate from other sub systems, the shared language for such directives must be abstracted from the details of the devices involved. The configuration management sub system must be able to translate unambiguously between directives and hard actions or to be able to signal the need for further detail on a directive. An inferential capacity may be appropriate to support sufficient flexibility  i.e. configuration never takes place because there is no unique one to one mapping between directive and configuration settings . Where standards are not sufficient, a learning capacity may also be required to acquire new knowledge of devices and their configuration. Configuration management interoperates with all of the other sub systems including  Policy management includes policy specification, deployment, reasoning over policies, updating and maintaining policies, and enforcement. Policy based management is required for  It provides the models of environment and behavior that represent effective interaction according to specific goals. In the human nervous system metaphor, these models are implicit in the evolutionary  design  of biological entities and specific to the goals of survival and procreation. Definition of what constitutes a policy is necessary to consider what is involved in managing it. A relatively flexible and abstract framework of values, relationships, roles, interactions, resources, and other components of the network environment is required. This sub system extends far beyond the physical network to the applications in use and the processes and end users that employ the network to achieve specific goals. It must express the relative values of various resources, outcomes, and processes and include a basis for assessing states and conditions. Unless embodied in some system outside the autonomic network or implicit to the specific policy implementation, the framework must also accommodate the definition of process, objectives and goals. Business process definitions and descriptions are then an integral part of the policy implementation. Further, as policy management represents the ultimate basis for the operation of the autonomic system, it must be able to report on its operation with respect to the details of its implementation. The policy management sub system interoperates  at least  indirectly with all other sub systems but primarily interacts with  Autodefense represents a dynamic and adaptive mechanism that responds to malicious and intentional attacks on the network infrastructure, or use of the network infrastructure to attack IT resources. As defensive measures tend to impede the operation of IT, it is optimally capable of balancing performance objectives with typically over riding threat management actions. In the biological metaphor, this sub system offers mechanisms comparable to the immune system. This sub system must proactively assess network and application infrastructure for risks, detect and identify threats, and define effective both proactive and reactive defensive responses. It has the role of the warrior and the security guard insofar as it has roles for both maintenance and corrective activities. Its relationship with security is close but not identical   security is more concerned with appropriately defined and implemented access and authorization controls to maintain legitimate roles and process. Autodefense deals with forces and processes, typically malicious, outside the normal operation of the system that offer some risk to successful execution. Autodefense requires high level and detailed knowledge of the entire network as well as imbedded models of risk that allow it to analyze dynamically the current status. Corrections to decrease risk must be considered in balance with performance objectives and value of process goals   an overzealous defensive response can immobilize the system  like the immune system inappropriately invoking an allergic reaction . The detection of network or application behaviors that signal possible attack or abuse is followed by the generation of an appropriate response   for example, ports might be temporarily closed or packets with a specific source or destination might be filtered out. Further assessment generates subsequent changes either relaxing the defensive measures or strengthening them. Autodefense interoperates closely with  It also may receive definition of relative value of various resources and processes from policy management in order to develop responses consistent with policy. Security provides the structure that defines and enforces the relationships between roles, content, and resources, particularly with respect to access. It includes the framework for definitions as well as the means to implement them. In metaphor, security parallels the complex mechanisms underlying social interactions, defining friends, foes, mates and allies and offering access to limited resources on the basis of assessed benefit. Several key means are employed by security   they include the well known 3 As of authentication, authorization, and access  control . The basis for applying these means requires the definition of roles and their relationships to resources, processes and each other. High level concepts like privacy, anonymity and verification are likely imbedded in the form of the role definitions and derive from policy. Successful security reliably supports and enforces roles and relationships. Autodefense has a close association with security   maintaining the assigned roles in balance with performance exposes the system to potential violations in security. In those cases, the system must compensate by making changes that may sacrifice balance on a temporary basis and indeed may violate the operational terms of security itself. Typically the two are viewed as inextricably intertwined   effective security somewhat hopefully negating any need for a defensive response. Security s revised role is to mediate between the competing demands from policy for maximized performance and minimized risk with auto defense recovering the balance when inevitable risk translates to threat. Federation represents one of the key challenges to be solved by effective security. The security sub system interoperates directly with  The connection fabric supports the interaction with all the elements and sub systems of the autonomic system. It may be composed of a variety of means and mechanisms, or may be a single central framework. The biological equivalent is the central nervous system itself   although referred to as the autonomic system, it actually is only the communication conduit between the human body s faculties. Consequently, it is currently under research by many research projects, how principles and paradigms of mother nature might be applied to networking. Instead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization. The goal is to produce an architectural design that enables flexible, dynamic, and fully autonomic formation of large scale networks in which the functionalities of each constituent network node are also composed in an autonomic fashion Functions should be divided into atomic units to allow for maximal re composition freedom. A fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking. A closed control loop maintains the properties of the controlled system within desired bounds by constantly monitoring target parameters. 
There are various definitions of autonomous agent. According to Brustoloni  1991    Autonomous agents are systems capable of autonomous, purposeful action in the real world. According to Maes  1995    Autonomous agents are computational systems that inhabit some complex dynamic environment, sense and act autonomously in this environment, and by doing so realize a set of goals or tasks for which they are designed. Franklin and Graesser  1997  review different definitions and propose their definition   An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.  They explain it   Humans and some animals are at the high end of being an agent, with multiple, conflicting drives, multiples senses, multiple possible actions, and complex sophisticated control structures. At the low end, with one or two senses, a single action, and an absurdly simple control structure we find a thermostat.  Lee et al.  2015  post safety issue from how the combination of external appearance and internal autonomous agent have impact on human reaction about autonomous vehicles. Their study explores the humanlike appearance agent and high level of autonomy are strongly correlated with social presence, intelligence, safety and trustworthiness. In specific, appearance impacts most on affective trust while autonomy impacts most on both affective and cognitive domain of trust where cognitive trust is characterized by knowledge based factors and affective trust is largely emotion driven  
Babelfy is a software algorithm for the disambiguation of text written in any language.  Specifically, Babelfy performs the tasks of multilingual Word Sense Disambiguation  i.e., the disambiguation of common nouns, verbs, adjectives and adverbs  and Entity Linking  i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc. . Babelfy is based on the BabelNet multilingual semantic network and performs disambiguation and entity linking in three steps  As a result, the text, written in any of the 271 languages supported by BabelNet, is output with possibly overlapping semantic annotations.  This programming language related article is a stub. You can help Wikipedia by expanding it.This computational linguistics related article is a stub. You can help Wikipedia by expanding it.This semantics article is a stub. You can help Wikipedia by expanding it.
 Batch normalization  also known as batch norm  is a method used to make training of artificial neural networks faster and more stable through normalization of the layers  inputs by re centering and re scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015. While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others maintain that batch normalization achieves length direction decoupling, and thereby accelerates neural networks. More recently a normalize gradient clipping technique and smart hyperparameter tuning has been introduced in Normalizer Free Nets, so called  NF Nets  which mitigates the need for batch normalization. Each layer of a neural network has inputs with a corresponding distribution, which is affected during the training process by the randomness in the parameter initialization and the randomness in the input data. The effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift. Although a clear cut precise definition seems to be missing, the phenomenon observed in experiments is the change on means and variances of the inputs to internal layers during training. Batch normalization was initially proposed to mitigate internal covariate shift. During the training stage of networks, as the parameters of the preceding layers change, the distribution of inputs to the current layer changes accordingly, such that the current layer needs to constantly readjust to new distributions. This problem is especially severe for deep networks, because small changes in shallower hidden layers will be amplified as they propagate within the network, resulting in significant shift in deeper hidden layers. Therefore, the method of batch normalization is proposed to reduce these unwanted shifts to speed up training and to produce more reliable models. Besides reducing internal covariate shift, batch normalization is believed to introduce many other benefits. With this additional operation, the network can use higher learning rate without vanishing or exploding gradients. Furthermore, batch normalization seems to have a regularizing effect such that the network improves its generalization properties, and it is thus unnecessary to use dropout to mitigate overfitting. It has been observed also that with batch norm the network becomes more robust to different initialization schemes and learning rates. In a neural network, batch normalization is achieved through a normalization step that fixes the means and variances of each layer s inputs. Ideally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini batch in the training process. Let us use B to denote a mini batch of size m of the entire training set. The empirical mean and variance of B could thus be denoted as         B       1 m       i   1   m    x  i        sum   x     and         B   2       1 m       i   1   m      x  i         B       2         sum    x   mu       . For a layer of the network with d dimensional input,     x      x    1     , . . . ,  x    d         ,...,x     , each dimension of its input is then normalized  i.e. re centered and re scaled  separately,         x       i     k           x  i     k           B     k               B     k         2                   mu      right    epsilon       , where     k     1 , d        and      i     1 , m                 B     k            and         B     k            are the per dimension mean and standard deviation, respectively.            is added in the denominator for numerical stability and is an arbitrarily small constant. The resulting normalized activation         x         k            have zero mean and unit variance, if            is not taken into account. To restore the representation power of the network, a transformation step then follows as      y  i     k             k         x       i     k             k          gamma       beta     , where the parameters           k           and           k           are subsequently learned in the optimization process. Formally, the operation that implements batch normalization is a transform     B  N        k     ,       k          x  1... m     k        y  1... m     k       , beta    x   rightarrow y      called the Batch Normalizing transform. The output of the BN transform      y    k       B  N        k     ,       k          x    k          BN , beta    x      is then passed to other network layers, while the normalized output          x       i     k              remains internal to the current layer. The described BN transform is a differentiable operation, and the gradient of the loss l  with respect to the different parameters can be computed directly with the chain rule. Specifically,          l      y  i     k                 depends on the choice of activation function, and the gradient against other parameters could be expressed as a function of          l      y  i     k                           l         x       i     k               l      y  i     k              k                 gamma     ,          l           k              i   1   m        l      y  i     k            x       i     k           sum            ,          l           k              i   1   m        l      y  i     k              sum         ,         l         B     k     2              i   1   m        l      y  i     k           x  i     k           B     k                     k     2         B     k     2               3     2               sum       x    mu     left      sigma      epsilon    right    ,          l         B     k              i   1   m        l      y  i     k                   k           B     k     2                   l         B     k     2          1 m       i   1   m       2        x  i     k           B     k              sum           epsilon           sum     2  cdot  x    mu       , and          l      x  i     k               l         x       i     k          1      B     k     2                   l         B     k     2           2    x  i     k           B     k        m          l         B     k          1 m                  epsilon            mu              . During the training stage, the normalization steps depend on the mini batches to ensure efficient and reliable training. However, in the inference stage, this dependence is not useful any more. Instead, the normalization step in this stage is computed with the population statistics such that the output could depend on the input in a deterministic manner. The population mean,     E    x    k           , and variance,     Var      x    k             , are computed as      E    x    k          E  B         B     k            , and     Var      x    k           m  m   1     E  B             B     k         2          E    . The population statistics thus is a complete representation of the mini batches. The BN transform in the inference step thus becomes      y    k       B  N        k     ,       k       inf      x    k               k         x    k       E    x    k         Var      x    k                      k        BN , beta      x    gamma   E    epsilon      beta     , where      y    k           is passed on to future layers instead of      x    k          . Since the parameters are fixed in this transformation, the batch normalization procedure is essentially applying a linear transform to the activation. Although batch normalization has become popular due to its strong empirical performance, the working mechanism of the method is not yet well understood. The explanation made in the original paper was that batch norm works by reducing internal covariate shift, but this has been challenged by more recent work. One experiment trained a VGG 16 network under 3 different training regimes  standard  no batch norm , batch norm, and batch norm with noise added to each layer during training. In the third model, the noise has non zero mean and non unit variance, i.e. it explicitly introduces covariate shift. Despite this, it showed similar accuracy to the second model, and both performed better than the first, suggesting that covariate shift is not the reason that batch norm improves performance. One alternative explanation, is that the improvement with batch normalization is instead due to it producing a smoother parameter space and smoother gradients, as formalized by a smaller Lipschitz constant.  Consider two identical networks, one contains batch normalization layers and the other doesn t, the behaviors of these two networks are then compared. Denote the loss functions as        L             and     L     , respectively. Let the input to both networks be     x     , and the output be     y     , for which     y   W x     , where     W      is the layer weights. For the second network,     y      additionally goes through a batch normalization layer. Denote the normalized activation as        y            , which has zero mean and unit variance. Let the transformed activation be     z        y               beta    , and suppose            and            are constants. Finally, denote the standard deviation over a mini batch         y  j            R   m        in  mathbb       as         j        . First, it can be shown that the gradient magnitude of a batch normalized network,                  y  i        L                       , is bounded, with the bound expressed as                  y  i        L                2           2       j   2                        y  i     L           2       1 m     1 ,      y  i     L     2       1 m          y  i     L ,     y       j       2                 leq        triangledown   L      langle 1, triangledown   L rangle     langle  triangledown   L,   rangle     . Since the gradient magnitude represents the Lipschitzness of the loss, this relationship indicates that a batch normalized network could achieve greater Lipschitzness comparatively. Notice that the bound gets tighter when the gradient          y  i        L              correlates with the activation         y  i               , which is a common phenomena. The scaling of           2       j   2               is also significant, since the variance is often large. Secondly, the quadratic form of the loss Hessian with respect to activation in the gradient direction can be bounded as            y  j        L          T           L           y  j      y  j             y  j        L                2       2                   L           y  j              T              L      y  j      y  j                          L           y  j                   m     2             y  j     L ,     y  j                              L           y  j                    2            partial y     triangledown      leq          partial y           langle  triangledown   L,   rangle         . The scaling of           2       j   2               indicates that the loss Hessian is resilient to the mini batch variance, whereas the second term on the right hand side suggests that it becomes smoother when the Hessian and the inner product are non negative. If the loss is locally convex, then the Hessian is positive semi definite, while the inner product is positive if         g  j                is in the direction towards the minimum of the loss. It could thus be concluded from this inequality that the gradient generally becomes more predictive with the batch normalization layer. It then follows to translate the bounds related to the loss with respect to the normalized activation to a bound on the loss with respect to the network weights          g  j                2       j   2        g  j   2     m      g  j     2         2          y  j     L ,     y       j       2          leq      g   m mu      lambda   langle  triangledown   L,   rangle      , where      g  j     m a  x          X                           W   L           2      max    triangledown  L       and         g       j     m a  x          X                           W      L                2        max    triangledown         . In addition to the smoother landscape, it is further shown that batch normalization could result in a better initialization with the following inequality               W  0         W                     2              W  0      W                2       1           W                2                 W                2        W      ,  W  0         2            leq   W  W             W      langle W ,W  rangle      , where      W            and         W                   are the local optimal weights for the two networks, respectively. Some scholars argue that the above analysis cannot fully capture the performance of batch normalization, because the proof only concerns the largest eigenvalue, or equivalently, one direction in the landscape at all points. It is suggested that the complete eigenspectrum needs to be taken into account to make a conclusive analysis.  Since it is hypothesized that batch normalization layers could reduce internal covariate shift, an experiment is set up to measure quantitatively how much covariate shift is reduced. First, the notion of internal covariate shift needs to be defined mathematically. Specifically, to quantify the adjustment that a layer s parameters make in response to updates in previous layers, the correlation between the gradients of the loss before and after all previous layers are updated is measured, since gradients could capture the shifts from the first order training method. If the shift introduced by the changes in previous layers is small, then the correlation between the gradients would be close to 1. The correlation between the gradients are computed for four models  a standard VGG network, a VGG network with batch normalization layers, a 25 layer deep linear network  DLN  trained with full batch gradient descent, and a DLN network with batch normalization layers. Interestingly, it is shown that the standard VGG and DLN models both have higher correlations of gradients compared with their counterparts, indicating that the additional batch normalization layers are not reducing internal covariate shift. Even though batchnorm was originally introduced to alleviate gradient vanishing or explosion problems, a deep batchnorm network in fact suffers from gradient explosion at initialization time, no matter what it uses for nonlinearity. Thus the optimization landscape is very far from smooth for a randomly initialized, deep batchnorm network. More precisely, if the network has     L      layers, then the gradient of the first layer weights has norm       c     L         for some         1 , c   0      depending only on the nonlinearity. For any fixed nonlinearity,            decreases as the batch size increases. For example, for ReLU,            decreases to                 1     1.467      as the batch size tends to infinity. Practically, this means deep batchnorm networks are untrainable. This is only relieved by skip connections in the fashion of residual networks. This gradient explosion on the surface contradicts the smoothness property explained in the previous section, but in fact they are consistent. The previous section studies the effect of inserting a single batchnorm in a network, while the gradient explosion depends on stacking batchnorms typical of modern deep neural networks. Another possible reason for the success of batch normalization is that it decouples the length and direction of the weight vectors and thus facilitates better training. By interpreting batch norm as a reparametrization of weight space, it can be shown that the length and the direction of the weights are separated and can thus be trained separately. For a particular neural network unit with input     x      and weight vector     w     , denote its output as     f   w      E  x          x  T   w          , where            is the activation function, and denote     S   E   x  x  T         . Assume that     E   x     0     , and that the spectrum of the matrix     S      is bounded as     0           m i n     S      S    ,     L       m a x     S          S   infty    , such that     S      is symmetric positive definite. Adding batch normalization to this unit thus results in      f  B N     w ,   ,        E  x         B N    x  T   w          E  x                         x  T   w    E  x      x  T   w     v a  r  x      x  T   w     1     2                           w, gamma , beta   E  E       beta    , by definition. The variance term can be simplified such that     v a  r  x      x  T   w      w  T   S w    w Sw   . Assume that     x      has zero mean and            can be omitted, then it follows that      f  B N     w ,        E  x                       x  T   w      w  T   S w     1     2                     w, gamma   E    , where        w  T   S w      1 2      Sw       is the induced norm of     S     ,             w           s        . Hence, it could be concluded that      f  B N     w ,        E  x          x  T      w             w, gamma   E    , where        w            w          w           s           gamma      , and            and     w      accounts for its length and direction separately. This property could then be used to prove the faster convergence of problems with batch normalization. With the reparametrization interpretation, it could then be proved that applying batch normalization to the ordinary least squares problem achieves a linear convergence rate in gradient descent, which is faster than the regular gradient descent with only sub linear convergence. Denote the objective of minimizing an ordinary least squares problem as     m i  n     w         R  d      f  O L S        w          m i  n     w         R  d        E  x , y       y    x  T      w          2         m i  n     w         R  d       2  u  T      w            w       T   S    w            in R  f     min   in R   E   min   in R   2u     S     , where     u   E     y x       . Since        w            w          w           s           gamma      , the objective thus becomes     m i  n  w    R  d       0   ,     R    f  O L S     w ,       m i  n  w    R  d       0   ,     R         2       u  T   w           w           S         2               backslash  , gamma  in R f  w, gamma   min  backslash  , gamma  in R 2 gamma w   gamma       , where 0 is excluded to avoid 0 in the denominator. Since the objective is convex with respect to           , its optimal value could be calculated by setting the partial derivative of the objective against            to 0. The objective could be further simplified to be     m i  n  w    R  d       0         w     m i  n  w    R  d       0                 w  T   u  u  T   w    w  T   S w             backslash    rho  w  min  backslash    uu w Sw     . Note that this objective is a form of the generalized Rayleigh quotient                 w         w  T   B w    w  T   A w        w  Bw Aw     , where     B    R  d   d         is a symmetric matrix and     A    R  d   d         is a symmetric positive definite matrix. It is proven that the gradient descent convergence rate of the generalized Rayleigh quotient is            1          w  t   1            w  t   1         2                1          1         2         1         m i n              2 t          1          w  t            w  t           2          rho  w     lambda      leq 1   lambda     lambda       rho  w      lambda       , where         1         is the largest eigenvalue of     B     ,         2         is the second largest eigenvalue of     B     , and         m i n         is the smallest eigenvalue of     B     . In our case,     B   u  u  T        is a rank one matrix, and the convergence result can be simplified accordingly. Specifically, consider gradient descent steps of the form      w  t   1      w  t         t          w  t        w   eta   triangledown  rho  w      with step size         t         w  t   T   S  w  t     2 L          w  t                Sw         , and starting from          w  0       0     neq 0   , then          w  t            w                1       L           2 t          w  0            w               rho  w   leq 1     rho  w    rho  w      . The problem of learning halfspaces refers to the training of the Perceptron, which is the simplest form of neural network. The optimization problem in this case is     m i  n     w         R  d      f  L H        w           E  y , x          z  T      w              in R  f     E    , where     z     y x      and            is an arbitrary loss function. Suppose that            is infinitely differentiable and has a bounded derivative. Assume that the objective function      f  L H         is            smooth, and that a solution               a r g m i  n                f     w             2      argmin    triangledown f  alpha w        exists and is bounded such that                           infty    . Also assume     z      is a multivariate normal random variable. With the Gaussian assumption, it can be shown that all critical points lie on the same line, for any choice of loss function           . Specifically, the gradient of      f  L H         could be represented as           w        f  L H        w           c  1        w        u    c  2        w        S    w         f     c    u c    S    , where       c  1        w           E  z           1        z  T      w             E  z           2        z  T      w             u  T      w              E  E  u      ,      c  2        w           E  z           2        z  T      w                E    , and           i           is the     i      th derivative of           . By setting the gradient to 0, it thus follows that the bounded critical points         w                   can be expressed as         w              g       S    1   u      g S u   , where      g            depends on         w                   and           . Combining this global property with length direction decoupling, it could thus be proved that this optimization problem converges linearly. First, a variation of gradient descent with batch normalization, Gradient Descent in Normalized Parameterization  GDNP , is designed for the objective function     m i  n  w    R  d       0   ,     R    f  L H     w ,        backslash  , gamma  in R f  w, gamma     , such that the direction and length of the weights are updated separately. Denote the stopping criterion of GDNP as     h    w  t   ,     t        E  z              z  T       w       t          u  T    w  t        E  z              z  T       w       t          u  T    w  t       2     , gamma    E  u w   E  u w      . Let the step size be      s  t     s    w  t   ,     t                     w  t             S   3     L  g  t   h    w  t   ,     t           s w , gamma          h w , gamma        . For each step, if     h    w  t   ,     t       0   , gamma    neq 0   , then update the direction as      w  t   1      w  t      s  t       w   f    w  t   ,     t        w  s  triangledown  f w , gamma      . Then update the length according to         t     B i s e c t i o n    T  s   , f ,  w  t        Bisection T ,f,w     , where     B i s e c t i o n          is the classical bisection algorithm, and      T  s         is the total iterations ran in the bisection step. Denote the total number of iterations as      T  d        , then the final output of GDNP is         w        T  d            T  d        w   T  d               w   T  d               S             gamma              . The GDNP algorithm thus slightly modifies the batch normalization step for the ease of mathematical analysis. It can be shown that in GDNP, the partial derivative of      f  L H        against the length component converges to zero at a linear rate, such that                f  L H      w  t   ,  a  t      T  s           2         2     T  s            b  t     0        a  t     0              2       f  w ,a       leq   zeta  b   a         , where      a  t     0            and      b  t   0          are the two starting points of the bisection algorithm on the left and on the right, correspondingly. Further, for each iteration, the norm of the gradient of      f  L H         with respect to     w      converges linearly, such that              w  t             S   2              f  L H      w  t   ,  g  t                S    1     2           1       L           2 t       2       t   2          w  0                          triangledown f  w ,g        leq 1    Phi   gamma     rho  w    rho      . Combining these two inequalities, a bound could thus be obtained for the gradient with respect to         w        T  d                                 w       f       w        T  d                 2           1       L           2  T  d         2          w  0                       2     T  s            b  t     0        a  t     0              2        f         leq 1     Phi    rho  w    rho      zeta  b   a         , such that the algorithm is guaranteed to converge linearly. Although the proof stands on the assumption of Gaussian input, it is also shown in experiments that GDNP could accelerate optimization without this constraint. Consider a multilayer perceptron  MLP  with one hidden layer and     m      hidden units with mapping from input     x    R  d         to a scalar output described as      F  x        W      ,           i   1   m       i        x  T       w         i           , Theta    sum    theta   phi  x       , where         w         i             and         i         are the input and output weights of unit     i      correspondingly, and            is the activation function and is assumed to be a tanh function. The input and output weights could then be optimized with     m i  n     W      ,        f  N N        W      ,        E  y , x     l     y  F  x        W      ,              , Theta   f   , Theta   E     , where     l      is a loss function,        W              w         1     , . . . ,     w         m              ,...,       , and                 1     , . . . ,       m         ,..., theta       . Consider fixed            and optimizing only        W            , it can be shown that the critical points of      f  N N        W                 of a particular hidden unit     i     ,         w         i            , all align along one line depending on incoming information into the hidden layer, such that         w         i           c         i      S    1   u        S u   , where         c         i       R      in R    is a scalar,     i   1 , . . . , m     . This result could be proved by setting the gradient of      f  N N         to zero and solving the system of equations. Apply the GDNP algorithm to this optimization problem by alternating optimization over the different hidden units. Specifically, for each hidden unit, run GDNP to find the optimal     W      and           . With the same choice of stopping criterion and stepsize, it follows that                     w         i       f       w       t     i                  S    1     2           1       L           2 t   C        w  0                       2     T  s     i              b  t     0        a  t     0              2          f           leq 1   C  rho  w    rho       zeta  b   a         . Since the parameters of each hidden unit converge linearly, the whole optimization problem has a linear rate of convergence. 
Bayesian programming is a formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available. Edwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory  The Logic of Science he developed this theory and proposed what he called  the robot,  which was not a physical device, but an inference engine to automate probabilistic reasoning a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this  robot . Bayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs. A Bayesian program is a means of specifying a family of probability distributions. The constituent elements of a Bayesian program are presented below  The purpose of a description is to specify an effective method of computing a joint probability distribution on a set of variables          X  1   ,  X  2   ,   ,  X  N         ,X , cdots ,X  right      given a set of experimental data            and some specification           . This joint distribution is denoted as      P      X  1      X  2          X  N                  wedge X  wedge  cdots  wedge X  mid  delta  wedge  pi  right    . To specify preliminary knowledge           , the programmer must undertake the following  Given a partition of          X  1   ,  X  2   ,   ,  X  N         ,X , ldots ,X  right      containing     K      subsets,     K      variables are defined      L  1   ,   ,  L  K     , cdots ,L    , each corresponding to one of these subsets. Each variable      L  k         is obtained as the conjunction of the variables          X   k  1     ,  X   k  2     ,          ,X  , cdots  right      belonging to the      k  t h         subset. Recursive application of Bayes  theorem leads to  Conditional independence hypotheses then allow further simplifications. A conditional independence hypothesis for variable      L  k         is defined by choosing some variable      X  n         among the variables appearing in the conjunction      L  k   1          L  2      L  1      wedge  cdots  wedge L  wedge L    , labelling      R  k         as the conjunction of these chosen variables and setting  We then obtain  Such a simplification of the joint distribution as a product of simpler distributions is called a decomposition, derived using the chain rule. This ensures that each variable appears at the most once on the left of a conditioning bar, which is the necessary and sufficient condition to write mathematically valid decompositions. Each distribution     P      L  k      R  k                  mid R  wedge  delta  wedge  pi  right     appearing in the product is then associated with either a parametric form  i.e., a function      f          L  k         left L  right      or a question to another Bayesian program     P      L  k      R  k                 P     L   R                                mid R  wedge  delta  wedge  pi  right  P left L mid R wedge   wedge   right    . When it is a form      f          L  k         left L  right    , in general,            is a vector of parameters that may depend on      R  k         or            or both. Learning takes place when some of these parameters are computed using the data set           . An important feature of Bayesian Programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program.     P      L  k      R  k                  mid R  wedge  delta  wedge  pi  right     is obtained by some inferences done by another Bayesian program defined by the specifications                      and the data                     . This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models. Given a description  i.e.,     P      X  1      X  2          X  N                  wedge X  wedge  cdots  wedge X  mid  delta  wedge  pi  right     , a question is obtained by partitioning          X  1   ,  X  2   ,   ,  X  N         ,X , cdots ,X  right      into three sets  the searched variables, the known variables and the free variables. The 3 variables     S e a r c h e d     ,     K n o w n      and     F r e e      are defined as the conjunction of the variables belonging to these sets. A question is defined as the set of distributions  made of many  instantiated questions  as the cardinal of     K n o w n     , each instantiated question being the distribution  Given the joint distribution     P      X  1      X  2          X  N                  wedge X  wedge  cdots  wedge X  mid  delta  wedge  pi  right    , it is always possible to compute any possible question using the following general inference  where the first equality results from the marginalization rule, the second results from Bayes  theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant     Z     . Theoretically, this allows to solve any Bayesian inference problem. In practice, however, the cost of computing exhaustively and exactly     P      Searched     Known                  mid   wedge  delta  wedge  pi  right     is too great in almost all cases. Replacing the joint distribution by its decomposition we get  which is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions. The purpose of Bayesian spam filtering is to eliminate junk e mails. The problem is very easy to formulate. E mails should be classified into one of two categories  non spam or spam. The only available information to classify the e mails is their content  a set of words. Using these words without taking the order into account is commonly called a bag of words model. The classifier should furthermore be able to adapt to its user and to learn from experience. Starting from an initial standard setting, the classifier should modify its internal parameters when the user disagrees with its own decision. It will hence adapt to the user s criteria to differentiate between non spam and spam. It will improve its results as it encounters increasingly classified e mails. The variables necessary to write this program are as follows  These     N   1      binary variables sum up all the information about an e mail. Starting from the joint distribution and applying recursively Bayes  theorem we obtain  This is an exact mathematical expression. It can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text  spam or not  is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model. For instance, the programmer can assume that  to finally obtain  This kind of assumption is known as the naive Bayes  assumption. It is  naive  in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is. To be able to compute the joint distribution, the programmer must now specify the     N   1      distributions appearing in the decomposition  where      a  f   n          stands for the number of appearances of the      n  t h         word in non spam e mails and      a  f         stands for the total number of non spam e mails. Similarly,      a  t   n          stands for the number of appearances of the      n  t h         word in spam e mails and      a  t         stands for the total number of spam e mails. The     N       forms     P    W  n      Spam       mid       are not yet completely specified because the     2 N   2      parameters      a  f   n   0 ,   , N   1         ,      a  t   n   0 ,   , N   1         ,      a  f         and      a  t         have no values yet. The identification of these parameters could be done either by batch processing a series of classified e mails or by an incremental updating of the parameters using the user s classifications of the e mails as they arrive. Both methods could be combined  the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user. The question asked to the program is   what is the probability for a given text to be spam knowing which words appear and don t appear in this text?  It can be formalized by  which can be computed as follows  The denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio  This computation is faster and easier because it requires only     2 N      products. The Bayesian spam filter program is completely defined by  Bayesian filters  often called Recursive Bayesian estimation  are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance  the Kalman filter or the Hidden Markov model  HMM . The decomposition is based  The parametrical forms are not constrained and different choices lead to different well known models  see Kalman filters and Hidden Markov models just below. The typical question for such models is     P      S  t   k      O  0          O  t          mid O  wedge  cdots  wedge O  right      what is the probability distribution for the state at time     t   k      knowing the observations from instant     0      to     t     ? The most common case is Bayesian filtering where     k   0     , which searches for the present state, knowing past observations. However, it is also possible       k   0       , to extrapolate a future state from past observations, or to do smoothing       k   0       , to recover a past state from observations made either before or after that instant. More complicated questions may also be asked as shown below in the HMM section. Bayesian filters       k   0        have a very interesting recursive property, which contributes greatly to their attractiveness.     P      S  t        O  0          O  t          O  wedge  cdots  wedge O  right     may be computed simply from     P      S  t   1      O  0          O  t   1          mid O  wedge  cdots  wedge O  right     with the following formula  Another interesting point of view for this equation is to consider that there are two phases  a prediction phase and an estimation phase  The very well known Kalman filters are a special case of Bayesian filters. They are defined by the following Bayesian program  With these hypotheses and by using the recursive formula, it is possible to solve the inference problem analytically to answer the usual     P    S  T      O  0          O  T            mid O  wedge  cdots  wedge O  wedge  pi      question. This leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications. When there are no obvious linear transition and observation models, it is still often possible, using a first order Taylor s expansion, to treat these models as locally linear. This generalization is commonly called the extended Kalman filter. Hidden Markov models  HMMs  are another very popular specialization of Bayesian filters. They are defined by the following Bayesian program  both specified using probability matrices. What is the most probable series of states that leads to the present state, knowing the past observations? This particular question may be answered with a specific and very efficient algorithm called the Viterbi algorithm. The Baum Welch algorithm has been developed for HMMs. Since 2000, Bayesian programming has been used to develop both robotics applications and life sciences models. In robotics, bayesian programming was applied to autonomous robotics, robotic CAD systems, advanced driver assistance systems, robotic arm control, mobile robotics, human robot interaction, human vehicle interaction  Bayesian autonomous driver models  video game avatar programming and training  and real time strategy games  AI . In life sciences, bayesian programming was used in vision to reconstruct shape from motion, to model visuo vestibular interaction and to study saccadic eye movements  in speech perception and control to study early speech acquisition and the emergence of articulatory acoustic systems  and to model handwriting perception and control. Bayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of compositionality  building abstract representations from parts , causality  building complexity from parts  and learning to learn  using previously recognized concepts to ease the creation of new concepts . The comparison between probabilistic approaches  not only bayesian programming  and possibility theories continues to be debated. Possibility theories like, for instance, fuzzy sets, fuzzy logic and possibility theory are alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete uncertain knowledge. The defense of probability is mainly based on Cox s theorem, which starts from four postulates concerning rational reasoning in the presence of uncertainty. It demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument is that any approach other than probability necessarily infringes one of these postulates and the value of that infringement. The purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling  especially bayesian networks  to deal with uncertainty while profiting from the programming languages  expressiveness to encode complexity. Extended classical programming languages include logical languages as proposed in Probabilistic Horn Abduction, Independent Choice Logic, PRISM, and ProbLog which proposes an extension of Prolog. It can also be extensions of functional programming languages  essentially Lisp and Scheme  such as IBAL or CHURCH. The underlying programming languages can be object oriented as in BLOG and FACTORIE or more standard ones as in CES and FIGARO. The purpose of Bayesian programming is different. Jaynes  precept of  probability as logic  argues that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming attempts to replace classical languages with a programming approach based on probability that considers incompleteness and uncertainty. The precise comparison between the semantics and power of expression of Bayesian and probabilistic programming is an open question. 
  Behavior informatics  BI  is the informatics of behaviors so as to obtain behavior intelligence and behavior insights. BI is a research method combining science and technology, specifically in the area of engineering. The purpose of BI includes analysis of current behaviors as well as the inference of future possible behaviors. This occurs through pattern recognition. Different from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and or organizations. BI is built on classic study of behavioral science, including behavior modeling, applied behavior analysis, behavior analysis, behavioral economics, and organizational behavior. Typical BI tasks consist of individual and group behavior formation, representation, computational modeling, analysis, learning, simulation, and understanding of behavior impact, utility, non occurring behaviors etc. for behavior intervention and management. The Behavior Informatics approach to data utilizes cognitive as well as behavioral data. By combining the data, BI has the potential to effectively illustrate the big picture when it comes to behavioral decisions and patterns. One of the goals of BI is also to be able to study human behavior while eliminating issues like self report bias. This creates more reliable and valid information for research studies.  Behavior informatics covers behavior analytics which focuses on analysis and learning of behavioral data. From an Informatics perspective, a behavior consists of three key elements  A behavior can be represented as a behavior vector, all behaviors of an actor or an actor group can be represented as behavior sequences and multi dimensional behavior matrix. The following table explains some of the elements of behavior.  Behavior Informatics takes into account behavior when analyzing business patterns and intelligence. The inclusion of behavior in these analyses provides prominent information on social and driving factors of patterns. Behavior Informatics is being used in a variety of settings, including but not limited to health care management, telecommunications, and marketing. Behavior Informatics is a turning point for the health care system. Behavior Informatics provides a manner in which to analyze and organize the many aspects that go into a person s health care needs and decisions. When it comes to business models, behavior informatics may be utilized for a similar role. Organizations implement behavior informatics to enhance business structure and regime where it helps moderate ideal business decisions and situations.  
In artificial intelligence, a behavior selection algorithm, or action selection algorithm, is an algorithm that selects appropriate behaviors or actions for one or more intelligent agents. In game artificial intelligence, it selects behaviors or actions for one or more non player characters. Common behavior selection algorithms include  In application programming, run time selection of the behavior of a specific method is referred to as the strategy design pattern.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
For popular psychology, the belief desire intention  BDI  model of human practical reasoning was developed by Michael Bratman as a way of explaining future directed intention. BDI is fundamentally reliant on folk psychology  the  theory theory  , which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief desire intention software model. BDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions. BDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.This philosophy related article is a stub. You can help Wikipedia by expanding it.
The belief desire intention software model  BDI  is a software model developed for programming intelligent agents. Superficially characterized by the implementation of an agent s beliefs, desires and intentions, it actually uses these concepts to solve a particular problem in agent programming. In essence, it provides a mechanism for separating the activity of selecting a plan  from a plan library or an external planner application  from the execution of currently active plans. Consequently, BDI agents are able to balance the time spent on deliberating about plans  choosing what to do  and executing those plans  doing it . A third activity, creating the plans in the first place  planning , is not within the scope of the model, and is left to the system designer and programmer. In order to achieve this separation, the BDI software model implements the principal aspects of Michael Bratman s theory of human practical reasoning  also referred to as Belief Desire Intention, or BDI . That is to say, it implements the notions of belief, desire and  in particular  intention, in a manner inspired by Bratman. For Bratman, desire and intention are both pro attitudes  mental attitudes concerned with action . He identifies commitment as the distinguishing factor between desire and intention, noting that it leads to  1  temporal persistence in plans and  2  further plans being made on the basis of those to which it is already committed. The BDI software model partially addresses these issues. Temporal persistence, in the sense of explicit reference to time, is not explored. The hierarchical nature of plans is more easily implemented  a plan consists of a number of steps, some of which may invoke other plans. The hierarchical definition of plans itself implies a kind of temporal persistence, since the overarching plan remains in effect while subsidiary plans are being executed. An important aspect of the BDI software model  in terms of its research relevance  is the existence of logical models through which it is possible to define and reason about BDI agents. Research in this area has led, for example, to the axiomatization of some BDI implementations, as well as to formal logical descriptions such as Anand Rao and Michael Georgeff s BDICTL. The latter combines a multiple modal logic  with modalities representing beliefs, desires and intentions  with the temporal logic CTL .  More recently, Michael Wooldridge has extended BDICTL to define LORA  the Logic Of Rational Agents , by incorporating an action logic. In principle, LORA allows reasoning not only about individual agents, but also about communication and other interaction in a multi agent system. The BDI software model is closely associated with intelligent agents, but does not, of itself, ensure all the characteristics associated with such agents. For example, it allows agents to have private beliefs, but does not force them to be private. It also has nothing to say about agent communication. Ultimately, the BDI software model is an attempt to solve a problem that has more to do with plans and planning  the choice and execution thereof  than it has to do with the programming of intelligent agents. This approach has recently been proposed by Steven Umbrello and Roman Yampolskiy as a means of designing autonomous vehicles for human values. A BDI agent is a particular type of bounded rational software agent, imbued with particular mental attitudes, viz  Beliefs, Desires and Intentions  BDI . This section defines the idealized architectural components of a BDI system. BDI was also extended with an obligations component, giving rise to the BOID agent architecture to incorporate obligations, norms and commitments of agents that act within a social environment. This section defines an idealized BDI interpreter that provides the basis of SRI s PRS lineage of BDI systems  The BDI software model is one example of a reasoning architecture for a single rational agent, and one concern in a broader multi agent system. This section bounds the scope of concerns for the BDI software model, highlighting known limitations of the architecture. 
A blackboard system is an artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the  blackboard , is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution.  Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state.  In this way, the specialists work together to solve the problem.  The blackboard model was originally designed as a way to handle complex, ill defined problems, where the solution is the sum of its parts. The following scenario provides a simple metaphor that gives some insight into how a blackboard functions  A group of specialists are seated in a room with a large blackboard. They work as a team to brainstorm a solution to a problem, using the blackboard as the workplace for cooperatively developing the solution. The session begins when the problem specifications are written onto the blackboard. The specialists all watch the blackboard, looking for an opportunity to apply their expertise to the developing solution. When someone writes something on the blackboard that allows another specialist to apply their expertise, the second specialist records their contribution on the blackboard, hopefully enabling other specialists to then apply their expertise. This process of adding contributions to the blackboard continues until the problem has been solved. A blackboard system application consists of three major components A blackboard system is the central space in a multi agent system. It s used for describing the world as a communication platform for agents. To realize a blackboard in a computer program, a machine readable notation is needed in which facts can be stored. One attempt in doing so is a SQL database, another option is the Learnable Task Modeling Language  LTML . The syntax of the LTML planning language is similar to PDDL, but adds extra features like control structures and OWL S models. LTML was developed in 2007 as part of a much larger project called POIROT  Plan Order Induction by Reasoning from One Trial , which is a Learning from demonstrations framework for process mining. In POIROT, Plan traces and hypotheses are stored in the LTML syntax for creating semantic web services. Here is a small example  A human user is executing a workflow in a computer game. The user presses some buttons and interacts with the game engine. While the user interacts with the game, a plan trace is created. That means the user s actions are stored in a logfile. The logfile gets transformed into a machine readable notation which is enriched by semantic attributes. The result is a textfile in the LTML syntax which is put on the blackboard. Agents  software programs in the blackboard system  are able to parse the LTML syntax. We start by discussing two well known early blackboard systems, BB1 and GBB, below and then discuss more recent implementations and applications. The BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip, used task planning as a simplified example of tactical planning for the Office of Naval Research. Hayes Roth   Hayes Roth found that human planning was more closely modeled as an opportunistic process, in contrast to the primarily top down planners used at the time  While not incompatible with successive refinement models, our view of planning is somewhat different. We share the assumption that planning processes operate in a two dimensional planning space defined on time and abstraction dimensions. However, we assume that people s planning activity is largely opportunistic. That is, at each point in the process, the planner s current decisions and observations suggest various opportunities for plan development. The planner s subsequent decisions follow up on selected opportunities. Sometimes, these decision sequences follow an orderly path and produce a neat top down expansion as described above. However, some decisions and observations might also suggest less orderly opportunities for plan development. A key innovation of BB1 was that it applied this opportunistic planning model to its own control, using the same blackboard model of incremental, opportunistic, problem solving that was applied to solve domain problems. Meta level reasoning with control knowledge sources could then monitor whether planning and problem solving were proceeding as expected or stalled. If stalled, BB1 could switch from one strategy to another as conditions   such as the goals being considered or the time remaining   changed. BB1 was applied in multiple domains  construction site planning, inferring 3 D protein structures from X ray crystallography, intelligent tutoring systems, and real time patient monitoring. BB1 also allowed domain general language frameworks to be designed for wide classes of problems. For example, the ACCORD  language framework defined a particular approach to solving configuration problems. The problem solving approach was to incrementally assemble a solution by adding objects and constraints, one at a time. Actions in the ACCORD language framework appear as short English like commands or sentences for specifying preferred actions, events to trigger KSes, preconditions to run a KS action, and obviation conditions to discard a KS action that is no longer relevant. GBB  focused on efficiency, in contrast to BB1, which focused more on sophisticated reasoning and opportunistic planning. GBB improves efficiency by allowing blackboards to be multi dimensional, where dimensions can be either ordered or not, and then by increasing the efficiency of pattern matching. GBB1, one of GBB s control shells implements BB1 s style of control while adding efficiency improvements. Other well known of early academic blackboard systems are the Hearsay II speech recognition system and Douglas Hofstadter s Copycat and Numbo projects. Some more recent examples of deployed real world applications include  Blackboard systems are used routinely in many military C4ISTAR systems for detecting and tracking objects. Another example of current use is in Game AI, where they are considered a standard AI tool to help with adding AI to video games. Blackboard like systems have been constructed within modern Bayesian machine learning settings, using agents to add and remove Bayesian network nodes.  In these  Bayesian Blackboard  systems, the heuristics can acquire more rigorous probabilistic meanings as proposal and acceptances in Metropolis Hastings sampling though the space of possible structures.  Conversely, using these mappings, existing Metropolis Hastings samplers over structural spaces may now thus be viewed as forms of blackboard systems even when not named as such by the authors.   Such samplers are commonly found in musical transcription algorithms for example. Blackboard systems have also been used to build large scale intelligent systems for the annotation of media content, automating parts of traditional social science research. In this domain, the problem of integrating various AI algorithms into a single intelligent system arises spontaneously, with blackboards providing a way for a collection of distributed, modular natural language processing algorithms to each annotate the data in a central space, without needing to coordinate their behavior. 
Brain technology, or self learning know how systems, defines a technology that employs latest findings in neuroscience.  The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the Roboy project. Brain Technology can be employed in robots, know how management systems and any other application with self learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as  know how maps . The first demonstrations of BC in humans and animals took place in the 1960s when Grey Walter demonstrated use of non invasively recorded encephalogram  EEG  signals from a human subject to control a slide projector  Graimann et al., 2010 .  Soon after Jacques J. Vidal coined the term brain computer interface  BCI  in 1971, the Defense Advanced Research Projects Agency  DARPA  first starting funding brain computer interface research and has since funded several brain computer interface projects. That market is expected to reach a value of  1.72 billion by 2022. Brain computer interfaces record brain activity, transmit the information out of the body, signal process the data via algorithms, and convert them into command control signals.  In 2012, a landmark study in Nature, led by pioneer Leigh Hochberg, MD, PhD, demonstrated that two people with tetraplegia were able to control robotic arms through thought when connected to the BrainGate neural interface system.  The two participants were able to reach for and grasp objects in three dimensional space, and one participant used the system to serve herself coffee for the first time since becoming paralyzed nearly 15 years prior. And in October 2020, two patients were able to wirelessly control an operating system to text, email, shop and bank using direct thought through the Stentrode brain computer interface  Journal of NeuroInterventional Surgery  in a study led by Thomas Oxley. This was the first time a brain computer interface was implanted via the patient s blood vessels, eliminating the need for open brain surgery. Currently a number of groups are exploring a range of experimental devices using brain computer interfaces, which have the potential to fundamentally change the way of life for patients with paralysis and a wide range of neurological disorders. These include  as Elon Musk, Facebook, and the University of California in San Francisco. The systems This technology is also being explored as a neuromodulation device and may ultimately help diagnose and treat a range of brain pathologies, such as epilepsy and Parkinson s disease 
Business process automation  BPA , also known as business automation, is the technology enabled automation of complex business processes. It can streamline a business for simplicity, achieve digital transformation, increase service quality, improve service delivery, or contain costs. BPA consists of integrating applications, restructuring labor resources, and using software applications throughout the organization. Robotic process automation is an emerging field within BPA. Toolsets vary in sophistication, but there is an increasing trend towards the use of artificial intelligence technologies that can understand natural language and unstructured data sets, interact with human beings, and adapt to new types of problems without human guided training. In order to automate the processes, connectors are needed to fit these systems solutions together with a data exchange layer to transfer the information. A process driven messaging service is an option for optimizing data exchange layer. By mapping the end to end process workflow, an integration between individual platforms using a process driven messaging platform can be built. A business process management system is different from BPA. However, it is possible to build automation on the back of a BPM implementation. The actual tools to achieve this vary, from writing custom application code to using specialist BPA tools. The advantages and disadvantages of this approach are inextricably linked   the BPM implementation provides an architecture for all processes in the business to be mapped, but this in itself delays the automation of individual processes and so benefits may be lost in the meantime. The practice of performing robotic process automation  RPA  results in the deployment of attended or unattended software agents to an organization s environment. These software agents, or robots, are deployed to perform pre defined structured and repetitive sets of business tasks or processes  The goal is for humans to focus on more productive tasks, while the software agents handle the repetitive ones. BPA providers tend to focus on different industry sectors but the underlying approach tends to be similar in that BPA providers will attempt to provide the shortest route to automation by interacting with the user interface rather than going into the application code or database behind it. BPA providers also simplify their own interface to the extent that these tools can be used directly by non technically qualified staff. The main advantage of these toolsets is therefore their speed of deployment. Artificial intelligence software robots are deployed to handle unstructured data sets  like images, texts, audios  and are deployed after performing and deploying robotic process automation  They can, for instance, populate an automatic transcript from a video. The combination of automation and artificial intelligence  AI  brings autonomy for the robots, along with the capability in mastering cognitive tasks  At this stage, the robot is able to learn and improve the processes by analyzing and adapting them. 
CarynAI is an artificial intelligence chatbot launched by SnapChat influencer Caryn Marjorie and developed by Forever Voices.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.





Examples of artificial intelligence algorithms applied to real world problems. For technologies used in these applications see  




Academic conferences related to artificial intelligence, machine learning and pattern recognition. 
This is a category for research labs and institutes in the area of artificial intelligence. 
Publications about the topic of artificial intelligence. 
Researchers who study Artificial intelligence 




This category is about cognitive architectures, models of human cognition. 
Computer vision is an interdisciplinary field related to, e.g., artificial intelligence, machine learning, robotics, signal processing and geometry. The purpose of computer vision is to program a computer to  understand  a scene or features in an image. Computer vision shares many topics and methods with image processing and machine vision. Listed here are subjects relating to the field of computer vision, image processing and machine vision. Notice  this category is intended for technical articles related to computer vision, not for showcasing research laboratories, Wikipedia users, or companies that are active in the area. 




Fuzzy logic is a form of many valued logic related to fuzzy sets. 



Significant articles  
Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts. 
The mind body problem is the central issue in the philosophy of mind. 
 This category is about multi agent systems, systems composed of several agents, which can be simulated entities or real software agents or robots.


Open source artificial intelligence 


Academic conferences related to signal processing, machine learning and pattern recognition. 


Character computing is a trans disciplinary field of research at the intersection of computer science and psychology. It is any computing that incorporates the human character within its context. Character is defined as all features or characteristics defining an individual and guiding their behavior in a specific situation. It consists of stable trait markers  e.g., personality, background, history, socio economic embeddings, culture,...  and variable state markers  emotions, health, cognitive state, ... . Character computing aims at providing a holistic psychologically driven model of human behavior. It models and predicts behavior based on the relationships between a situation and character. Three main research modules fall under the umbrella of character computing  character sensing and profiling, character aware adaptive systems, and artificial characters. Character computing can be viewed as an extension of the well established field of affective computing. Based on the foundations of the different psychology branches, it advocates defining behavior as a compound attribute that is not driven by either personality, emotions, situation or cognition alone. It rather defines behavior as a function of everything that makes up an individual i.e., their character and the situation they are in. Affective computing aims at allowing machines to understand and translate the non verbal cues of individuals into affect. Accordingly, character computing aims at understanding the character attributes of an individual and the situation to translate it to predicted behavior, and vice versa.   In practical terms, depending on the application context, character computing is a branch of research that deals with the design of systems and interfaces that can observe, sense, predict, adapt to, affect, understand, or simulate the following  character based on behavior and situation, behavior based on character and situation, or situation based on character and behavior.   The Character Behavior Situation  CBS  triad is at the core of character computing and defines each of the three edges based on the other two. Character computing relies on simultaneous development from a computational and psychological perspective and is intended to be used by researchers in both fields. Its main concept is aligning the computational model of character computing with empirical results from in lab and in the wild psychology experiments. The model is to be continuously built and validated through the emergence of new data. Similar to affective and personality computing, the model is to be used as a base for different applications towards improving user experience. Character computing as such was first coined in its first workshop in 2017. Since then it has had 3 international workshops and numerous publications. Despite its young age, it has already drawn some interest in the research community, leading to the publication of the first book under the same title in early 2020 published by Springer Nature. Research that can be categorized under the field dates much older than 2017. The notion of combining several factors towards the explanation of behavior or traits and states has long been investigated in both Psychology and Computer Science, for example. The word character originates from the Greek word meaning  stamping tool , referring to distinctive features and traits. Over the years it has been given many different connotations, like the moral character in philosophy, the temperament in psychology, a person in literature or an avatar in various virtual worlds, including video games. According to character computing character is a unification of all the previous definitions, by referring back to the original meaning of the word. Character is defined as the holistic concept representing all interacting trait and state markers that distinguish an individual. Traits are characteristics that mainly remain stable over time. Traits include personality, affect, socio demographics, and general health. States are characteristics that vary in short periods of time. They include emotions, well being, health, cognitive state. Each characteristic has many representation methods and psychological models. The different models can be combined together or one model can be preset for each characteristic. This depends on the use case and the design choices. Research into character computing can be divided into three areas, which complement each other but can each be investigated separately. The first area is sensing and predicting character states and traits or ensuing behavior. The second area is adapting applications to certain character states or traits and the behavior they predict. It also deals with trying to change or monitor such behavior. The final area deals with creating artificial agents e.g., chatbots or virtual reality avatars that exhibit certain characteristics. The three areas are investigated separately and build on existing findings in the literature. The results of each of the three areas can also be used as a stepping stone for the next area. Each of the three areas has already been investigated on its own in different research fields with focus on different subsets of character. For example, affective computing and personality computing both cover different areas with a focus on some character components without the others to account for human behavior. Character computing is based on a holistic psychologically driven model of human behavior. Human behavior is modeled and predicted based on the relationships between a situation and a human s character. To further define character in a more formal or holistic manner, we represent it in light of the Character Behavior Situation triad. This highlights that character not only determines who we are but how we are, i.e., how we behave. The triad investigated in Personality Psychology is extended through character computing to the Character Behavior Situation triad. Any member of the CBS triad is a function of the two other members, e.g., given the situation and personality, the behavior can be predicted. Each of the components in the triad can be further decomposed into smaller units and features that may best represent the human s behavior or character in a particular situation. Character is thus behind a person s behavior in any given situation. While this is a causality relation, the correlation between the three components is often more easily utilized to predict the components that are most difficult to measure from those measured more easily. There are infinitely many components to include in the representation of any of C, B, and S. The challenge is always to choose the smallest subset needed for prediction of a person s behavior in a particular situation. 
 Cognitive computing refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision  object recognition , human computer interaction, dialog and narrative generation, among other technologies. At present, there is no widely agreed upon definition for cognitive computing in either academia or industry. In general, the term cognitive computing has been used to refer to new hardware and or software that mimics the functioning of the human brain  2004  and helps to improve human decision making. In this sense, cognitive computing is a new type of computing with the goal of more accurate models of how the human brain mind senses, reasons, and responds to stimulus. Cognitive computing applications link data analysis and adaptive page displays  AUI  to adjust content for a particular type of audience. As such, cognitive computing hardware and applications strive to be more affective and more influential by design. The term  cognitive system  also applies to any artificial construct able to perform a cognitive process where a cognitive process is the transformation of data, information, knowledge, or wisdom to a new level in the DIKW Pyramid. While many cognitive systems employ techniques having their origination in artificial intelligence research, cognitive systems, themselves, may not be artificially intelligent. For example, a  neural network trained to recognize cancer on an MRI scan may achieve a higher success rate than a human doctor. This system is certainly a cognitive system but is not artificially intelligent. Cognitive systems may be engineered to feed on dynamic data in real time, or near real time, and may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs  visual, gestural, auditory, or sensor provided . Cognitive computing branded technology platforms typically specialize in the processing and analysis of large, unstructured datasets. Cognitive computing in conjunction with big data and algorithms that comprehend customer needs, can be a major advantage in economic decision making. The powers of cognitive computing and artificial intelligence hold the potential to affect almost every task that humans are capable of performing. This can negatively affect employment for humans, as there would be no such need for human labor anymore. It would also increase the inequality of wealth  the people at the head of the cognitive computing industry would grow significantly richer, while workers without ongoing, reliable employment would become less well off. The more industries start to utilize cognitive computing, the more difficult it will be for humans to compete. Increased use of the technology will also increase the amount of work that AI driven robots and machines can perform. Only extraordinarily talented, capable and motivated humans would be able to keep up with the machines. The influence of competitive individuals in conjunction with artificial intelligence cognitive computing with has the potential to change the course of humankind. 
Cognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence.  The point is not the text, but the mind that made it . Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other. Cognitive philology     Among the founding thinkers and noteworthy scholars devoted to such investigations are   This philology related article is a stub. You can help Wikipedia by expanding it.
In artificial intelligence research,  commonsense knowledge consists of facts about the everyday world, such as  Lemons are sour , or  Cows say moo , that all humans are expected to know.  It is currently an unsolved problem in Artificial General Intelligence. The first AI program to address common sense knowledge was  Advice Taker in 1959 by John McCarthy. Commonsense knowledge  can underpin a commonsense reasoning process, to attempt inferences such as  You might bake a cake because you want people to eat the cake.   A natural language processing process can be attached to the commonsense knowledge base to allow the knowledge base to attempt to answer questions about the world. Common sense knowledge also helps to solve problems in the face of incomplete information.  Using widely held beliefs about everyday objects, or common sense knowledge, AI systems make common sense assumptions or default assumptions about the unknown similar to the way people do.  In an AI system or in English, this is expressed  as  Normally P holds ,  Usually P  or  Typically P so Assume P .   For example, if we know the fact  Tweety is a bird , because we know the commonly held belief about birds,  typically birds fly,  without knowing anything else about Tweety, we may reasonably assume the fact that  Tweety can fly.   As more knowledge of the world is discovered or learned over time, the AI system can revise its assumptions about Tweety using a truth maintenance process.  If we later learn that  Tweety is a penguin  then truth maintenance revises this assumption because we also know  penguins do not fly . Commonsense reasoning simulates the human ability to use commonsense knowledge to make presumptions about the type and essence of ordinary situations they encounter every day, and to change their  minds  should new information come to light.  This includes time, missing or incomplete information and cause and effect.  The ability to explain cause and effect is an important aspect of explainable AI.  Truth maintenance algorithms automatically provide an explanation facility because they create elaborate records of presumptions.  Compared with humans, all existing computer programs that attempt human level AI perform extremely poorly on modern  commonsense reasoning  benchmark tests such as the Winograd Schema Challenge. The problem of attaining human level competency at  commonsense knowledge  tasks is considered to probably be  AI complete   that is, solving it would require the ability to synthesize a fully human level intelligence , although some oppose this notion and believe compassionate intelligence is also required for human level AI. Common sense reasoning has been applied successfully in more limited domains such as natural language processing and automated diagnosis or analysis. Compiling comprehensive knowledge bases of commonsense assertions  CSKBs  is a long standing challenge in AI research. From early expert driven efforts like CYC and WordNet, significant advances were achieved via the crowdsourced OpenMind Commonsense project, which lead to the crowdsourced ConceptNet KB. Several approaches have attempted to automate CSKB construction, most notably, via text mining  WebChild, Quasimodo, TransOMCS, Ascent , as well as harvesting these directly from pre trained language models  AutoTOMIC . These resources are significantly larger than ConceptNet, though the automated construction mostly makes them of moderately lower quality. Challenges also remain on the representation of commonsense knowledge  Most CSKB projects follow a triple data model, which is not necessarily best suited for breaking more complex natural language assertions. A notable exception here is GenericsKB, which applies no further normalization to sentences, but retains them in full. Around 2013, MIT researchers developed BullySpace, an extension of the commonsense knowledgebase ConceptNet, to catch taunting social media comments. BullySpace included over 200 semantic assertions based around stereotypes, to help the system infer that comments like  Put on a wig and lipstick and be who you really are  are more likely to be an insult if directed at a boy than a girl. ConceptNet has also been used by chatbots and by computers that compose original fiction.  At Lawrence Livermore National Laboratory, common sense knowledge was used in an intelligent software agent to detect violations of a comprehensive nuclear test ban treaty. As an example, as of 2012 ConceptNet includes these 21 language independent relations  
There are a number of competitions and prizes to promote research in artificial intelligence. The David E. Rumelhart prize is an annual award for making a  significant contemporary contribution to the theoretical foundations of human cognition .  The prize is  100,000. The Human Competitive Award is an annual challenge started in 2004 to reward results  competitive with the work of creative and inventive humans . The prize is  10,000.  Entries are required to use evolutionary computing. The IJCAI Award for Research Excellence is a biannual award given at the IJCAI conference to researcher in artificial intelligence as a recognition of excellence of their career. The 2011 Federal Virtual World Challenge, advertised by The White House and sponsored by the U.S. Army Research Laboratory s Simulation and Training Technology Center, held a competition offering a total of US 52,000 in cash prize awards for general artificial intelligence applications, including  adaptive learning systems, intelligent conversational bots, adaptive behavior  objects or processes   and more. The Machine Intelligence Prize is awarded annually by the British Computer Society for progress towards machine intelligence. The Kaggle    the world s largest community of data scientists compete to solve most valuable problems . The Loebner prize is an annual competition to determine the best Turing test competitors. The winner is the computer system that, in the judges  opinions, demonstrates the  most human  conversational behaviour, they have an additional prize for a system that in their opinion passes a Turing test. This second prize has not yet been awarded. The International Aerial Robotics Competition is a long running event begun in 1991 to advance the state of the art in fully autonomous air vehicles.  This competition is restricted to university teams  although industry and governmental sponsorship of teams is allowed .  Key to this event is the creation of flying robots which must complete complex missions without any human intervention.  Successful entries are able to interpret their environment and make real time decisions based only on a high level mission directive  e.g.,  find a particular target inside a building having certain characteristics which is among a group of buildings 3 kilometers from the aerial robot launch point  .  In 2000, a  30,000 prize was awarded during the 3rd Mission  search and rescue , and in 2008,  80,000 in prize money was awarded at the conclusion of the 4th Mission  urban reconnaissance . The DARPA Grand Challenge is a series of competitions to promote driverless car technology, aimed at a congressional mandate stating that by 2015 one third of the operational ground combat vehicles of the US Armed Forces should be unmanned. While the first race had no winner, the second awarded a  2 million prize for the autonomous navigation of a hundred mile trail, using GPS, computers and a sophisticated array of sensors. In November 2007, DARPA introduced the DARPA Urban Challenge, a sixty mile urban area race requiring vehicles to navigate through traffic. In November 2010 the US Armed Forces extended the competition with the  1.6 million prize Multi Autonomous Ground robotic International Challenge to consider cooperation between multiple vehicles in a simulated combat situation. Roborace will be a global motorsport championship with autonomously driving, electrically powered vehicles. The series will be run as a support series during the Formula E championship for electric vehicles. This will be the first global championship for driverless cars. The Netflix Prize was a competition for the best collaborative filtering algorithm that predicts user ratings for films, based on previous ratings.  The competition was held by Netflix, an online DVD rental service.  The prize was  1,000,000. The Pittsburgh Brain Activity Interpretation Competition will reward analysis of fMRI data  to predict what individuals perceive and how they act and feel in a novel Virtual Reality world involving searching for and collecting objects, interpreting changing instructions, and avoiding a threatening dog.   The prize in 2007 was  22,000. The Face Recognition Grand Challenge  May 2004 to March 2006  aimed to promote and advance face recognition technology. The American Meteorological Society s artificial intelligence competition involves learning a classifier to characterise precipitation based on meteorological analyses of environmental conditions and polarimetric radar data. The RoboCup and FIRA are annual international robot soccer competitions. The International RoboCup Federation challenge is by 2050  a team of fully autonomous humanoid robot soccer players shall win the soccer game, comply with the official rule of the FIFA, against the winner of the most recent World Cup.  The Herbrand Award is a prize given by CADE Inc. to honour persons or groups for important contributions to the field of automated deduction.  The prize is  1000. The CADE ATP System Competition  CASC  is a yearly competition of fully automated theorem provers for classical first order logic associated with the CADE and IJCAR conferences. The competition was part of the Alan Turing Centenary Conference in 2012, with total prizes of 9000 GBP given by Google. The SUMO prize is an annual prize for the best open source ontology extension of the Suggested Upper Merged Ontology  SUMO , a formal theory of terms and logical definitions describing the world.  The prize is  3000. The Hutter Prize for Lossless Compression of Human Knowledge is a cash prize which rewards compression improvements on a specific 100 MB English text file. The prize awards 500 euros for each one percent improvement, up to  50,000. The organizers believe that text compression and AI are equivalent problems and 3 prizes were already given, at around   2k. The Cyc TPTP Challenge is a competition to develop reasoning methods for the Cyc comprehensive ontology and database of everyday common sense knowledge.  The prize is 100 euros for  each winner of two related challenges . The Eternity II challenge was a constraint satisfaction problem very similar to the Tetravex game.  The objective is to lay 256 tiles on a 16x16 grid while satisfying a number of constraints.  The problem is known to be NP complete. The prize was US 2,000,000. The competition ended in December 2010. The World Computer Chess Championship has been held since 1970.  The International Computer Games Association continues to hold an annual Computer Olympiad which includes this event plus computer competitions for many other games. The Ing Prize was a substantial money prize attached to the World Computer Go Congress, starting from 1985 and expiring in 2000.  It was a graduated set of handicap challenges against young professional players with increasing prizes as the handicap was lowered.  At the time it expired in 2000, the unclaimed prize was 400,000 NT dollars for winning a 9 stone handicap match. The AAAI General Game Playing Competition is a competition to develop programs that are effective at general game playing. Given a definition of a game, the program must play it effectively without human intervention.  Since the game is not known in advance the competitors cannot especially adapt their programs to a particular scenario.  The prize in 2006 and 2007 was  10,000. The General Video Game AI Competition  GVGAI  poses the problem of creating artificial intelligence that can play a wide, and in principle unlimited, range of games. Concretely, it tackles the problem of devising an algorithm that is able to play any game it is given, even if the game is not known a priori. Additionally, the contests poses the challenge of creating level and rule generators for any game is given. This area of study can be seen as an approximation of General Artificial Intelligence, with very little room for game dependent heuristics. The competition runs yearly in different tracks  single player planning, two player planning, single player learning, level and rule generation, and each track prizes ranging from 200 to 500 US dollars for winners and runner ups. The 2007 Ultimate Computer Chess Challenge was a competition organised by World Chess Federation that pitted Deep Fritz against Deep Junior.  The prize was  100,000. The annual Arimaa Challenge offered a  10,000 prize until the year 2020 to develop a program that plays the board game Arimaa and defeats a group of selected human opponents. In 2015, David Wu s bot bot sharp beat the humans, losing only 2 games out of 9. As a result, the Arimaa Challenge was declared over and David Wu received the prize of  12,000   2,000 being offered by third parties for 2015 s championship . 2K Australia is offering a prize worth A 10,000 to develop a game playing bot that plays a first person shooter.  The aim is to convince a panel of judges that it is actually a human player. The competition started in 2008 and was won in 2012. A new competition is planned for 2014. The Google AI Challenge was a bi annual online contest organized by the University of Waterloo Computer Science Club and sponsored by Google that ran from 2009 to 2011. Each year a game was chosen and contestants submitted specialized automated bots to play against other competing bots. Cloudball had its first round in Spring 2012 and finished on June 15. It is an international artificial intelligence programming contest, where users continuously submit the actions their soccer teams will take in each time step, in simple high level C  code. 
 Computational heuristic intelligence  CHI  refers to specialized programming techniques in computational intelligence  also called artificial intelligence, or AI . These techniques have the express goal of avoiding complexity issues, also called NP hard problems, by using human like techniques. They are best summarized as the use of exemplar based methods  heuristics , rather than rule based methods  algorithms . Hence the term is distinct from the more conventional computational algorithmic intelligence, or GOFAI. An example of a CHI technique is the encoding specificity principle of Tulving and Thompson. In general, CHI principles are problem solving techniques used by people, rather than programmed into machines. It is by drawing attention to this key distinction that the use of this term is justified in a field already replete with confusing neologisms. Note that the legal systems of all modern human societies employ both heuristics  generalisations of cases  from individual trial records as well as legislated statutes  rules  as regulatory guides. Another recent approach to the avoidance of complexity issues is to employ feedback control rather than feedforward modeling as a problem solving paradigm. This approach has been called computational cybernetics, because  a  the term  computational  is associated with conventional computer programming techniques which represent a strategic, compiled, or feedforward model of the problem, and  b  the term  cybernetic  is associated with conventional system operation techniques which represent a tactical, interpreted, or feedback model of the problem. Of course, real programs and real problems both contain both feedforward and feedback components. A real example which illustrates this point is that of human cognition, which clearly involves both perceptual  bottom up, feedback, sensor oriented  and conceptual  top down, feedforward, motor oriented  information flows and hierarchies. The AI engineer must choose between mathematical and cybernetic problem solution and machine design paradigms. This is not a coding  program language  issue, but relates to understanding the relationship between the declarative and procedural programming paradigms.  The vast majority of STEM professionals never get the opportunity to design or implement pure cybernetic solutions. When pushed, most responders will dismiss the importance of any difference by saying that all code can be reduced to a mathematical model anyway. Unfortunately, not only is this belief false, it fails most spectacularly in many AI scenarios. Mathematical models are not time agnostic, but by their very nature are pre computed, ie feedforward. Dyer  and Feldman  have independently investigated the simplest of all somatic governance paradigms, namely control of a simple jointed limb by a single flexor muscle. They found that it is impossible to determine forces from limb positions  therefore, the problem cannot have a pre computed  feedforward  mathematical solution. Instead, a top down command bias signal changes the threshold feedback level in the sensorimotor loop, e.g. the loop formed by the afferent and efferent nerves, thus changing the so called  equilibrium point  of the flexor muscle  elbow joint system. An overview of the arrangement reveals that global postures and limb position are commanded in feedforward terms, using global displacements  common coding , with the forces needed being computed locally by feedback loops. This method of sensorimotor unit governance, which is based upon what Anatol Feldman calls the  equilibrium Point  theory, is formally equivalent to a servomechanism such as a car s  cruise control . 
Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. Computational human modeling emphasizes descriptions of human for A.I. research and applications. Research in computational human modeling can include computer vision studies on identify  face recognition , attributes  gender, age, skin color , expressions, geometry  3D face modeling, 3D body modeling , and activity  pose, gaze, actions, and social interactions . 
Computational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996. An approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification. Simple prototypes for computer pun generation were reported in the early 1990s, based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question answer type puns from a general, i.e., non humorous, lexicon.   The program name is an acronym for  Joke Analysis and Production Engine .  Some examples produced by JAPE are  Since then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy.  The project name is an acronym for  System To Augment Non speakers  Dialog Using Puns  and an allusion to standup comedy.  Children responded to this  language playground  with enthusiasm, and showed marked improvement on certain types of language tests. The two young people, who used the system over a ten week period, regaled their peers, staff, family and neighbors with jokes such as   What do you call a spicy missile? A hot shot!  Their joy and enthusiasm at entertaining others was inspirational. Stock and Strapparava described a program to generate funny acronyms.  AskTheBrain   2002   used clustering and bayesian analysis to associate concepts in a comical way. A statistical machine learning algorithm to detect whether a sentence contained a  That s what she said  double entendre was developed by Kiddon and Brun  2011 .  There is an open source Python implementation of Kiddon   Brun s TWSS system. A program to recognize knock knock jokes was reported by Taylor and Mazlack. This kind of research is important in analysis of human computer interaction. An application of machine learning techniques for the distinguishing of joke texts from non jokes was described by Mihalcea and Strapparava  2006 . Takizawa et al.  1996  reported on a heuristic program for detecting puns in the Japanese language. A possible application for the assistance in language acquisition is described in the section  Pun generation . Another envisioned use of joke generators is in cases of steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation. It is known that humans interact with computers in ways similar to interacting with other humans that may be described in terms of personality, politeness, flattery, and in group favoritism. Therefore, the role of humor in human computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested. Craig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences.  Basing on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into  Arafat joined Quayle s Ant, while TARAR Jeopardized thurmond s vase . John Allen Paulos is known for his interest in mathematical foundations of humor. His book Mathematics and Humor  A Study of the Logic of Humor demonstrates structures common to humor and formal sciences  mathematics, linguistics  and develops a mathematical model of jokes based on catastrophe theory. Conversational systems which have been designed to take part in Turing test competitions generally have the ability to learn humorous anecdotes and jokes. Because many people regard humor as something particular to humans, its appearance in conversation can be quite useful in convincing a human interrogator that a hidden entity, which could be a machine or a human, is in fact a human. 
 The expression computational intelligence  CI  usually refers to the ability of a computer to learn a specific task from data or experimental observation. Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence. Generally, computational intelligence is a set of nature inspired computational methodologies and approaches to address complex real world problems to which mathematical or traditional modelling can be useless for a few reasons  the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. Indeed, many real life problems cannot be translated into binary language  unique values of 0 and 1  for computers to process it. Computational Intelligence therefore provides solutions for such problems. The methods used are close to the human s way of reasoning, i.e. it uses inexact and incomplete knowledge, and it is able to produce control actions in an adaptive way. CI therefore uses a combination of five main complementary techniques. The fuzzy logic which enables the computer to understand natural language, artificial neural networks which permits the system to learn experiential data by operating like the biological one, evolutionary computing, which is based on the process of natural selection, learning theory, and probabilistic methods which helps dealing with uncertainty imprecision. Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. But although both Computational Intelligence  CI  and Artificial Intelligence  AI  seek similar goals, there s a clear distinction between them. Computational Intelligence is thus a way of performing like human beings. Indeed, the characteristic of  intelligence  is usually attributed to humans. More recently, many products and items also claim to be  intelligent , an attribute which is directly linked to the reasoning and decision making. Source  The notion of Computational Intelligence was first used by the IEEE Neural Networks Council in 1990. This Council was founded in the 1980s by a group of researchers interested in the development of biological and artificial neural networks. On November 21, 2001, the IEEE Neural Networks Council became the IEEE Neural Networks Society, to become the IEEE Computational Intelligence Society two years later by including new areas of interest such as fuzzy systems and evolutionary computation, which they related to Computational Intelligence in 2011  Dote and Ovaska . But the first clear definition of Computational Intelligence was introduced by Bezdek in 1994  a system is called computationally intelligent if it deals with low level data such as numerical data, has a pattern recognition component and does not use knowledge in the AI sense, and additionally when it begins to exhibit computational adaptively, fault tolerance, speed approaching human like turnaround and error rates that approximate human performance. Bezdek and Marks  1993  clearly differentiated CI from AI, by arguing that the first one is based on soft computing methods, whereas AI is based on hard computing ones. Although Artificial Intelligence and Computational Intelligence seek a similar long term goal  reach general intelligence, which is the intelligence of a machine that could perform any intellectual task that a human being can  there s a clear difference between them. According to Bezdek  1994 , Computational Intelligence is a subset of Artificial Intelligence. There are two types of machine intelligence  the artificial one based on hard computing techniques and the computational one based on soft computing methods, which enable adaptation to many situations. Hard computing techniques work following binary logic based on only two values  the Booleans true or false, 0 or 1  on which modern computers are based. One problem with this logic is that our natural language cannot always be translated easily into absolute terms of 0 and 1. Soft computing techniques, based on fuzzy logic can be useful here. Much closer to the way the human brain works by aggregating data to partial truths  Crisp fuzzy systems , this logic is one of the main exclusive aspects of CI. Within the same principles of fuzzy and binary logics follow crispy and fuzzy systems. Crisp logic is a part of artificial intelligence principles and consists of either including an element in a set, or not, whereas fuzzy systems  CI  enable elements to be partially in a set. Following this logic, each element can be given a degree of membership  from 0 to 1  and not exclusively one of these 2 values. The main applications of Computational Intelligence include computer science, engineering, data analysis and bio medicine. As explained before, fuzzy logic, one of CI s main principles, consists in measurements and process modelling made for real life s complex processes. It can face incompleteness, and most importantly ignorance of data in a process model, contrarily to Artificial Intelligence, which requires exact knowledge. This technique tends to apply to a wide range of domains such as control, image processing and decision making. But it is also well introduced in the field of household appliances with washing machines, microwave ovens, etc. We can face it too when using a video camera, where it helps stabilizing the image while holding the camera unsteadily. Other areas such as medical diagnostics, foreign exchange trading and business strategy selection are apart from this principle s numbers of applications. Fuzzy logic is mainly useful for approximate reasoning, and doesn t have learning abilities, a qualification much needed that human beings have. It enables them to improve themselves by learning from their previous mistakes. This is why CI experts work on the development of artificial neural networks based on the biological ones, which can be defined by 3 main components  the cell body which processes the information, the axon, which is a device enabling the signal conducting, and the synapse, which controls signals. Therefore, artificial neural networks are doted of distributed information processing systems, enabling the process and the learning from experiential data. Working like human beings, fault tolerance is also one of the main assets of this principle. Concerning its applications, neural networks can be classified into five groups  data analysis and classification, associative memory, clustering generation of patterns and control. Generally, this method aims to analyze and classify medical data, proceed to face and fraud detection, and most importantly deal with nonlinearities of a system in order to control it. Furthermore, neural networks techniques share with the fuzzy logic ones the advantage of enabling data clustering. Based on the process of natural selection firstly introduced by Charles Robert Darwin, the evolutionary computation consists in capitalizing on the strength of natural evolution to bring up new artificial evolutionary methodologies. It also includes other areas such as evolution strategy, and evolutionary algorithms which are seen as problem solvers... This principle s main applications cover areas such as optimization and multi objective optimization, to which traditional mathematical one techniques aren t enough anymore to apply to a wide range of problems such as DNA Analysis, scheduling problems... Still looking for a way of  reasoning  close to the humans  one, learning theory is one of the main approaches of CI. In psychology, learning is the process of bringing together cognitive, emotional and environmental effects and experiences to acquire, enhance or change knowledge, skills, values and world views  Ormrod, 1995  Illeris, 2004 . Learning theories then helps understanding how these effects and experiences are processed, and then helps making predictions based on previous experience. Being one of the main elements of fuzzy logic, probabilistic methods firstly introduced by Paul Erdos and Joel Spencer 1974 , aim to evaluate the outcomes of a Computation Intelligent system, mostly defined by randomness. Therefore, probabilistic methods bring out the possible solutions to a problem, based on prior knowledge. According to bibliometrics studies, computational intelligence plays a key role in research. All the major academic publishers are accepting manuscripts in which a combination of Fuzzy logic, neural networks and evolutionary computation is discussed. On the other hand, Computational intelligence isn t available in the university curriculum. The amount of technical universities in which students can attend a course is limited. Only British columbia, Technical University of Dortmund  involved in the european fuzzy boom  and Georgia Southern University are offering courses from this domain. The reason why major university are ignoring the topic is because they don t have the resources. The existing computer science courses are so complex, that at the end of the semester there is no room for fuzzy logic. Sometimes it is taught as a subproject in existing introduction courses, but in most cases the universities are preferring courses about classical AI concepts based on boolean logic, turing machines and toy problems like blocks world. Since a while with the upraising of STEM education, the situation has changed a bit. There are some efforts available in which multidisciplinary approaches are preferred which allows the student to understand complex adaptive systems. These objectives are discussed only on a theoretical basis. The curriculum of real universities wasn t adapted yet. 
The Computer Science Ontology  CSO  is an automatically generated taxonomy of research topics in the field of Computer Science. It was produced by the Open University in collaboration with Springer Nature by running an information extraction system over a large corpus of scientific articles. Several branches were manually improved by domain experts. The current version  CSO 3.2  includes about 14K research topics and 160K semantic relationships. CSO is available in OWL, Turtle, and N Triples. It is aligned with several other knowledge graphs, including DBpedia, Wikidata, YAGO, Freebase, and Cyc. New versions of CSO are regularly released on the CSO Portal. CSO is mostly used to characterise scientific papers and other documents according to their research areas, in order to enable different kinds of analytics. The CSO Classifier is an open source python tool for automatically annotating documents with CSO. 
Computer audition  CA  or machine listening is the general field of study of algorithms and systems for audio interpretation by machines. Since the notion of what it means for a machine to  hear  is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in Technology Review, talks about these systems    software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.  Inspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation. Like computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings. Applications of computer audition are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on. Computer Audition overlaps with the following disciplines  Since audio signals are interpreted by the human ear brain system, that complex perceptual mechanism should be simulated somehow in software for  machine listening . In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields  electrical engineering  spectrum analysis, filtering, and audio transforms   artificial intelligence  machine learning and sound classification   psychoacoustics  sound perception   cognitive sciences  neuroscience and artificial intelligence   acoustics  physics of sound production   and music  harmony, rhythm, and timbre . Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high level feature extraction, and sound analysis synthesis. Finally, structuring and coding the content of an audio file  sound and metadata  could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human machine interfaces. The study of CA could be roughly divided into the following sub problems  Computer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files. Since audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models  such as source filter model , it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio visual recordings. Description of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity  bandwidth  in frequency or octave invariance  chroma . Since parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation. Finding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on. Comparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to  correct  for different temporal scales of acoustic events. Finding repetitions and similar sub sequences of sonic events is important for tasks such as texture synthesis and machine improvisation. Since one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection. Some methods, before explicit recognition, rely on revealing structures in data without knowing the structures  like recognizing objects in abstract pictures without attributing them meaningful labels  by finding the least complex data representations, for instance describing audio scenes as generated by a few tone patterns and their trajectories  polyphonic voices  and acoustical contours drawn by a tone  chords . Listening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics. Among the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents. 
Concurrent MetateM is a multi agent language in which each agent is programmed using a set of  augmented  temporal logic specifications of the behaviour it should exhibit. These specifications are executed directly to generate the behaviour of the agent. As a result, there is no risk of invalidating the logic as with systems where logical specification must first be translated to a lower level implementation. The root of the MetateM concept is Gabbay s separation theorem  any arbitrary temporal logic formula can be rewritten in a logically equivalent past   future form. Execution proceeds by a process of continually matching rules against a history, and firing those rules when antecedents are satisfied. Any instantiated future time consequents become commitments which must subsequently be satisfied, iteratively generating a model for the formula made up of the program rules. The Temporal Connectives of Concurrent MetateM can divided into two categories, as follows  The connectives  are unary  the remainder are binary.    is satisfied now if   was true in the previous time. If    is interpreted at the beginning of time, it is satisfied despite there being no actual previous time.  Hence  weak  last.    is satisfied now if   was true in the previous time.  If    is interpreted at the beginning of time, it is not satisfied because there is no actual previous time.  Hence  strong  last.    is satisfied now if   was true in any previous moment in time.    is satisfied now if   was true in every previous moment in time.  S  is satisfied now if   is true at any previous moment and   is true at every moment after that moment.  Z  is satisfied now if    is true at any previous moment and   is true at every moment after that moment  OR   has not happened in the past.    is satisfied now if   is true in the next moment in time.    is satisfied now if   is true now or in any future moment in time.    is satisfied now if   is true now and in every future moment in time.  U  is satisfied now if   is true at any future moment and   is true at every moment prior.  W  is satisfied now if    is true at any future moment and   is true at every moment prior  OR   does not happen in the future. 
Connectionist expert systems are artificial neural network  ANN  based expert systems where the ANN generates inferencing rules e.g., fuzzy multi layer perceptron where linguistic and natural form of inputs are used. Apart from that, rough set theory may be used for encoding knowledge in the weights better and also genetic algorithms may be used to optimize the search solutions better. Symbolic reasoning methods may also be incorporated  see hybrid intelligent system .  Also see  expert system,  neural network, clinical decision support system.  This robotics related article is a stub. You can help Wikipedia by expanding it.
DABUS  Device for the Autonomous Bootstrapping of Unified Sentience  is an artificial intelligence  AI  system created by Stephen Thaler. It reportedly conceived of two novel products   a food container constructed using fractal geometry, which enables rapid reheating, and a flashing beacon for attracting attention in an emergency. The filing of patent applications designating DABUS as inventor has led to decisions by patent offices and courts on whether a patent can be granted for an invention reportedly made by an AI system. DABUS itself is a patented AI paradigm capable of accommodating trillions of computational neurons within extensive artificial neural systems that emulate the limbo thalamo cortical loop within the mammalian brain. Such systems utilize arrays of trainable neural modules, each containing interrelated memories representative of some conceptual space. Through simple learning rules, these modules bind together to represent both complex ideas  e.g., juxtapositional inventions  and their consequences as chaining topologies. An electro optical attention window scans the entire array of neural modules in search of so called  hot buttons,  those neural modules containing impactful memories. Detection of such hot buttons within consequence chains triggers the release or retraction of synaptic disturbances into the system, selectively reinforcing the most salient chain based notions. On 17 September 2019, Thaler filed an application to patent a  Food container and devices and methods for attracting enhanced attention,  naming DABUS as the inventor. On 21 September 2020, IP Australia found that section 15 1  of the Patents Act 1990  Cth  is inconsistent with an artificial intelligence machine being treated as an inventor, and Thaler s application had lapsed. Thaler sought judicial review, and on 30 July 2021, the Federal Court set aside IP Australia s decision and ordered IP Australia to reconsider the application. On 13 April 2022, the Full Court of the Federal Court set aside that decision, holding that only a natural person can be an inventor for the purposes of the Patents Act 1990  Cth  and the Patents Regulations 1991  Cth , and that such an inventor must be identified for any person to be entitled to a grant of a patent. On 11 November 2022, Thaler was refused special leave to appeal to the High Court. On 17 October 2018 and 7 November 2018, Thaler filed two European patent applications with the European Patent Office. The first claimed invention was a  Food Container  and the second was  Devices and Methods for Attracting Enhanced Attention.  On 27 January 2020, the EPO rejected the applications on the grounds that the application listed an AI system named DABUS, and not a human, as the inventor, based on Article 81 and Rule 19 1  of the European Patent Convention. On 21 December 2021, the Board of Appeal of the EPO dismissed Thaler s appeal from the EPO s primary decision. The Board of Appeal confirmed that  under the EPC the designated inventor has to be a person with legal capacity. This is not merely an assumption on which the EPC was drafted. It is the ordinary meaning of the term inventor.   4.3.1  Similar applications were filed by Thaler to the United Kingdom Intellectual Property Office on 17 October and 7 November 2018. The Office asked Thaler to file statements of inventorship and of right of grant to a patent  Patent Form 7  in respect of each invention within 16 months of the filing date. Thaler filed those forms naming DABUS as the inventor and explaining in some detail why he believed that machines should be regarded as inventors in the circumstances. His application was rejected on the grounds that   1  naming a machine as inventor did not meet the requirements of the Patents Act 1977  and   2  the IPO was not satisfied as to the manner in which Thaler had acquired rights that would otherwise vest in the inventor. Thaler was not satisfied with the decision and asked for a hearing before an official known as the  hearing officer . By a decision dated 4 December 2019 the hearing officer rejected Thaler s appeal. Thaler appealed against the hearing officer s decision to the Patents Court  a specialist court within the Chancery Division of the High Court of England and Wales that determines patent disputes .  On 21 September 2020, Mr Justice Marcus Smith upheld the decision of the hearing officer. On 21 September 2021, Thaler s further appeal to the Court of Appeal was dismissed by Arnold LJ and Laing LJ  Birss LJ dissenting . The patent applications on the inventions were refused by the USPTO, which held that only natural persons can be named as inventors in a patent application. Thaler first fought this result by filing a complaint under Administrative Procedure Act  APA  alleging that the decision was  arbitrary, capricious, an abuse of discretion and not in accordance with the law  unsupported by substantial evidence, and in excess of Defendants  statutory authority.  A month later on August 19, 2019, Thaler filed a petition with the USPTO as allowed in 37 C.F.R.   1.181 stating that DABUS should be the inventor. The judge and Thaler agreed in this case that Thaler himself is unable to receive the patent on behalf of DABUS. On the 31st of January 2022, the Intellectual Property Office of New Zealand  IPONZ  decided that a patent application  776029  filed by Stephen Thaler was void, on the basis that no inventor was identified on the patent applications. IPONZ determined that DABUS could not be  an actual devisor of the invention  as required by the Patents Act 2013, and that this must be a natural person as held by the previous patent offices above. On the 24th of June 2021, the South African Companies and Intellectual Property Commission  CIPC  accepted Dr Thaler s Patent Cooperation Treaty, for a patent in respect of inventions generated by DABUS. In July 2021, the CIPC released a notice of issuance for the patent. It is the first patent granted for an AI invention. 
The first edition of the textbook Data Science and Predictive Analytics  Biomedical and Health Applications using R, authored by Ivo D. Dinov, was published in August 2018 by Springer. The second edition of the book was printed in 2023. This textbook covers some of the mathematical foundations, computational techniques, and artificial intelligence approaches used in data science research and applications. Using the statistical computing platform R and a broad range of biomedical case studies, the 23 chapters of the book first edition provide explicit examples of importing, exporting, processing, modeling, visualizing, and interpreting large, multivariate, incomplete, heterogeneous, longitudinal, and incomplete datasets  big data . The first edition of the Data Science and Predictive Analytics  DSPA  textbook  is divided into the following 23 chapters, each progressively building on the previous content. The significantly reorganized revised edition of the book  2023    expands and modernizes the presented mathematical principles, computational methods, data science techniques, model based machine learning and model free artificial intelligence algorithms. The 14 chapters of the new edition start with an introduction and progressively build foundational skills to naturally reach biomedical applications of deep learning. The materials in the Data Science and Predictive Analytics  DSPA  textbook have been peer reviewed in the International Statistical Institute s  ISI Review Journal and the Journal of the American Library Association. Many scholarly publications reference the DSPA textbook. As of January 17, 2021, the electronic version of the book first edition  ISBN 978 3 319 72347 1  is freely available on SpringerLink and has been downloaded over 6 million times. The textbook is globally available in print  hardcover and softcover  and electronic formats  PDF and EPub  in many college and university libraries and has been used for data science, computational statistics, and analytics classes at various institutions. 
A data pack  or fact pack  is a pre made database that can be fed to a software, such as software agents, game, Internet bots or chatterbots, to teach information and facts, which it can later look up. In other words, a data pack can be used to feed minor updates into a system. Common data packs may include abbreviations, acronyms, dictionaries, lexicons and technical data, such as country codes, RFCs, filename extensions, TCP and UDP port numbers, country calling codes, and so on. Data packs may come in formats of CSV and SQL that can easily be parsed or imported into a database management system. The database may consist of a key value pair, like an association list. Data packs are commonly used within the video game industry to provide minor updates within their games. When a user downloads an update for a game they will be downloading loads of data packs which will contain updates for the game such as minor bug fixes or additional content. An example of a data pack used to update a game can be found on the references. A data pack DataPack Definition is similar to a data packet it contains loads of information  data  and stores it within a pack where the data can be compressed to reduce its file size. Only certain programs can read a data pack therefore when the data is packed it is vital to know whether the receiving program is able to unpack the data. An example of data packs which are able to effective deliver information can be found on the reference page. When you refer to the word data pack it can come in many forms such as a mobile data pack. A mobile data pack refers to an add on which can enable you to boost the amount of data which you can use on your mobile phone. The rate at which you use your data can also be monitored, so you know how much data you have left. Mobile data is a service which provides a similar service to Wi Fi and allows you to connect to the Internet. So the purpose of a data pack is to increase the amount of data that your mobile has access to. An example of a mobile data pack can be found on the references. 
A data processing unit  DPU  is a channel controller, a programmable specialized electronic circuit with hardware acceleration of data processing for data centric computing. The data is transmitted to and from the component as multiplexed packets of information. A DPU generally contains a CPU, NIC and programmable data acceleration engines. This allows DPUs to have the generality and the programmability of central processing units while being specialized to operate efficiently on networking packets, storage requests or analytics requests. The data acceleration engines differentiates itself from a CPU by a larger degree of parallelism  required to process many requests  and from a GPU by a MIMD architecture rather than an SIMD architecture  required as each request needs to make different decisions and follow a different path through the chip . DPUs can be either ASIC based, FPGA based or SoC based. DPUs have been increasingly used in data centers and supercomputers since their introduction in the 2010s due to the rise in use of data centric computing, big data, security, and artificial intelligence machine learning deep learning. DPUs are designed to be independent infrastructure endpoints. DPU vendors   product lines include  Software Vendors utilizing DPUs include  This electronics related article is a stub. You can help Wikipedia by expanding it.
Data driven models are a class of computational models that primarily rely on historical data collected throughout a system s or process  lifetime to establish relationships between input, internal, and output variables. Commonly found in numerous articles and publications, data driven models have evolved from earlier statistical models, overcoming limitations posed by strict assumptions about probability distributions. These models have gained prominence across various fields, particularly in the era of big data, artificial intelligence, and machine learning, where they offer valuable insights and predictions based on the available data. These models have evolved from earlier statistical models, which were based on certain assumptions about probability distributions that often proved to be overly restrictive. The emergence of data driven models in the 1950s and 1960s coincided with the development of digital computers, advancements in artificial intelligence research, and the introduction of new approaches in non behavioural modelling, such as pattern recognition and automatic classification. Data driven models encompass a wide range of techniques and methodologies that aim to intelligently process and analyse large datasets. Examples include fuzzy logic, fuzzy and rough sets for handling uncertainty, neural networks for approximating functions, global optimization and evolutionary computing, statistical learning theory, and Bayesian methods. These models have found applications in various fields, including economics, customer relations management, financial services, medicine, and the military, among others. Machine learning, a subfield of artificial intelligence, is closely related to data driven modelling as it also focuses on using historical data to create models that can make predictions and identify patterns. In fact, many data driven models incorporate machine learning techniques, such as regression, classification, and clustering algorithms, to process and analyse data. In recent years, the concept of data driven models has gained considerable attention in the field of water resources, with numerous applications, academic courses, and scientific publications using the term as a generalization for models that rely on data rather than physics. This classification has been featured in various publications and has even spurred the development of hybrid models in the past decade. Hybrid models attempt to quantify the degree of physically based information used in hydrological models and determine whether the process of building the model is primarily driven by physics or purely data based. As a result, data driven models have become an essential topic of discussion and exploration within water resources management and research. The term  data driven modelling   DDM  refers to the overarching paradigm of using historical data in conjunction with advanced computational techniques, including machine learning and artificial intelligence, to create models that can reveal underlying trends, patterns, and, in some cases, make predictions Data driven models can be built with or without detailed knowledge of the underlying processes governing the system behavior, which makes them particularly useful when such knowledge is missing or fragmented. 
Description logics  DL  are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first order logic. In contrast to the latter, the core reasoning problems for DLs are  usually  decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors. DLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain  known as terminological knowledge . It is of particular importance in providing a logical formalism for ontologies and the Semantic Web  the Web Ontology Language  OWL  and its profiles are based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge. A description logic  DL  models concepts, roles and individuals, and their relationships. The fundamental modeling concept of a DL is the axiom a logical statement relating roles and or concepts. This is a key difference from the  frames paradigm where a frame specification declares and completely defines a class. The description logic community uses different terminology than the first order logic  FOL  community for operationally equivalent notions  some examples are given below. The Web Ontology Language  OWL  uses again a different terminology, also given in the table below. There are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. The expressivity is encoded in the label for a logic starting with one of the following basic logics  Followed by any of the following extensions  Some canonical DLs that do not exactly fit this convention are  As an example,       A L C          is a centrally important description logic from which comparisons with other varieties can be made.       A L C          is simply       A L          with complement of any concept allowed, not just atomic concepts.       A L C          is used instead of the equivalent       A L U E         . A further example, the description logic       S H I Q          is the logic       A L C          plus  extended cardinality restrictions, and transitive and inverse roles. The naming conventions aren t purely systematic so that the logic       A L C O I N          might be referred to as       A L C N I O          and other abbreviations are also made where possible. The Prot g  ontology editor supports        S H O I N       D              . Three major biomedical informatics terminology bases, SNOMED CT, GALEN, and GO, are expressible in       E L           with additional role properties . OWL 2 provides the expressiveness of         S R O I Q       D              , OWL DL is based on        S H O I N       D              , and for OWL Lite it is        S H I F       D              . Description logic was given its current name in the 1980s. Previous to this it was called  chronologically   terminological systems, and concept languages. Frames and semantic networks lack formal  logic based  semantics. DL was first introduced into knowledge representation  KR  systems to overcome this deficiency. The first DL based KR system was KL ONE  by Ronald J. Brachman and Schmolze, 1985 . During the  80s other DL based systems using structural subsumption algorithms were developed including KRYPTON  1983 , LOOM  1987 , BACK  1988 , K REP  1991  and CLASSIC  1991 . This approach featured DL with limited expressiveness but relatively efficient  polynomial time  reasoning. In the early  90s, the introduction of a new tableau based algorithm paradigm allowed efficient reasoning on more expressive DL. DL based systems using these algorithms   such as KRIS  1991    show acceptable reasoning performance on typical inference problems even though the worst case complexity is no longer polynomial. From the mid  90s, reasoners were created with good practical performance on very expressive DL with high worst case complexity. Examples from this period include FaCT, RACER  2001 , CEL  2005 , and KAON 2  2005 . DL reasoners, such as FaCT, FaCT  , RACER, DLP and Pellet, implement the method of analytic tableaux. KAON2 is implemented by algorithms which reduce a SHIQ D  knowledge base to a disjunctive datalog program. The DARPA Agent Markup Language  DAML  and Ontology Inference Layer  OIL  ontology languages for the Semantic Web can be viewed as syntactic variants of DL. In particular, the formal semantics and reasoning in OIL use the       S H I Q          DL. The DAML OIL DL was developed as a submission to and formed the starting point of the World Wide Web Consortium  W3C  Web Ontology Working Group. In 2004, the Web Ontology Working Group completed its work by issuing the OWL recommendation. The design of OWL is based on the       S H          family of DL with OWL DL and OWL Lite based on        S H O I N       D               and        S H I F       D               respectively. The W3C OWL Working Group began work in 2007 on a refinement of   and extension to   OWL. In 2009, this was completed by the issuance of the OWL2 recommendation. OWL2 is based on the description logic        S R O I Q       D              . Practical experience demonstrated that OWL DL lacked several key features necessary to model complex domains. In DL, a distinction is drawn between the so called TBox  terminological box  and the ABox  assertional box . In general, the TBox contains sentences describing concept hierarchies  i.e., relations between concepts  while the ABox contains ground sentences stating where in the hierarchy, individuals belong  i.e., relations between individuals and concepts . For example, the statement       1 belongs in the TBox, while the statement       2 belongs in the ABox. Note that the TBox ABox distinction is not significant, in the same sense that the two  kinds  of sentences are not treated differently in first order logic  which subsumes most DL . When translated into first order logic, a subsumption axiom like  1  is simply a conditional restriction to unary predicates  concepts  with only variables appearing in it. Clearly, a sentence of this form is not privileged or special over sentences in which only constants   grounded  values  appear like  2 . So why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision procedures for various DL. For example, a reasoner might process the TBox and ABox separately, in part because certain key inference problems are tied to one but not the other one   classification  is related to the TBox,  instance checking  to the ABox . Another example is that the complexity of the TBox can greatly affect the performance of a given decision procedure for a certain DL, independently of the ABox. Thus, it is useful to have a way to talk about that specific part of the knowledge base. The secondary reason is that the distinction can make sense from the knowledge base modeler s perspective. It is plausible to distinguish between our conception of terms concepts in the world  class axioms in the TBox  and particular manifestations of those terms concepts  instance assertions in the ABox . In the above example  when the hierarchy within a company is the same in every branch but the assignment to employees is different in every department  because there are other people working there , it makes sense to reuse the TBox for different branches that do not use the same ABox. There are two features of description logic that are not shared by most other data description formalisms  DL does not make the unique name assumption  UNA  or the closed world assumption  CWA . Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent. Not having CWA, or rather having the open world assumption  OWA  means that lack of knowledge of a fact does not immediately imply knowledge of the negation of a fact. Like first order logic  FOL , a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants. The syntax of a member of the description logic family is characterized by its recursive definition, in which the constructors that can be used to form concept terms are stated. Some constructors are related to logical constructors in first order logic  FOL  such as intersection or conjunction of concepts, union or disjunction of concepts, negation or complement of concepts, universal restriction and existential restriction. Other constructors have no corresponding construction in FOL including restrictions on roles for example, inverse, transitivity and functionality. Let C and D be concepts, a and b be individuals, and R be a role. If a is R related to b, then b is called an R successor of a. The prototypical DL Attributive Concept Language with Complements        A L C           was introduced by Manfred Schmidt Schau  and Gert Smolka in 1991, and is the basis of many more expressive DLs. The following definitions follow the treatment in Baader et al. Let      N  C        ,      N  R         and      N  O          be  respectively  sets of concept names  also known as atomic concepts , role names and individual names  also known as individuals, nominals or objects . Then the ordered triple       N  C        ,      N  R        ,      N  O          is the signature. The set of       A L C          concepts is the smallest set such that  A general concept inclusion  GCI  has the form     C   D      where     C      and     D      are concepts. Write     C   D      when     C   D      and     D   C     . A TBox is any finite set of GCIs.  An ABox is a finite set of assertional axioms. A knowledge base  KB  is an ordered pair         T   ,   A        ,      for TBox       T          and ABox       A         . The semantics of description logics are defined by interpreting concepts as sets of individuals and roles as sets of ordered pairs of individuals. Those individuals are typically assumed from a given domain. The semantics of non atomic concepts and roles is then defined in terms of atomic concepts and roles. This is done by using a recursive definition similar to the syntax. The following definitions follow the treatment in Baader et al. A terminological interpretation       I            I    ,      I            Delta   , cdot        over a signature        N  C   ,  N  R   ,  N  O       ,N ,N      consists of such that Define       I         models      read in I holds  as follows Let       K         T   ,   A           ,      be a knowledge base. In addition to the ability to describe concepts formally, one also would like to employ the description of a set of concepts to ask questions about the concepts and instances described. The most common decision problems are basic database query like questions like instance checking  is a particular instance  member of an ABox  a member of a given concept  and relation checking  does a relation role hold between two instances, in other words does a have property b , and the more global database questions like subsumption  is a concept a subset of another concept , and concept consistency  is there no contradiction among the definitions or chain of definitions . The more operators one includes in a logic and the more complicated the TBox  having cycles, allowing non atomic concepts to include each other , usually the higher the computational complexity is for each of these problems  see Description Logic Complexity Navigator for examples . Many DLs are decidable fragments of first order logic  FOL  and are usually fragments of two variable logic or guarded logic. In addition, some DLs have features that are not covered in FOL  this includes concrete domains  such as integer or strings, which can be used as ranges for roles such as hasAge or hasName  or an operator on roles for the transitive closure of that role. Fuzzy description logics combines fuzzy logic with DLs. Since many concepts that are needed for intelligent systems lack well defined boundaries, or precisely defined criteria of membership, fuzzy logic is needed to deal with notions of vagueness and imprecision. This offers a motivation for a generalization of description logic towards dealing with imprecise and vague concepts. Description logic is related to but developed independently of modal logic  ML . Many but not all DLs are syntactic variants of ML. In general, an object corresponds to a possible world, a concept corresponds to a modal proposition, and a role bounded quantifier to a modal operator with that role as its accessibility relation. Operations on roles  such as composition, inversion, etc.  correspond to the modal operations used in dynamic logic. Temporal description logic represents and allows reasoning about time dependent concepts and many different approaches to this problem exist. For example, a description logic might be combined with a modal temporal logic such as linear temporal logic. There are some semantic reasoners that deal with OWL and DL. These are some of the most popular  
As a subfield in artificial intelligence, diagnosis is concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct.  If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing.  The computation is based on observations, which provide information on the current behaviour. The expression diagnosis also refers to the answer of the question of whether the system is malfunctioning or not, and to the process of computing the answer.  This word comes from the medical context where a diagnosis is the process of identifying a disease by its symptoms. An example of diagnosis is the process of a garage mechanic with an automobile.  The mechanic will first try to detect any abnormal behavior based on the observations on the car and his knowledge of this type of vehicle.  If he finds out that the behavior is abnormal, the mechanic will try to refine his diagnosis by using new observations and possibly testing the system, until he discovers the faulty component  the mechanic plays an important role in the vehicle diagnosis. The expert diagnosis  or diagnosis by expert system  is based on experience with the system.  Using this experience, a mapping is built that efficiently associates the observations to the corresponding diagnoses. The experience can be provided  The main drawbacks of these methods are  A slightly different approach is to build an expert system from a model of the system rather than directly from an expertise.  An example is the computation of a diagnoser for the diagnosis of discrete event systems.  This approach can be seen as model based, but it benefits from some advantages and suffers some drawbacks of the expert system approach. Model based diagnosis is an example of abductive reasoning using a model of the system.  In general, it works as follows  We have a model that describes the behaviour of the system  or artefact .  The model is an abstraction of the behaviour of the system and can be incomplete.  In particular, the faulty behaviour is generally little known, and the faulty model may thus not be represented.  Given observations of the system, the diagnosis system simulates the system using the model, and compares the observations actually made to the observations predicted by the simulation. The modelling can be simplified by the following rules  where     A b       is the Abnormal predicate         A b   S     I n t 1   O b s 1          A b   S     I n t 2   O b s 2       fault model  The semantics of these formulae is the following  if the behaviour of the system is not abnormal  i.e. if it is normal , then the internal  unobservable  behaviour will be     I n t 1       and the observable behaviour     O b s 1      .  Otherwise, the internal behaviour will be     I n t 2       and the observable behaviour     O b s 2      .  Given the observations     O b s      , the problem is to determine whether the system behaviour is normal or not        A b   S         or     A b   S         .  This is an example of abductive reasoning. A system is said to be diagnosable if whatever the behavior of the system, we will be able to determine without ambiguity a unique diagnosis. The problem of diagnosability is very important when designing a system because on one hand one may want to reduce the number of sensors to reduce the cost, and on the other hand one may want to increase the number of sensors to increase the probability of detecting a faulty behavior. Several algorithms for dealing with these problems exist. One class of algorithms answers the question whether a system is diagnosable  another class looks for sets of sensors that make the system diagnosable, and optionally comply to criteria such as cost optimization. The diagnosability of a system is generally computed from the model of the system. In applications using model based diagnosis, such a model is already present and doesn t need to be built from scratch. DX is the annual International Workshop on Principles of Diagnosis that started in 1989.  Epistemology 
Digital empathy is the application of the core principles of empathy   compassion, cognition, and emotion   into technical designs to enhance user experience. According to Friesem  2016 , digital empathy is the cognitive and emotional ability to be reflective and socially responsible while strategically using digital media. Digital empathy finds its roots in empathy, a human behaviour explained by cognitive and behavioral neuroscientists as,  a multifaceted construct used to account for the capacity to share and understand the thoughts and feelings of others.    The neurological basis for empathy lies in mirror neurons, where perception and imitation facilitate empathy. At the centre of empathy creation is communication. As communication became increasingly online due to the rapid adoption of digital communications technology in the 1990s through the 2000s, society s communication patterns altered rapidly, both positively and negatively. Technology usage has transformed human interactions into digital conversations where people now have the ability to instantly share thoughts, feelings, and behaviours via digital channels in a few seconds. It has been observed and researched that digital conversations threaten the appropriate expression of empathy, largely as a result of  the  online disinhibition effect . Psychologist Dr. John Suler defines the online disinhibition effect as the tendency for   people say and do things in cyberspace that they wouldn t ordinarily say and do in the face to face world . Research has shown that the shift away from face to face communication has caused a decline in the social emotional skills of youth and suggest that  generations raised on technology  are becoming less empathic. Increasingly online communication patterns, and the associated phenomenon of online disinhibition, have led to research on   digital empathy . Christopher Terry and  Cain  2015  in their research paper  The Emerging Issue of Digital Empathy  define digital empathy as the  traditional empathic characteristics such as concern and caring for others expressed through computer mediated communications.   Yonty Friesem  2016  wrote that  digital empathy seeks to expand our thinking about traditional empathy phenomena into the digital arena.  In the handbook of research on media literacy in the digital age, Friesem  2015  further elaborates on this concept by stating that,  digital empathy explores the ability to analyze and evaluate another s internal state  empathy accuracy , have a sense of identity and agency  self empathy , recognize, understand and predict other s thoughts and emotions  cognitive empathy , feel what others feel  affective empathy , role play  imaginative empathy , and be compassionate to others  empathic concern  via digital media.  Digital empathy is used in DEX  Digital employee experience , healthcare and education. In healthcare, traditional empathetic characteristics can be understood as a physician s ability to understand the patient s experience and feelings, communicate and check their understanding of the situation, and act helpfully. According to Nina Margarita Gonzalez, digitally empathetic tools in healthcare should meet these conditions of physician empathy by designing empathetic tools through 3 key steps  understand, check, and act.  By ensuring that the digital tool understands the patient experience, the tool can act through  automated empathy  to provide validating statements or tips. For example, The National Cancer Institute created texting programs that collected information on user s smoking cessation efforts and provided validation or tips to support them, such as,  We know how you are feeling. Think about what you are gaining and why you want to quit smoking.   New health communications technology and telehealth makes clear the need for medical practitioners to recognize and adapt to online disinhibition and the lack of nonverbal cues. The University of the Highlands and Islands Experience Lab completed a study on empathy in video conferencing consultations with diabetes patients. It found that many factors impact the level of perceived empathy in a video conferencing consultation, including clarity of verbal communication, choice in pathways of care, and preparation and access to information before the consultation. Given the particular challenge that the online disinhibition effect poses to telehealth or other digital health communications, Terry and Cain suggest that, for physicians to effectively communicate empathy through digitally mediated interactions, they must be taught traditional empathy more broadly, as the foundational principles are the same. In education, researchers are often concerned with how to use digital technologies to teach empathy and how to teach students to be empathetic when using digital platforms. In  Empathy for the Digital Age , Yonty Friesem  2016  found that empathy can be taught to youth through video production, where the majority of students experienced higher levels of empathy after writing, producing, creating, and screening their videos in a project designed to foster empathy. Cheryl Wei yu Chen similarly found that video projects can help youth develop awareness of empathy in digitally mediated interactions. In their research, Friesem and Greene  2020  used digital empathy to promote digital and media literacy skills of foster youth. Practicing cognitive, emotional and social empathy through digital media have been effective to support not only the academic skills of the foster youth, but also their well being, sense of belonging and media literacy skills. As an important practice of digital literacy and media education, digital empathy is an inclusive and collaborative experience as students learn to produce their own media. Digital Employee Experience is a relatively new  circa 2019  field of IT. It focuses on ensuring employees have the best possible  Digital  experience when using technology in work. DEX market leaders 1E  use concepts of Digital Empathy in the implementation of their DEX platform. They describe Digital Empathy as the solution to  IT Friction  which occurs when devices, applications and systems do not work or react as users expect them to, causing user frustration, increasing IT costs and loss of productivity.  
Dynamic epistemic logic  DEL  is a logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. These events can change factual properties of the actual world  they are called ontic events   for example a red card is painted in blue. They can also bring about changes of knowledge without changing factual properties of the world  they are called epistemic events   for example a card is revealed publicly  or privately  to be red. Originally, DEL focused on epistemic events. We only present in this entry some of the basic ideas of the original DEL framework  more details about DEL in general can be found in the references. Due to the nature of its object of study and its abstract approach, DEL is related and has applications to numerous research areas, such as computer science  artificial intelligence , philosophy  formal epistemology , economics  game theory  and cognitive science. In computer science, DEL is for example very much related to multi agent systems, which are systems where multiple intelligent agents interact and exchange information. As a combination of dynamic logic and epistemic logic, dynamic epistemic logic is a young field of research. It really started in 1989 with Plaza s logic of public announcement.  Independently, Gerbrandy and Groeneveld  proposed a system dealing moreover with private announcement and that was inspired by the work of Veltman. Another system was proposed by van Ditmarsch whose main inspiration was the Cluedo game. But the most influential and original system was the system proposed by Baltag, Moss and Solecki. This system can deal with all the types of situations studied in the works above and its underlying methodology is conceptually grounded. We will present in this entry some of its basic ideas. Formally, DEL extends ordinary epistemic logic by the inclusion of event models to describe actions, and a product update operator that defines how epistemic models are updated as the consequence of executing actions described through event models. Epistemic logic will first be recalled. Then, actions and events will enter into the picture and we will introduce the DEL framework. Epistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of reasoning about knowledge and belief  which principles relating the notions of knowledge and belief are intuitively plausible? Like epistemology, it stems from the Greek word                          or  episteme  meaning knowledge. Epistemology is nevertheless more concerned with analyzing the very nature and scope of knowledge, addressing questions such as  What is the definition of knowledge?  or  How is knowledge acquired? . In fact, epistemic logic grew out of epistemology in the Middle Ages thanks to the efforts of Burley and Ockham. The formal work, based on modal logic, that inaugurated contemporary research into epistemic logic dates back only to 1962 and is due to Hintikka. It then sparked in the 1960s discussions about the principles of knowledge and belief and many axioms for these notions were proposed and discussed. For example, the interaction axioms     K p   B p      and     B p   K B p      are often considered to be intuitive principles  if an agent Knows     p      then  s he also Believes     p     , or if an agent Believes     p     , then  s he Knows that  s he Believes     p     . More recently, these kinds of philosophical theories were taken up by researchers in economics, artificial intelligence and theoretical computer science where reasoning about knowledge is a central topic. Due to the new setting in which epistemic logic was used, new perspectives and new features such as computability issues were then added to the research agenda of epistemic logic. In the sequel,     A G T S     1 ,   , n         is a finite set whose elements are called agents and     P R O P      is a set of propositional letters. The epistemic language is an extension of the basic multi modal language of modal logic with a common knowledge operator      C  A         and a distributed knowledge operator      D  A        . Formally, the epistemic language        L     EL    C             is defined inductively by the following grammar in BNF         L     EL    C                   p                                  K  j            C  A            D  A             phi        p  mid   neg  phi   mid    phi  land  phi    mid  K  phi   mid  C  phi   mid  D  phi     where     p   P R O P     ,     j    A G T S        and     A    A G T S       . The basic epistemic language        L    E L           is the language        L    E L   C            without the common knowledge and distributed knowledge operators. The formula            is an abbreviation for       p   p       for a given     p   P R O P      ,         K  j          rangle  phi     is an abbreviation for        K  j          neg  phi    ,      E  A        phi     is an abbreviation for         j   A    K  j       K  phi     and     C        an abbreviation for      C  A G T S        phi    . Group notions  general, common and distributed knowledge. In a multi agent setting there are three important epistemic concepts  general knowledge, distributed knowledge and common knowledge. The notion of common knowledge was first studied by Lewis in the context of conventions. It was then applied to distributed systems  and to game theory, where it allows to express that the rationality of the players, the rules of the game and the set of players are commonly known. General knowledge. General knowledge of            means that everybody in the group of agents      A G T S        knows that           . Formally, this corresponds to the following formula      E           j    A G T S      K  j     .     K  phi .    Common knowledge. Common knowledge of            means that everybody knows            but also that everybody knows that everybody knows           , that everybody knows that everybody knows that everybody knows           , and so on ad infinitum. Formally, this corresponds to the following formula     C      E     E E     E E E            As we do not allow infinite conjunction the notion of common knowledge will have to be introduced as a primitive in our language. Before defining the language with this new operator, we are going to give an example introduced by Lewis that illustrates the difference between the notions of general knowledge and common knowledge. Lewis wanted to know what kind of knowledge is needed so that the statement     p        every driver must drive on the right  be a convention among a group of agents. In other words, he wanted to know what kind of knowledge is needed so that everybody feels safe to drive on the right. Suppose there are only two agents     i      and     j     . Then everybody knowing     p       formally     E p       is not enough. Indeed, it might still be possible that the agent     i      considers possible that the agent     j      does not know     p       formally        K  i    K  j   p   K p    . In that case the agent     i      will not feel safe to drive on the right because he might consider that the agent     j     , not knowing     p     , could drive on the left. To avoid this problem, we could then assume that everybody knows that everybody knows that     p       formally     E E p      . This is again not enough to ensure that everybody feels safe to drive on the right. Indeed, it might still be possible that agent     i      considers possible that agent     j      considers possible that agent     i      does not know     p       formally        K  i    K  j    K  i   p   K K p    . In that case and from     i      s point of view,     j      considers possible that     i     , not knowing     p     , will drive on the left. So from     i      s point of view,     j      might drive on the left as well  by the same argument as above . So     i      will not feel safe to drive on the right. Reasoning by induction, Lewis showed that for any     k    N        ,     E p    E  1   p        E  k   p   p land  ldots  land E p    is not enough for the drivers to feel safe to drive on the right. In fact what we need is an infinite conjunction. In other words, we need common knowledge of     p           C p     . Distributed knowledge. Distributed knowledge of            means that if the agents pulled their knowledge altogether, they would know that            holds. In other words, the knowledge of            is distributed among the agents. The formula      D  A        phi     reads as  it is distributed knowledge among the set of agents     A      that            holds . Epistemic logic is a modal logic. So, what we call an epistemic model       M       W ,  R  1   ,   ,  R  n   , I        W,R , ldots ,R ,I     is just a Kripke model as defined in modal logic. The set     W      is a non empty set whose elements are called possible worlds and the interpretation     I   W    2  P R O P         is a function specifying which propositional facts  such as  Ann has the red card   are true in each of these worlds. The accessibility relations      R  j     W   W    subseteq W times W    are binary relations for each agent     j   A G T S       they are intended to capture the uncertainty of each agent  about the actual world and about the other agents  uncertainty . Intuitively, we have       w , v      R  j         when the world     v      is compatible with agent     j      s information in world     w      or, in other words, when agent     j      considers that world     v      might correspond to the world     w       from this standpoint . We abusively write     w     M          for     w   W      and      R  j     w      w     denotes the set of worlds       v   W     w , v      R  j            . Intuitively, a pointed epistemic model         M   , w      ,w    , where     w     M         , represents from an external point of view how the actual world     w      is perceived by the agents      A G T S       . For every epistemic model       M         , every     w     M          and every            L     EL            , we define       M   , w        ,w models  phi     inductively by the following truth conditions  where               j   A     R  j               R  right      is the transitive closure of          j   A     R  j      R      we have that     v             j   A     R  j              w      R  right   w     if, and only if, there are      w  0   ,   ,  w  m       M     , ldots ,w  in      and      j  1   ,   ,  j  m     A   , ldots ,j  in A    such that      w  0     w ,  w  m     v    w,w  v    and for all     i     1 ,   , m        ,      w  i   1    R   j  i      w  i     R  w    . Despite the fact that the notion of common belief has to be introduced as a primitive in the language, we can notice that the definition of epistemic models does not have to be modified in order to give truth value to the common knowledge and distributed knowledge operators. Card Example  Players     A     ,     B      and     C       standing for Ann, Bob and Claire  play a card game with three cards  a red one, a green one and a blue one. Each of them has a single card but they do not know the cards of the other players. Ann has the red card, Bob has the green card and Claire has the blue card. This example is depicted in the pointed epistemic model         M   , w      ,w     represented below. In this example,     A G T S      A , B , C         and     P R O P         A    ,    B    ,    C    ,    B    ,    C    ,    A    ,    C    ,    A    ,    B         , , , , , , , ,      . Each world is labelled by the propositional letters which are true in this world and     w      corresponds to the actual world. There is an arrow indexed by agent     j     A , B , C         from a possible world     u      to a possible world     v      when       u , v      R  j        . Reflexive arrows are omitted, which means that for all     j     A , B , C         and all     v     M         , we have that       v , v      R  j        .        A           stands for        A      has the red card          C           stand for       C      has the blue card          B           stands for       B      has the green card   and so on... When accessibility relations are equivalence relations  like in this example  and we have that       w , v      R  j        , we say that agent     j      cannot distinguish world     w      from world     v       or world     w      is indistinguishable from world     v      for agent     j      . So, for example,     A      cannot distinguish the actual world     w      from the possible world where     B      has the blue card         B           ,     C      has the green card         C            and     A      still has the red card         A           . In particular, the following statements hold        M   , w        A       K  A      A             C       K  C      C             B       K  B      B         ,w models    land K    land    land K    land    land K        All the agents know the color of their card .       M   , w    K  A        B         B         K  A        C         C         ,w models K    vee    land K    vee            A      knows that     B      has either the blue or the green card and that     C      has either the blue or the green card .       M   , w   E      A         A         A        C      A         A         A         ,w models E   vee   vee    land C   vee   vee        Everybody knows that     A      has either the red, green or blue card and this is even common knowledge among all agents . We use the same notation      K  j         for both knowledge and belief. Hence, depending on the context,      K  j        phi     will either read  the agent     j      Knows that            holds  or  the agent     j      Believes that            holds . A crucial difference is that, unlike knowledge, beliefs can be wrong  the axiom      K  j            phi  rightarrow  phi     holds only for knowledge, but not necessarily for belief. This axiom called axiom T  for Truth  states that if the agent knows a proposition, then this proposition is true. It is often considered to be the hallmark of knowledge and it has not been subjected to any serious attack ever since its introduction in the Theaetetus by Plato. The notion of knowledge might comply to some other constraints  or axioms  such as      K  j        K  j    K  j        phi  rightarrow K K  phi      if agent     j      knows something, she knows that she knows it. These constraints might affect the nature of the accessibility relations      R  j         which may then comply to some extra properties. So, we are now going to define some particular classes of epistemic models that all add some extra constraints on the accessibility relations      R  j        . These constraints are matched by particular axioms for the knowledge operator      K  j        . Below each property, we give the axiom which defines the class of epistemic frames that fulfill this property.      K        stands for      K  j        phi     for any     j   A G T S     .  We discuss the axioms above. Axiom 4 states that if the agent knows a proposition, then she knows that she knows it  this axiom is also known as the  KK principle or  KK thesis  . In epistemology, axiom 4 tends to be accepted by internalists, but not by externalists. Axiom 4 is nevertheless widely accepted by computer scientists  but also by many philosophers, including Plato, Aristotle, Saint Augustine, Spinoza and Schopenhauer, as Hintikka recalls  . A more controversial axiom for the logic of knowledge is axiom 5 for Euclidicity  this axiom states that if the agent does not know a proposition, then she knows that she does not know it. Most philosophers  including Hintikka  have attacked this axiom, since numerous examples from everyday life seem to invalidate it. In general, axiom 5 is invalidated when the agent has mistaken beliefs, which can be due for example to misperceptions, lies or other forms of deception. Axiom B states that it cannot be the case that the agent considers it possible that she knows a false proposition  that is,                 K   K          . If we assume that axioms T and 4 are valid, then axiom B falls prey to the same attack as the one for axiom 5 since this axiom is derivable. Axiom D states that the agent s beliefs are consistent. In combination with axiom K  where the knowledge operator is replaced by a belief operator , axiom D is in fact equivalent to a simpler axiom D  which conveys, maybe more explicitly, the fact that the agent s beliefs cannot be inconsistent        B       . The other intricate axioms .2, .3, .3.2 and .4 have been introduced by epistemic logicians such as Lenzen and Kutchera in the 1970s and presented for some of them as key axioms of epistemic logic. They can be characterized in terms of intuitive interaction axioms relating knowledge and beliefs. The Hilbert proof system K for the basic modal logic is defined by the following axioms and inference rules  for all     j   A G T S     , The axioms of an epistemic logic obviously display the way the agents reason. For example, the axiom K together with the rule of inference Nec entail that if I know                 K         and I know that            implies                  K                  then I know that                 K        . Stronger constraints can be added. The following  proof systems for        L     EL             are often used in the literature. We define the set of proof systems       L    EL           K   ,   KD45   ,   S4   ,   S4.2   ,   S4.3   ,   S4.3.2   ,   S4.4   ,   S5              , , , , , , ,      . Moreover, for all       H       L    EL        in  mathbb       , we define the proof system        H     C             by adding the following axiom schemes and rules of inference to those of       H         . For all     A   A G T S     , The relative strength of the proof systems for knowledge is as follows        S4       S4.2       S4.3       S4.3.2       S4.4       S5   .     subset   subset   subset   subset   subset  .    So, all the theorems of       S4.2          are also theorems of       S4.3   ,   S4.3.2   ,   S4.4      , ,     and       S5         . Many philosophers claim that in the most general cases, the logic of knowledge is       S4.2          or       S4.3         . Typically, in computer science and in many of the theories developed in artificial intelligence, the logic of belief  doxastic logic  is taken to be       KD45          and the logic of knowledge  epistemic logic  is taken to be       S5         , even if       S5          is only suitable for situations where the agents do not have mistaken beliefs.       Br          has been propounded by Floridi as the logic of the notion of  being informed  which mainly differs from the logic of knowledge by the absence of introspection for the agents. For all       H       L    EL        in  mathbb       , the class of       H          models or        H     C             models is the class of epistemic models whose accessibility relations satisfy the properties listed above defined by the axioms of       H          or        H     C            . Then, for all       H       L    EL        in  mathbb       ,       H          is sound and strongly complete for        L     EL             w.r.t. the class of       H          models, and        H     C             is sound and strongly complete for        L     EL     C               w.r.t. the class of        H     C             models. The satisfiability problem for all the logics introduced is decidable. We list below the computational complexity of the satisfiability problem for each of them. Note that it becomes linear in time if there are only finitely many propositional letters in the language. For     n   2     , if we restrict to finite nesting, then the satisfiability problem is NP complete for all the modal logics considered. If we then further restrict the language to having only finitely many primitive propositions, the complexity goes down to linear in time in all cases. The computational complexity of the model checking problem is in P in all cases. Dynamic Epistemic Logic  DEL  is a logical framework for modeling epistemic situations involving several agents, and changes that occur to these situations as a result of incoming information or more generally incoming action. The methodology of DEL is such that it splits the task of representing the agents  beliefs and knowledge into three parts  Typically, an informative event can be a public announcement to all the agents of a formula             this public announcement and correlative update constitute the dynamic part. However, epistemic events can be much more complex than simple public announcement, including hiding information for some of the agents, cheating, lying, bluffing, etc. This complexity is dealt with when we introduce the notion of event model. We will first focus on public announcements to get an intuition of the main underlying ideas of DEL. In this section, we assume that all events are public. We start by giving a concrete example where DEL can be used, to better understand what is going on. This example is called the muddy children puzzle. Then, we will present a formalization of this puzzle in a logic called Public Announcement Logic  PAL . The muddy children puzzle is one of the most well known puzzles that played a role in the development of DEL. Other significant puzzles include the sum and product puzzle, the Monty Hall dilemma, the Russian cards problem, the two envelopes problem, Moore s paradox, the hangman paradox, etc. Muddy Children Example  We have two children, A and B, both dirty. A can see B but not himself, and B can see A but not herself. Let     p      be the proposition stating that A is dirty, and     q      be the proposition stating that B is dirty. Public announcement logic  PAL   We present the syntax and semantic of Public Announcement Logic  PAL , which combines features of epistemic logic and propositional dynamic logic. We define the language         L    P A L             inductively by the following grammar in BNF          L    P A L                    p                                  K  j               !            phi        p  mid   neg  phi   mid    phi  land  phi    mid  K  phi   mid   phi     where     j   A G T S     . The language         L    P A L             is interpreted over epistemic models. The truth conditions for the connectives of the epistemic language are the same as in epistemic logic  see above . The truth condition for the new dynamic action modality         !          is defined as follows  where        M              W      ,  R  1       ,   ,  R  n       ,  I               W ,R  , ldots ,R  ,I      with      W           w   W     M   , w             ,w models  psi      ,      R  j           R  j        W         W             R  cap  W  times W      for all     j     1 ,   , n         and      I        w      I   w      for all    w    W         w   I w  w in W    .  The formula         !          intuitively means that after a truthful announcement of           ,            holds. A public announcement of a proposition            changes the current epistemic model like in the figure below.The proof system        H    P A L           defined below is sound and strongly complete for         L    P A L             w.r.t. the class of all pointed epistemic models. The axioms Red 1   Red 4 are called reduction axioms because they allow to reduce any formula of         L    P A L             to a provably equivalent formula of        L    E L           in        H    P A L          . The formula       q !   K q      is a theorem provable in        H    P A L          . It states that after a public announcement of     q     , the agent knows that     q      holds. PAL is decidable, its model checking problem is solvable in polynomial time and its satisfiability problem is PSPACE complete. Muddy children puzzle formalized with PAL  Here are some of the statements that hold in the muddy children puzzle formalized in PAL.       N   , s   p   q    ,s models p land q     In the initial situation, A is dirty and B is dirty .       N   , s        K  A   p      K  A     p          K  B   q      K  B     q      ,s models   neg K p land  neg K  neg p  land   neg K q land  neg K  neg q      In the initial situation, A does not know whether he is dirty and B neither .       N   , s     p   q !      K  A     p   q      K  B     p   q        ,s models  K  p vee q  land K  p vee q       After the public announcement that at least one of the children A and B is dirty, both of them know that at least one of them is dirty . However        N   , s     p   q !          K  A   p      K  A     p          K  B   q      K  B     q        ,s models    neg K p land  neg K  neg p  land   neg K q land  neg K  neg q       After the public announcement that at least one of the children A and B is dirty, they still do not know that they are dirty . Moreover        N   , s     p   q !          K  A   p      K  A     p          K  B   q      K  B     q   !      K  A   p    K  B   q      ,s models  K p land K q      After the successive public announcements that at least one of the children A and B is dirty and that they still do not know whether they are dirty, A and B then both know that they are dirty . In this last statement, we see at work an interesting feature of the update process  a formula is not necessarily true after being announced. That is what we technically call  self persistence  and this problem arises for epistemic formulas  unlike propositional formulas . One must not confuse the announcement and the update induced by this announcement, which might cancel some of the information encoded in the announcement. In this section, we assume that events are not necessarily public and we focus on items 2 and 3 above, namely on how to represent events and on how to update an epistemic model with such a representation of events by means of a product update. Epistemic models are used to model how agents perceive the actual world. Their perception can also be described in terms of knowledge and beliefs about the world and about the other agents  beliefs. The insight of the DEL approach is that one can describe how an event is perceived by the agents in a very similar way. Indeed, the agents  perception of an event can also be described in terms of knowledge and beliefs. For example, the private announcement of     A      to     B      that her card is red can also be described in terms of knowledge and beliefs  while     A      tells     B      that her card is red  event     e           C      believes that nothing happens  event     f      . This leads to define the notion of event model whose definition is very similar to that of an epistemic model. A pointed event model         E   , e      ,e     represents how the actual event represented by     e      is perceived by the agents. Intuitively,     f    R  j     e      e     means that while the possible event represented by     e      is occurring, agent     j      considers possible that the possible event represented by     f      is actually occurring. An event model is a tuple       E        W      ,  R  1       ,   ,  R  m       ,  I             W ,R  , ldots ,R  ,I      where       R  j         e       e     denotes the set       f    W          e , f      R  j             e,f  in R        .We write     e     E          for     e    W           , and         E   , e      ,e     is called a pointed event model      e      often represents the actual event . Card Example  Let us resume the card example and assume that players     A      and     B      show their card to each other. As it turns out,     C      noticed that     A      showed her card to     B      but did not notice that     B      did so to     A     . Players     A      and     B      know this. This event is represented below in the event model         E   , e      ,e    . The possible event     e      corresponds to the actual event  players     A      and     B      show their and cards respectively to each other   with precondition        A         B        land      ,     f      stands for the event  player     A      shows her green card   with precondition        A            and     g      stands for the atomic event  player     A      shows her red card   with precondition        A           . Players     A      and     B      show their cards to each other, players     A      and     B      know this and consider it possible, while player     C      considers possible that player     A      shows her red card and also considers possible that player     A      shows her green card, since he does not know her card. In fact, that is all that player     C      considers possible because she did not notice that     B      showed her card. Another example of event model is given below. This second example corresponds to the event whereby Player     A      shows her red card publicly to everybody. Player     A      shows her red card, players     A     ,     B      and     C       know  it, players     A     ,     B      and     C       know  that each of them  knows  it, etc. In other words, there is common knowledge among players     A     ,     B      and     C      that player     A      shows her red card. The DEL product update is defined below. This update yields a new pointed epistemic model         M   , w         E   , e      ,w  otimes   ,e     representing how the new situation which was previously represented by         M   , w      ,w     is perceived by the agents after the occurrence of the event represented by         E   , e      ,e    . Let       M       W ,  R  1   ,   ,  R  n   , I        W,R , ldots ,R ,I     be an epistemic model and let       E        W      ,  R  1       ,   ,  R  n       ,  I             W ,R  , ldots ,R  ,I      be an event model. The product update of       M          and       E          is the epistemic model       M        E         W      ,  R  1       ,   ,  R  n       ,  I            otimes     W ,R  , ldots ,R  ,I      defined as follows  for all     v   W      and all     f    W           , If     w   W      and     e    W            are such that       M   , w    I        e      ,w models I  e     then         M   , w         E   , e      ,w  otimes   ,e     denotes the pointed epistemic model         M       E   ,   w , e         otimes  , w,e     . This definition of the product update is conceptually grounded. Card Example  As a result of the first event described above  Players     A      and     B      show their cards to each other in front of player     C      , the agents update their beliefs. We get the situation represented in the pointed epistemic model         M   , w         E   , e      ,w  otimes   ,e     below. In this pointed epistemic model, the following statement holds          M   , w         E   , e          B       K  A      B         K  C      K  A      B    .    ,w  otimes   ,e  models    land K    land K  neg K  .    It states that player     A      knows that player     B      has the card but player     C       believes  that it is not the case. The result of the second event is represented below. In this pointed epistemic model, the following statement holds          M   , w         F   , e      C    B , C          A         B         C           K  A        B         C         ,w  otimes   ,e  models C     land   land    land  neg K    land      . It states that there is common knowledge among     B      and     C      that they know the true state of the world  namely     A      has the red card,     B      has the green card and     C      has the blue card , but     A      does not know it. Based on these three components  epistemic model, event model and product update , Baltag, Moss and Solecki defined a general logical language inspired from the logical language of propositional dynamic logic to reason about information and knowledge change. 
Edmond de Belamy is a generative adversarial network portrait painting constructed in 2018 by Paris based arts collective Obvious. Printed on canvas, the work belongs to a series of generative images called La Famille de Belamy. The name Belamy is a tribute to Ian Goodfellow, inventor of GANs  In French  bel ami  means  good friend  so it is a translated pun of Goodfellow.    It achieved widespread notoriety after Christie s announced its intention to auction the piece as the first artwork created using artificial intelligence to be featured in a Christie s auction. It surpassed pre auction estimates which valued it at  7,000 to  10,000, instead selling for  432,500. The piece is a portrait of a somewhat blurry man. It is a print on canvas measuring 27 1 2 x 27 1 2 in  700 x 700 mm.  set within a gilded wood frame. The image was created by an algorithm that referenced 15,000 portraits from various periods. It is signed at the bottom right with      min   G     max   D     E  x       log       D     x            E  z       log     1     D       G     z               max   E  left E  left   , which is part of the algorithm code that produced it. The algorithm was trained on a set of 15,000 portraits from online art encyclopedia WikiArt, spanning the 14th to the 19th century. The organization that produced it is called Obvious. It is a collective comprising three people, Pierre Fautrel, Hugo Caselles Dupr  and Gauthier Vernier, who are based in Paris, France. The piece has been criticized because it was created using a generative adversarial network  GAN  software package based on prior research by others and implemented by Robbie Barrat, an AI artist who was not affiliated with Obvious, leading to allegations that Obvious contributed minimally to the final work product. Posts on the project s issue tracker show Obvious members requesting that Barrat provide support and custom features. The piece has also been placed within a tradition, dating back to Duchamp s Bicycle Wheel of 1913 and Tinguely s M ta matics of the late 1950s, of works calling into question the basis of the modern art market, and highlighting the comic aspects of technology. Research has used Edmond de Belamy to show how anthropomorphizing AI can affect allocations of responsibility and credit to artists. 
Elements of AI is a massive open online course  MOOC  teaching the basics of artificial intelligence. The course, originally launched in 2018, is designed and organized by the University of Helsinki and learning technology company MinnaLearn. The course includes modules on machine learning, neural networks, the philosophy of artificial intelligence, and using artificial intelligence to solve problems. It consists of two parts  Introduction to AI and its sequel, Building AI, that was released in late 2020. University of Helsinki s computer science department is known as the alma mater of Linus Torvalds, a Finnish American software engineer who is the creator of the Linux kernel, which is the kernel for Linux operating systems. The government of Finland has pledged to offer the course for all EU citizens by the end of 2021, as the course is made available in all the official EU languages. The initiative was launched as part of Finland s Presidency of the Council of the European Union in 2019, with the European Commission providing translations of the course materials. In 2017, Finland launched an AI strategy to stay competitive in the field of AI amid growing competition between China and the United States. With the support of private companies and the government, Finland s now realized goal was to get 1 percent of its citizens to participate in Elements of AI. Other governments have also given their support to the course. For instance, Germany s Federal Minister for Economic Affairs and Energy Peter Altmeier has encouraged citizens to take part in the course to help Germany gain a competitive advantage in AI. Sweden s Minister for Energy and Minister for Digital Development Anders Ygeman has said that Sweden aims to teach 1 percent of its population the basics of AI like Finland has. Elements of AI had enrolled more than 850,000 students from more than 110 countries by September 2022. A quarter of the course s participants are aged 45 and over, and some 40 percent are women. Among Nordic participants, the share of women is nearly 60 percent. In September 2022, the course was available in Finnish, Swedish, Estonian, English, German, Latvian, Norwegian, French, Belgian, Czech, Greek, Slovakian, Slovenian, Latvian, Lithuanian, Portuguese, Spanish, Irish, Icelandic, Maltese, Croatian, Romanian, Italian, Dutch, Polish, and Danish. 
In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. A branch of artificial intelligence focuses on empowering such agents to interact autonomously with human beings and the environment. Mobile robots are one example of physically embodied agents  Ananova and Microsoft Agent are examples of graphically embodied agents. Embodied conversational agents are embodied agents  usually with a graphical front end as opposed to a robotic body  that are capable of engaging in conversation with one another and with humans employing the same verbal and nonverbal means that humans do  such as gesture, facial expression, and so forth . Embodied conversational agents are a form of intelligent user interface. Graphically embodied agents aim to unite gesture, facial expression and speech to enable face to face communication with users, providing a powerful means of human computer interaction. Face to face communication allows communication protocols that give a much richer communication channel than other means of communicating. It enables pragmatic communication acts such as conversational turn taking, facial expression of emotions, information structure and emphasis, visualisation and iconic gestures, and orientation in a three dimensional environment. This communication takes place through both verbal and non verbal channels such as gaze, gesture, spoken intonation and body posture. Research has found that users prefer a non verbal visual indication of an embodied system s internal state to a verbal indication, demonstrating the value of additional non verbal communication channels. As well as this, the face to face communication involved in interacting with an embodied agent can be conducted alongside another task without distracting the human participants, instead improving the enjoyment of such an interaction. Furthermore, the use of an embodied presentation agent results in improved recall of the presented information. Embodied agents also provide a social dimension to the interaction. Humans willingly ascribe social awareness to computers, and thus interaction with embodied agents follows social conventions, similar to human to human interactions. This social interaction both raises the believability and perceived trustworthiness of agents, and increases the user s engagement with the system. Rickenberg and Reeves found that the presence of an embodied agent on a website increased the level of user trust in that website, but also increased users  anxiety and affected their performance, as if they were being watched by a real human. Another effect of the social aspect of agents is that presentations given by an embodied agent are perceived as being more entertaining and less difficult than similar presentations given without an agent. Research shows that perceived enjoyment, followed by perceived usefulness and ease of use, is the major factor influencing user adoption of embodied agents. A study in January 2004 by Byron Reeves at Stanford demonstrated how digital characters could  enhance online experiences  through explaining how virtual characters essentially add a sense of relatability to the user experience and make it more approachable. This increase in likability in turn helps make the products better, which benefits both the end users and those creating the product.  The rich style of communication that characterises human conversation makes conversational interaction with embodied conversational agents ideal for many non traditional interaction tasks. A familiar application of graphically embodied agents is computer games  embodied agents are ideal for this setting because the richer communication style makes interacting with the agent enjoyable. Embodied conversational agents have also been used in virtual training environments, portable personal navigation guides, interactive fiction and storytelling systems, interactive online characters and automated presenters and commentators. Major virtual assistants like Siri, Amazon Alexa and Google Assistant do not come with any visual embodied representation, which is believed to limit the sense of human presence by users. The U.S. Department of Defense utilizes a software agent called SGT STAR on U.S. Army run Web sites and Web applications for site navigation, recruitment and propaganda purposes. Sgt. Star is run by the Army Marketing and Research Group, a division operated directly from The Pentagon. Sgt. Star is based upon the ActiveSentry technology developed by Next IT, a Washington based information technology services company. Other such bots in the Sgt. Star  family  are utilized by the Federal Bureau of Investigation and the Central Intelligence Agency for intelligence gathering purposes. 
Embodied cognitive science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies  the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity  the formation of a common set of general principles of intelligent behavior  and the experimental use of robotic agents in controlled environments. Embodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. Contributors to the field include  In 1950, Alan Turing proposed that a machine may need a human like body to think and speak  It can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then teach it to understand and speak English. That process could follow the normal teaching of a child. Things would be pointed out and named, etc. Again, I do not know what the right answer is, but I think both approaches should be tried.Embodied cognitive science is an alternative theory to cognition in which it minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism s body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit finds semantic meaning. Thus, an appropriate output is produced. For example, a human s sensory organs are its input devices, and the stimuli obtained from the external environment are fed into the nervous system which serves as the processing unit. From here, the nervous system is able to read the sensory information because it follows a syntactic structure, thus an output is created. This output then creates bodily motions and brings forth behavior and cognition. Of particular note is that cognition is sealed away in the brain, meaning that mental cognition is cut off from the external world and is only possible by the input of sensory information. Embodied cognitive science differs from the traditionalist approach in that it denies the input output system. This is chiefly due to the problems presented by the Homunculus argument, which concluded that semantic meaning could not be derived from symbols without some kind of inner interpretation. If some little man in a person s head interpreted incoming symbols, then who would interpret the little man s inputs? Because of the specter of an infinite regress, the traditionalist model began to seem less plausible. Thus, embodied cognitive science aims to avoid this problem by defining cognition in three ways.  340  The first aspect of embodied cognition examines the role of the physical body, particularly how its properties affect its ability to think. This part attempts to overcome the symbol manipulation component that is a feature of the traditionalist model. Depth perception, for instance, can be better explained under the embodied approach due to the sheer complexity of the action. Depth perception requires that the brain detect the disparate retinal images obtained by the distance of the two eyes. In addition, body and head cues complicate this further. When the head is turned in a given direction, objects in the foreground will appear to move against objects in the background. From this, it is said that some kind of visual processing is occurring without the need of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary.  A more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible auditory acuity. Also relevant is the amount of density in between the ears, for the strength of the frequency wave alters as it passes through a given medium. The brain s auditory system takes these factors into account as it process information, but again without any need for a symbolic manipulation system. This is because the distance between the ears for example does not need symbols to represent it. The distance itself creates the necessary opportunity for greater auditory acuity. The amount of density between the ears is similar, in that it is the actual amount itself that simply forms the opportunity for frequency alteration. Thus under consideration of the physical properties of the body, a symbolic system is unnecessary and an unhelpful metaphor. The second aspect draws heavily from George Lakoff s and Mark Johnson s work on concepts. They argued that humans use metaphors whenever possible to better explain their external world. Humans also have a basic stock of concepts in which other concepts can be derived from. These basic concepts include spatial orientations such as up, down, front, and back. Humans can understand what these concepts mean because they can directly experience them from their own bodies. For example, because human movement revolves around standing erect and moving the body in an up down motion, humans innately have these concepts of up and down. Lakoff and Johnson contend this is similar with other spatial orientations such as front and back too. As mentioned earlier, these basic stocks of spatial concepts are the basis in which other concepts are constructed. Happy and sad for instance are seen now as being up or down respectively. When someone says they are feeling down, what they are really saying is that they feel sad for example. Thus the point here is that true understanding of these concepts is contingent on whether one can have an understanding of the human body. So the argument goes that if one lacked a human body, they could not possibly know what up or down could mean, or how it could relate to emotional states. magine a spherical being living outside of any gravitational field, with no knowledge or imagination of any other kind of experience. What could UP possibly mean to such a being?  342 While this does not mean that such beings would be incapable of expressing emotions in other words, it does mean that they would express emotions differently from humans. Human concepts of happiness and sadness would be different because human would have different bodies. So then an organism s body directly affects how it can think, because it uses metaphors related to its body as the basis of concepts. A third component of the embodied approach looks at how agents use their immediate environment in cognitive processing. Meaning, the local environment is seen as an actual extension of the body s cognitive process. The example of a personal digital assistant  PDA  is used to better imagine this. Echoing functionalism  philosophy of mind , this point claims that mental states are individuated by their role in a much larger system. So under this premise, the information on a PDA is similar to the information stored in the brain. So then if one thinks information in the brain constitutes mental states, then it must follow that information in the PDA is a cognitive state too. Consider also the role of pen and paper in a complex multiplication problem. The pen and paper are so involved in the cognitive process of solving the problem that it seems ridiculous to say they are somehow different from the process, in very much the same way the PDA is used for information like the brain. Another example examines how humans control and manipulate their environment so that cognitive tasks can be better performed. Leaving one s car keys in a familiar place so they aren t missed for instance, or using landmarks to navigate in an unfamiliar city. Thus, humans incorporate aspects of their environment to aid in their cognitive functioning. The value of the embodiment approach in the context of cognitive science is perhaps best explained by Andy Clark.  345 351  He makes the claim that the brain alone should not be the single focus for the scientific study of cognition It is increasingly clear that, in a wide variety of cases, the individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.  350 The following examples used by Clark will better illustrate how embodied thinking is becoming apparent in scientific thinking. Thunnus, or tuna, long baffled conventional biologists with its incredible abilities to accelerate quickly and attain great speeds. A biological examination of the tuna shows that it should not be capable of such feats. However, an answer can be found when taking the tuna s embodied state into account. The bluefin tuna is able to take advantage of and exploit its local environment by finding naturally occurring currents to increase its speed. The tuna also uses its own physical body for this end as well, by utilizing its tailfin to create the necessary vortices and pressure so it can accelerate and maintain high speeds. Thus, the bluefin tuna is actively using its local environment for its own ends through the attributes of its physical body. Clark uses the example of the hopping robot constructed by Raibert and Hodgins to demonstrate further the value of the embodiment paradigm. These robots were essentially vertical cylinders with a single hopping foot. The challenge of managing the robot s behavior can be daunting because in addition to the intricacies of the program itself, there were also the mechanical matters regarding how the foot ought to be constructed so that it could hop. An embodied approach makes it easier to see that in order for this robot to function, it must be able to exploit its system to the fullest. That is, the robot s systems should be seen as having dynamic characteristics as opposed to the traditional view that it is merely a command center that just executes actions. Clark distinguishes between two kinds of vision, animate and pure vision. Pure vision is an idea that is typically associated with classical artificial intelligence, in which vision is used to create a rich world model so that thought and reason can be used to fully explore the inner model. In other words, pure vision passively creates the external perceivable world so that the faculties of reason can be better used introspectively. Animate vision, by contrast, sees vision as the means by which real time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark s example of going to the drugstore to buy some Kodak film. In one s mind, one is familiar with the Kodak logo and its trademark gold color. Thus, one uses incoming visual stimuli to navigate around the drugstore until one finds the film. Therefore, vision should not be seen as a passive system but rather an active retrieval device that intelligently uses sensory information and local environmental cues to perform specific real world actions. Inspired by the work of the American psychologist James J. Gibson, this next example emphasizes the importance of action relevant sensory information, bodily movement, and local environment cues. These three concepts are unified by the concept of affordances, which are possibilities of action provided by the physical world to a given agent. These are in turn determined by the agent s physical body, capacities, and the overall action related properties of the local environment as well. Clark uses the example of an outfielder in baseball to better illustrate the concept of affordance. Traditional computational models would claim that an outfielder attempting to catch a fly ball can be calculated by variables such as the running speed of the outfielder and the arc of the baseball. However, Gibson s work shows that a simpler method is possible. The outfielder can catch the ball so long as they adjust their running speed so that the ball continually moves in a straight line in their field of vision. Note that this strategy uses various affordances that are contingent upon the success of the outfielder, including their physical body composition, the environment of the baseball field, and the sensory information obtained by the outfielder.  Clark points out here that the latter strategy of catching the ball as opposed to the former has significant implications for perception. The affordance approach proves to be non linear because it relies upon spontaneous real time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily.  346  This is because they utilize environmental cues made possible by perceptual information that is actively used in the real time by the agent. In the formation of general principles of intelligent behavior, Pfeifer intended to be contrary to older principles given in traditional artificial intelligence.  The most dramatic difference is that the principles are applicable only to situated robotic agents in the real world, a domain where traditional artificial intelligence showed the least promise. Principle of cheap design and redundancy  Pfeifer realized that implicit assumptions made by engineers often substantially influence a control architecture s complexity.  436   This insight is reflected in discussions of the scalability problem in robotics.  The internal processing needed for some bad architectures can grow out of proportion to new tasks needed of an agent.   One of the primary reasons for scalability problems is that the amount of programming and knowledge engineering that the robot designers have to perform grows very rapidly with the complexity of the robot s tasks. There is mounting evidence that pre programming cannot be the solution to the scalability problem ... The problem is that programmers introduce too many hidden assumptions in the robot s code.The proposed solutions are to have the agent exploit the inherent physics of its environment, to exploit the constraints of its niche, and to have agent morphology based on parsimony and the principle of Redundancy. Redundancy reflects the desire for the error correction of signals afforded by duplicating like channels.  Additionally, it reflects the desire to exploit the associations between sensory modalities.  See redundant modalities .  In terms of design, this implies that redundancy should be introduced with respect not only to one sensory modality but to several.  448   It has been suggested that the fusion and transfer of knowledge between modalities can be the basis of reducing the size of the sense data taken from the real world.  This again addresses the scalability problem. Principle of parallel, loosely coupled processes  An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense Think Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem. Principle of sensory motor coordination  Ideally, internal mechanisms in an agent should give rise to things like memory and choice making in an emergent fashion, rather than being prescriptively programmed from the beginning.   These kinds of things are allowed to emerge as the agent interacts with the environment. The motto is, build fewer assumptions into the agent s controller now, so that learning can be more robust and idiosyncratic in the future.  Principle of ecological balance  This is more a theory than a principle, but its implications are widespread.  Its claim is that the internal processing of an agent cannot be made more complex unless there is a corresponding increase in complexity of the motors, limbs, and sensors of the agent. In other words, the extra complexity added to the brain of a simple robot will not create any discernible change in its behavior. The robot s morphology must already contain the complexity in itself to allow enough  breathing room  for more internal processing to develop. Value principle  This was the architecture developed in the Darwin III robot of Gerald Edelman. It relies heavily on connectionism. A traditionalist may argue that objects may be used to aid in cognitive processes, but this does not mean they are part of a cognitive system.  343  Eyeglasses are used to aid in the visual process, but to say they are a part of a larger system would completely redefine what is meant by a visual system. However, supporters of the embodied approach could make the case that if objects in the environment play the functional role of mental states, then the items themselves should not be counted among the mental states. Lars Ludwig explores mind extension further outlining its role in technology. He proposes a cognitive theory of  extended artificial memory , which represents a theoretical update and extension of the  memory theories of Richard Semon.  
Empowerment in the field of artificial intelligence  formalises and quantifies  via information theory  the potential an agent perceives that it has to influence its environment. An agent which follows an empowerment maximising policy, acts to maximise future options  typically up to some limited horizon . Empowerment can be used as a  pseudo  utility function that depends only on information gathered from the local environment to guide action, rather than seeking an externally imposed goal, thus is a form of intrinsic motivation. The empowerment formalism depends on a probabilistic model commonly used in artificial intelligence. An autonomous agent operates in the world by taking in sensory information and acting to change its state, or that of the environment, in a cycle of perceiving and acting known as the perception action loop. Agent state and actions are modelled by random variables      S   s     S   , A   a     A      ,A a in       and time      t      . The choice of action depends on the current state, and the future state depends on the choice of action, thus the perception action loop unrolled in time forms a causal bayesian network. Empowerment        E           is defined as the channel capacity      C       of the actuation channel of the agent, and is formalised as the maximal possible information flow between the actions of the agent and the effect of those actions some time later. Empowerment can be thought of as the future potential of the agent to affect its environment, as measured by its sensors. In a discrete time model, Empowerment can be computed for a given number of cycles into the future, which is referred to in the literature as  n step  empowerment. The unit of empowerment depends on the logarithm base. Base 2 is commonly used in which case the unit is bits. In general the choice of action  action distribution  that maximises empowerment varies from state to state. Knowing the empowerment of an agent in a specific state is useful, for example to construct an empowerment maximising policy. State specific empowerment can be found using the more general formalism for  contextual empowerment .     C      is a random variable describing the context  e.g. state . Empowerment maximisation can be used as a pseudo utility function to enable agents to exhibit intelligent behaviour without requiring the definition of external goals, for example balancing a pole in a cart pole balancing scenario where no indication of the task is provided to the agent.  Empowerment has been applied in studies of collective behaviour and in continuous domains. As is the case with Bayesian methods in general, computation of empowerment becomes computationally expensive as the number of actions and time horizon extends, but approaches to improve efficiency have led to usage in real time control. Empowerment has been used for intrinsically motivated reinforcement learning agents playing video games, and in the control of underwater vehicles. 
In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models  average out.  Ensemble averaging is one of the simplest types of committee machines. Along with boosting, it is one of the two major types of static committee machines. In contrast to standard network design in which many networks are generated but only one is kept, ensemble averaging keeps the less satisfactory networks around, but with less weight. The theory of ensemble averaging relies on two properties of artificial neural networks  Ensemble averaging creates a group of networks, each with low bias and high variance, then combines them to a new network with  hopefully  low bias and low variance. It is thus a resolution of the bias variance dilemma. The idea of combining experts has been traced back to Pierre Simon Laplace. The theory mentioned above gives an obvious strategy  create a set of experts with low bias and high variance, and then average them. Generally, what this means is to create a set of experts with varying parameters  frequently, these are the initial synaptic weights, although other factors  such as the learning rate, momentum etc.  may be varied as well. Some authors recommend against varying weight decay and early stopping. The steps are therefore  Alternatively, domain knowledge may be used to generate several classes of experts. An expert from each class is trained, and then combined. A more complex version of ensemble average views the final result not as a mere average of all the experts, but rather as a weighted sum. If each expert is      y  i        , then the overall result        y             can be defined as  where                is a set of weights. The optimization problem of finding alpha is readily solved through neural networks, hence a  meta network  where each  neuron  is in fact an entire neural network can be trained, and the synaptic weights of the final network is the weight applied to each expert. This is known as a linear combination of experts. It can be seen that most forms of neural networks are some subset of a linear combination  the standard neural net  where only one expert is used  is simply a linear combination with all         j     0    0    and one         k     1    1   . A raw average is where all         j         are equal to some constant value, namely one over the total number of experts. A more recent ensemble averaging method is negative correlation learning, proposed by Y. Liu and X. Yao. Now this method has been widely used in evolutionary computing. 
Enterprise cognitive systems  ECS  are part of a broader shift in computing, from a programmatic to a probabilistic approach, called cognitive computing. An Enterprise Cognitive System makes a new class of complex decision support problems computable, where the business context is ambiguous, multi faceted, and fast evolving, and what to do in such a situation is usually assessed today by the business user. An ECS is designed to synthesize a business context and link it to the desired outcome. It recommends evidence based actions to help the end user achieve the desired outcome. It does so by finding past situations similar to the current situation, and extracting the repeated actions that best influence the desired outcome. While general purpose cognitive systems can be used for different outputs, prescriptive, suggestive, instructive, or simply entertaining, an enterprise cognitive system is focused on action, not insight, to help in assessing what to do in a complex situation. ECS have to be  
Core concepts Distinctions Schools of thought Topics and views Specialized domains of inquiry Notable epistemologists Related fields Epistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge.  While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics and linguistics.  While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Avicenna, Ockham, and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912.  It continued to mature as a field, reaching its modern form in 1963 with the work of Kripke. Many papers were written in the 1950s that spoke of a logic of knowledge in passing, but the Finnish philosopher G. H. von Wright s 1951 paper titled An Essay in Modal Logic is seen as a founding document.  It was not until 1962 that another Finn, Hintikka, would write Knowledge and Belief, the first book length work to suggest using modalities to capture the semantics of knowledge rather than the alethic statements typically discussed in modal logic.  This work laid much of the groundwork for the subject, but a great deal of research has taken place since that time.  For example, epistemic logic has been combined recently with some ideas from dynamic logic to create dynamic epistemic logic, which can be used to specify and reason about information change and exchange of information in multi agent systems. The seminal works in this field are by Plaza, Van Benthem, and Baltag, Moss, and Solecki. Most attempts at modeling knowledge have been based on the possible worlds model.  In order to do this, we must divide the set of possible worlds between those that are compatible with an agent s knowledge, and those that are not. This generally conforms with common usage. If I know that it is either Friday or Saturday, then I know for sure that it is not Thursday. There is no possible world compatible with my knowledge where it is Thursday, since in all these worlds it is either Friday or Saturday. While we will primarily be discussing the logic based approach to accomplishing this task, it is worthwhile to mention here the other primary method in use, the event based approach.  In this particular usage, events are sets of possible worlds, and knowledge is an operator on events.  Though the strategies are closely related, there are two important distinctions to be made between them  Typically, the logic based approach has been used in fields such as philosophy, logic and AI, while the event based approach is more often used in fields such as game theory and mathematical economics.  In the logic based approach, a syntax and semantics have been built using the language of modal logic, which we will now describe. The basic modal operator of epistemic logic, usually written K, can be read as  it is known that,   it is epistemically necessary that,  or  it is inconsistent with what is known that not.   If there is more than one agent whose knowledge is to be represented, subscripts can be attached to the operator         K    1          ,        K    2          , etc.  to indicate which agent one is talking about.  So        K    a          varphi     can be read as  Agent     a      knows that           .   Thus, epistemic logic can be an example of multimodal logic applied for knowledge representation. The dual of K, which would be in the same relationship to K as            is to           , has no specific symbol, but can be represented by        K  a          neg  varphi    , which can be read as      a      does not know that not             or  It is consistent with     a      s knowledge that            is possible . The statement      a      does not know whether or not             can be expressed as        K  a          K  a          varphi  land  neg K  neg  varphi    . In order to accommodate notions of common knowledge and distributed knowledge, three other modal operators can be added to the language.  These are        E     G            , which reads  every agent in group G knows   mutual knowledge          C     G            , which reads  it is common knowledge to every agent in G   and        D     G            , which reads  it is distributed knowledge to the whole group G.   If            is a formula of our language, then so are        E    G          varphi    ,        C    G          varphi    , and        D    G          varphi    .  Just as the subscript after       K          can be omitted when there is only one agent, the subscript after the modal operators       E         ,       C         , and       D          can be omitted when the group is the set of all agents. As we mentioned above, the logic based approach is built upon the possible worlds model, the semantics of which are often given definite form in Kripke structures, also known as Kripke models.  A Kripke structure M for n agents over            is an  n   2  tuple       S ,   ,    K    1   , . . . ,    K    n         ,...,      , where S is a nonempty set of states or possible worlds,            is an interpretation, which associates with each state in S a truth assignment to the primitive propositions in             the set of all primitive propositions , and        K    1   , . . . ,    K    n       ,...,      are binary relations on S for n numbers of agents.  It is important here not to confuse      K  i        , our modal operator, and        K    i          , our accessibility relation. The truth assignment tells us whether or not a proposition p is true or false in a certain state.  So         s     p        tells us whether p is true in state s in model       M         .  Truth depends not only on the structure, but on the current world as well.  Just because something is true in one world does not mean it is true in another.  To state that a formula            is true at a certain world, one writes       M , s           , normally read as             is true at  M,s ,  or   M,s  satisfies            . It is useful to think of our binary relation        K    i           as a possibility relation, because it is meant to capture what worlds or states agent i considers to be possible  In other words,     w    K    i   v     v    if and only if             w    K  i             v             , and such     v      s are called epistemic alternatives for agent i. In idealized accounts of knowledge  e.g., describing the epistemic status of perfect reasoners with infinite memory capacity , it makes sense for        K    i           to be an equivalence relation, since this is the strongest form and is the most appropriate for the greatest number of applications.  An equivalence relation is a binary relation that is reflexive, symmetric, and transitive.  The accessibility relation does not have to have these qualities  there are certainly other choices possible, such as those used when modeling belief rather than knowledge. Assuming that        K    i           is an equivalence relation, and that the agents are perfect reasoners, a few properties of knowledge can be derived.  The properties listed here are often known as the  S5 Properties,  for reasons described in the Axiom Systems section below. This axiom is traditionally known as K.  In epistemic terms, it states that if an agent knows            and knows that                 , then the agent must also know            .  So, This axiom is valid on any frame in relational semantics. This axiom logically establishes modus ponens as a rule of inference for every epistemically possible world. Another property we can derive is that if            is valid  i.e. a tautology , then      K  i        phi    .  This does not mean that if            is true, then agent i knows           .  What it means is that if            is true in every world that an agent considers to be a possible world, then the agent must know            at every possible world. This principle is traditionally called N  Necessitation rule .  This rule always preserves truth in relational semantics. This axiom is also known as T.  It says that if an agent knows facts, the facts must be true.  This has often been taken as the major distinguishing feature between knowledge and belief.  We can believe a statement to be true when it is false, but it would be impossible to know a false statement. This axiom can also be expressed in its contraposition as agents cannot know a false statement  This axiom is valid on any reflexive frame. This property and the next state that an agent has introspection about its own knowledge, and are traditionally known as 4 and 5, respectively.  The Positive Introspection Axiom, also known as the KK Axiom, says specifically that agents know that they know what they know.  This axiom may seem less obvious than the ones listed previously, and Timothy Williamson has argued against its inclusion forcefully in his book, Knowledge and Its Limits. Equivalently, this modal axiom 4 says that agents do not know what they do not know that they know This axiom is valid on any transitive frame. The Negative Introspection Axiom says that agents know that they do not know what they do not know. Or, equivalently, this modal axiom 5 says that agents know what they do not know that they do not know This axiom is valid on any Euclidean frame. Different modal logics can be derived from taking different subsets of these axioms, and these logics are normally named after the important axioms being employed.  However, this is not always the case.  KT45, the modal logic that results from the combining of K, T, 4, 5, and the Knowledge Generalization Rule, is primarily known as S5.  This is why the properties of knowledge described above are often called the S5 Properties. However, it can be proven that modal axiom B is a theorem in S5  viz.     S 5    B         , which says that what an agent does not know that they do not know is true         K  i      K  i              neg K  varphi  implies  varphi    . The modal axiom B is true on any symmetric frame, but is very counterintuitive in epistemic logic  How can the ignorance on one s own ignorance imply truth? It is therefore debatable whether S4 describes epistemic logic better, rather than S5. Epistemic logic also deals with belief, not just knowledge. The basic modal operator is usually written B instead of K. In this case, though, the knowledge axiom no longer seems right agents only sometimes believe the truth so it is usually replaced with the Consistency Axiom, traditionally called D  which states that the agent does not believe a contradiction, or that which is false.  When D replaces T in S5, the resulting system is known as KD45.  This results in different properties for        K    i           as well.  For example, in a system where an agent  believes  something to be true, but it is not actually true, the accessibility relation would be non reflexive.  The logic of belief is called doxastic logic. When there are multiple agents in the domain of discourse where each agent i corresponds to a separate epistemic modal operator      K  i        , in addition to the axiom schemata for each individual agent listed above to describe the rationality of each agent, it is usually also assumed that the rationality of each agent is common knowledge. If we take the possible worlds approach to knowledge, it follows that our epistemic agent a knows all the logical consequences of their beliefs  known as logical omniscience . If     Q      is a logical consequence of     P     , then there is no possible world where     P      is true but     Q      is not. So if a knows that     P      is true, it follows that all of the logical consequences of     P      are true of all of the possible worlds compatible with a s beliefs. Therefore, a knows     Q     . It is not epistemically possible for a that not     Q      given his knowledge that     P     . This consideration was a part of what led Robert Stalnaker to develop two dimensionalism, which can arguably explain how we might not know all the logical consequences of our beliefs even if there are no worlds where the propositions we know come out true but their consequences false. Even when we ignore possible world semantics and stick to axiomatic systems, this peculiar feature holds. With K and N  the Distribution Rule and the Knowledge Generalization Rule, respectively , which are axioms that are minimally true of all normal modal logics, we can prove that we know all the logical consequences of our beliefs. If     Q      is a logical consequence of     P       i.e. we have the tautology         P   Q        , then we can derive      K  a     P   Q      P rightarrow Q     with N, and using a conditional proof with the axiom K, we can then derive      K  a   P    K  a   Q   P rightarrow K Q    with K. When we translate this into epistemic terms, this says that if     Q      is a logical consequence of     P     , then a knows that it is, and if a knows     P     , a knows     Q     . That is to say, a knows all the logical consequences of every proposition. This is necessarily true of all classical modal logics. But then, for example, if a knows that prime numbers are divisible only by themselves and the number one, then a knows that 8683317618811886495518194401279999999 is prime  since this number is only divisible by itself and the number one . That is to say, under the modal interpretation of knowledge, when a knows the definition of a prime number, a knows that this number is prime. This generalizes to any provable theorem in any axiomatic theory  i.e. if a knows all the axioms in a theory, then a knows all the provable theorems in that theory . It should be clear at this point that a is not human  otherwise there would not be any unsolved conjectures in mathematics, like P versus NP problem or Goldbach s conjecture . This shows that epistemic modal logic is an idealized account of knowledge, and explains objective, rather than subjective knowledge  if anything . In philosophical logic, the masked man fallacy  also known as the intensional fallacy or epistemic fallacy  is committed when one makes an illicit use of Leibniz s law in an argument. The fallacy is  epistemic  because it posits an immediate identity between a subject s knowledge of an object with the object itself, failing to recognize that Leibniz s Law is not capable of accounting for intensional contexts. The name of the fallacy comes from the example  The premises may be true and the conclusion false if Bob is the masked man and the speaker does not know that. Thus the argument is a fallacious one. In symbolic form, the above arguments are Note, however, that this syllogism happens in the reasoning by the speaker  I   Therefore, in the formal modal logic form, it ll be Premise 1        B    s     t   t   X    K  s     t   X          forall t t X rightarrow K  t X      is a very strong one, as it is logically equivalent to        B  s       t      K  s     t   X     t   X        forall t  neg K  t X  rightarrow t not  X    . It is very likely that this is a false belief        t      K  s     t   X     t   X      t X  rightarrow t not  X     is likely a false proposition, as the ignorance on the proposition     t   X      does not imply the negation of it is true. Another example  Expressed in doxastic logic, the above syllogism is  The above reasoning is invalid  not truth preserving . The valid conclusion to be drawn is        B    Lois      Superman     Clark            neq      . 
Evolutionary developmental robotics  evo devo robo for short  refers to methodologies that systematically integrate evolutionary robotics, epigenetic robotics and morphogenetic robotics to study the evolution, physical and mental development and learning of natural intelligent systems in robotic systems. The field was formally suggested and fully discussed in a published paper and further discussed in a published dialogue. The theoretical foundation of evo devo robo includes evolutionary developmental biology  evo devo , evolutionary developmental psychology, developmental cognitive neuroscience etc. Further discussions on evolution, development and learning in robotics and design can be found in a number of papers, including papers on hardware systems and computing tissues. 
Explainable AI  XAI , also known as Interpretable AI, or Explainable Machine Learning  XML , is artificial intelligence  AI  in which humans can understand the reasoning behind decisions or predictions made by the AI. It contrasts with the  black box  concept in machine learning, where even the AI s designers cannot explain why it arrived at a specific decision. XAI hopes to help users of AI powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions. Machine learning  ML  algorithms used in AI can be categorized as white box or black box. White box models provide results that are understandable to experts in the domain. Black box models, on the other hand, are extremely hard to explain and can hardly be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability. A model is transparent  if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.  Interpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision making in a way that is understandable to humans. Explainability is a concept that is recognized as important, but a consensus definition is not available. One possibility is  the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision  e.g., classification or regression  . If algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts. Sometimes it is also possible to achieve a high accuracy result with a white box ML algorithm that is interpretable in itself. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset. AI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command  maximize accuracy of assessing how positive film reviews are in the test dataset.  The AI may learn useful general rules from the test set, such as  reviews containing the word  horrible  are likely to be negative.  However, it may also learn inappropriate rules, such as  reviews containing   Daniel Day Lewis  are usually positive   such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be  cheating  or  unfair.  A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real world data outside the test set. Cooperation between agents   in this case, algorithms and humans   depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria. This is particularly relevant in medicine, especially with clinical decision support systems  CDSS , in which medical professionals should be able to understand how and why a machine based decision was made in order to trust the decision and augment their decision making process. AI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to  cheat  by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured. In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object. One transparency project, the DARPA XAI program, aims to produce  glass box  models that are explainable to a  human in the loop  without greatly sacrificing AI performance. Human users of such a system can understand the AI s cognition  both in real time and after the fact  and can determine whether to trust the AI. Other applications of XAI are knowledge extraction from black box models and model comparisons. In the context of monitoring systems for ethical and socio legal compliance, the term  glass box  is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision making processes are transparent and accountable. The term  glass box  is often used in contrast to  black box  systems, which lack transparency and can be more difficult to monitor and regulate. The term is also used to name a voice assistant that produces counterfactual statements as explanations. During the 1970s to 1990s, symbolic reasoning systems, such as MYCIN, GUIDON, SOPHIE, and PROTOS could represent, reason about, and explain their reasoning for diagnostic, instructional, or machine learning  explanation based learning  purposes. MYCIN, developed in the early 1970s as a research prototype for diagnosing bacteremia infections of the bloodstream, could explain which of its hand coded rules contributed to a diagnosis in a specific case. Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an  articulate expert , explaining problem solving strategy at a level the student could understand, so they would know what action to take next. For instance, SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting, even though it ultimately relied on the SPICE circuit simulator. Similarly, GUIDON added tutorial rules to supplement MYCIN s domain level rules so it could explain strategy for medical diagnosis. Symbolic approaches to machine learning, especially those relying on explanation based learning, such as PROTOS, explicitly relied on representations of explanations, both to explain their actions and to acquire new knowledge. In the 1980s through early 1990s, truth maintenance systems  TMS  extended the capabilities of causal reasoning, rule based, and logic based inference systems.  360 362  A TMS explicitly tracks alternate lines of reasoning, justifications for conclusions, and lines of reasoning that lead to contradictions, allowing future reasoning to avoid these dead ends. To provide explanation, they trace reasoning from conclusions to assumptions through rule operations or logical inferences, allowing explanations to be generated from the reasoning traces. As an example, consider a rule based problem solver with just a few rules about Socrates that concludes he has died from poison  By just tracing through the dependency structure the problem solver can construct the following explanation   Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding conservative dissident beliefs under conservative governments must drink poison.   164 165 By the 1990s researchers began studying whether it is possible to meaningfully extract the non hand coded rules being generated by opaque trained neural networks. Researchers in clinical expert systems creating neural network powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice. In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence. As a result, many academics and organizations are developing tools to help detect bias in their systems. Marvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI  Humanistic Intelligence  as a way to create a more fair and balanced  human in the loop  AI. Modern complex AI techniques, such as deep learning and genetic algorithms, are naturally opaque. To address this issue, methods have been developed to make new models more explainable and interpretable. This includes layerwise relevance propagation  LRP , a technique for determining which features in a particular input vector contribute most strongly to a neural network s output. Other techniques explain some particular prediction made by a  nonlinear  black box model, a goal referred to as  local interpretability . The mere transposition of the concepts of local interpretability into a remote context  where the black box model is executed at a third party  is currently under scrutiny. There has been work on making glass box models which are more transparent to inspection. This includes decision trees, Bayesian networks, sparse linear models, and more. The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency  ACM FAccT  was established in 2018 to study transparency and explainability in the context of socio technical systems, many of which include artificial intelligence. Some techniques allow visualisations of the inputs which individual software neurons respond to most strongly. Several groups found that neurons can be aggregated into circuits that perform human comprehensible functions, some of which reliably arise across different networks trained independently. There are various techniques to extract compressed representations of the features of given inputs, which can then be analysed by standard clustering techniques. Alternatively, networks can be trained to output linguistic explanations of their behaviour, which are then directly human interpretable. Model behaviour can also be explained with reference to training data for example, by evaluating which training inputs influenced a given behaviour the most. As regulators, official bodies, and general users come to depend on AI based dynamic systems, clearer accountability will be required for automated decision making processes to ensure trust and transparency. The first global conference exclusively dedicated to this emerging discipline was the 2017 International Joint Conference on Artificial Intelligence  Workshop on Explainable Artificial Intelligence  XAI . The European Union introduced a right to explanation in the General Data Protection Right  GDPR  to address potential problems stemming from the rising importance of algorithms. The implementation of the regulation began in 2018. However, the right to explanation in GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions. In France the Loi pour une R publique num rique  Digital Republic Act  grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them. Despite efforts to increase the explainability of AI models, they still have a number of limitations. By making an AI system more explainable, we also reveal more of its inner workings. For example, the explainability method of feature importance identifies features or variables that are most important in determining the model s output, while the influential samples method identifies the training samples that are most influential in determining the output, given a particular input. Adversarial parties could take advantage of this knowledge. For example, competitor firms could replicate aspects of the original AI system in their own product, thus reducing competitive advantage. An explainable AI system is also susceptible to being  gamed  influenced in a way that undermines its intended purpose. One study gives the example of a predictive policing system  in this case, those who could potentially  game  the system are the criminals subject to the system s decisions. In this study, developers of the system discussed the issue of criminal gangs looking to illegally obtain passports, and they expressed concerns that, if given an idea of what factors might trigger an alert in the passport application process, those gangs would be able to  send guinea pigs  to test those triggers, eventually finding a loophole that would allow them to  reliably get passports from under the noses of the authorities . A fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical ones, geared toward machine learning engineers for debugging purposes, rather than toward the end users who are ultimately affected by the system, causing  a gap between explainability in practice and the goal of transparency . Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users, or providing explanations in layperson terms. The solution must avoid oversimplification. It is important to strike a balance between accuracy   how faithfully the explanation reflects the process of the AI system   and explainability   how well end users understand the process. This is a difficult balance to strike, since the complexity of machine learning makes it difficult for even ML engineers to fully understand, let alone non experts. The goal of explainability to end users of AI systems is to increase trust in the systems, even  address concerns about lack of  fairness  and discriminatory effects . However, even with a good understanding of an AI system, end users may not necessarily trust the system. In one study, participants were presented with combinations of white box and black box explanations, and static and interactive explanations of AI systems. While these explanations served to increase both their self reported and objective understanding, it had no impact on their level of trust, which remained skeptical. This outcome was especially true for decisions that impacted the end user in a significant way, such as graduate school admissions. Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision makers  instead of rigidly adhering to a set of rules, humans are able to consider exceptional cases as well as appeals to their initial decision. For such decisions, explainability will not necessarily cause end users to accept the use of decision making algorithms. We will need to either turn to another method to increase trust and acceptance of decision making algorithms, or question the need to rely solely on AI for such impactful decisions in the first place. Scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness, and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly. Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators. Moreover, XAI systems have primarily focused on making AI systems understandable to AI practitioners rather than end users, and their results on user perceptions of these systems have been somewhat fragmented. Some researchers advocate the use of inherently interpretable machine learning models, rather than using post hoc explanations in which a second model is created to explain the first. This is partly because post hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post hoc explanation can mimic the computations of an entirely separate model. However, another view is that what is important is that the explanation accomplishes the given task at hand, and whether it is pre or post hoc doesn t matter. If a post hoc explanation method helps a doctor diagnose cancer better, it is of secondary importance whether it is a correct incorrect explanation. The goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters. Along with other factors this leads to a theoretical limit for explainability. 
Extremal optimization  EO  is an optimization heuristic inspired by the Bak Sneppen model of self organized criticality from the field of statistical physics. This heuristic was designed initially to address combinatorial optimization problems such as the travelling salesman problem and spin glasses, although the technique has been demonstrated to function in optimization domains. Self organized criticality  SOC  is a statistical physics concept to describe a class of dynamical systems that have a critical point as an attractor. Specifically, these are non equilibrium systems that evolve through avalanches of change and dissipations that reach up to the highest scales of the system. SOC is said to govern the dynamics behind some natural systems that have these burst like phenomena including landscape formation, earthquakes, evolution, and the granular dynamics of rice and sand piles. Of special interest here is the Bak Sneppen model of SOC, which is able to describe evolution via punctuated equilibrium  extinction events    thus modelling evolution as a self organised critical process. Another piece in the puzzle is work on computational complexity, specifically that critical points have been shown to exist in NP complete problems, where near optimum solutions are widely dispersed and separated by barriers in the search space causing local search algorithms to get stuck or severely hampered. It was the evolutionary self organised criticality model by Bak and Sneppen and the observation of critical points in combinatorial optimisation problems that lead to the development of Extremal Optimization by Stefan Boettcher and Allon Percus. EO was designed as a  local search  algorithm for combinatorial optimization problems. Unlike genetic algorithms, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure   fitness  . This differs from holistic approaches such as ant colony optimization and evolutionary computation that assign equal fitness to all components of a solution based upon their collective evaluation against an objective function. The algorithm is initialized with an initial solution, which can be constructed randomly, or derived from another search process. The technique is a fine grained search, and superficially resembles a hill climbing  local search  technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population based approaches  evolutionary computation and artificial immune system . The governing principle behind this algorithm is that of improvement through selectively removing low quality components and replacing them with a randomly selected component. This is obviously at odds with genetic algorithms, the quintessential evolutionary computation algorithm that selects good solutions in an attempt to make better solutions. The resulting dynamics of this simple principle is firstly a robust hill climbing search behaviour, and secondly a diversity mechanism that resembles that of multiple restart search. Graphing holistic solution quality over time  algorithm iterations  shows periods of improvement followed by quality crashes  avalanche  very much in the manner as described by punctuated equilibrium. It is these crashes or dramatic jumps in the search space that permit the algorithm to escape local optima and differentiate this approach from other local search procedures. Although such punctuated equilibrium behaviour can be  designed  or  hard coded , it should be stressed that this is an emergent effect of the negative component selection principle fundamental to the algorithm. EO has primarily been applied to combinatorial problems such as graph partitioning and the travelling salesman problem, as well as problems from statistical physics such as spin glasses. Generalised extremal optimization  GEO  was developed to operate on bit strings where component quality is determined by the absolute rate of change of the bit, or the bits contribution to holistic solution quality. This work includes application to standard function optimisation problems as well as engineering problem domains. Another similar extension to EO is Continuous Extremal Optimization  CEO . EO has been applied to image rasterization as well as used as a local search after using ant colony optimization. EO has been used to identify structures in complex networks. EO has been used on a multiple target tracking problem. Finally, some work has been done on investigating the probability distribution used to control selection. 
In machine learning, fine tuning is an approach to transfer learning in which the weights of a pre trained model are trained on new data. Fine tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine tuned are  frozen   not updated during the backpropagation step . For some architectures, such as convolutional neural networks, it is common to keep the earlier layers  those closest to the input layer  frozen because they capture lower level features, while later layers often discern high level features that can be more related to the task that the model is trained on. Fine tuning is common in natural language processing  NLP , especially in the domain of language modeling. Large language models like OpenAI s GPT 2 can be fine tuned on downstream NLP tasks to produce better results than the pre trained model can normally achieve. Models that are pre trained on large and general corpora are usually fine tuned by reusing the model s parameters as a starting point and adding a task specific layer trained from scratch. Fine tuning the full model is common as well and often yields better results, but it is more computationally expensive. Full fine tuning is also more prone to overfitting and may cause the model to perform worse on data outside of the distribution of training data used during finetuning. Fine tuning is typically accomplished with supervised learning, but there are also techniques to fine tune a model using weak supervision. Reinforcement learning is also used to fine tune language models like ChatGPT  a fine tuned version of GPT 3  and Sparrow by means of reinforcement learning from human feedback. Low rank adaption  LoRA  is a technique for efficiently finetuning models. The basic idea is to design a low rank matrix that is then added to the original matrix.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Frames are an artificial intelligence data structure used to divide knowledge into substructures by representing  stereotyped situations . They were proposed by Marvin Minsky in his 1974 article  A Framework for Representing Knowledge . Frames are the primary data structure used in artificial intelligence frame languages  they are stored as ontologies of sets. Frames are also an extensive part of knowledge representation and reasoning schemes. They were originally derived from semantic networks and are therefore part of structure based knowledge representations. According to Russell and Norvig s  Artificial Intelligence  A Modern Approach , structural representations assemble  facts about particular objects and event types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy . The frame contains information on how to use the frame, what to expect next, and what to do when these expectations are not met. Some information in the frame is generally unchanged while other information, stored in  terminals , usually change. Terminals can be considered as variables. Top level frames carry information, that is always true about the problem in hand, however, terminals do not have to be true. Their value might change with the new information encountered. Different frames may share the same terminals. Each piece of information about a particular frame is held in a slot. The information can contain  A frame s terminals are already filled with default values, which is based on how the human mind works. For example, when a person is told  a boy kicks a ball , most people will visualize a particular ball  such as a familiar soccer ball  rather than imagining some abstract ball with no attributes. One particular strength of frame based knowledge representations is that, unlike semantic networks, they allow for exceptions in particular instances. This gives frames an amount of flexibility that allows representations of real world phenomena to be reflected more accurately. Like semantic networks, frames can be queried using spreading activation. Following the rules of inheritance, any value given to a slot that is inherited by subframes will be updated  IF ADDED  to the corresponding slots in the subframes and any new instances of a particular frame will feature that new value as the default. Because frames are based on structures, it is possible to generate a semantic network given a set of frames even though it lacks explicit arcs. References to Noam Chomsky and his generative grammar of 1950 are generally missing from Minsky s work.  The simplified structures of frames allow for easy analogical reasoning, a much prized feature in any intelligent agent. The procedural attachments provided by frames also allow a degree of flexibility that makes for a more realistic representation and gives a natural affordance for programming applications. Worth noticing here is the easy analogical reasoning  comparison  that can be done between a boy and a monkey just by having similarly named slots. Also notice that Alex, an instance of a boy, inherits default values like  Sex  from the more general parent object Boy, but the boy may also have different instance values in the form of exceptions such as the number of legs. A frame language is a technology used for knowledge representation in artificial intelligence. They are similar to class hierarchies in object oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice, the techniques and capabilities of frame and object oriented languages overlap significantly. A simple example of concepts modeled in a frame language is the Friend of A Friend  FOAF  ontology defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a Person. Example slots are the person s email, home page, phone, etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot knows links each person with other persons. Default values for a person s interests can be inferred by the web of people they are friends of. The earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re used by other researchers. Just as with expert system inference engines, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL. One of the most influential early Frame languages was KL ONE KL ONE spawned several subsequent Frame languages. One of the most widely used successors to KL ONE was the Loom language developed by Robert MacGregor at the Information Sciences Institute. In the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge based systems. These early products were usually developed in Lisp and integrated constructs such as IF THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge base tools was the Knowledge Engineering Environment  KEE  from Intellicorp. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in Lisp on Lisp machine platforms but was eventually ported to PCs and Unix workstations. The research agenda of the Semantic Web spawned a renewed interest in automatic classification and frame languages.  An example is the Web Ontology Language  OWL  standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications  e.g. Google  do today the web can be organized by concepts organized in an ontology. The name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for  OWL  using the Internet today most of the pages retrieved would be on the bird Owl rather than the standard OWL. With a Semantic Web it would be possible to specify the concept  Web Ontology Language  and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise, the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example. In addition to OWL, various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include OIL and DAML.  The Protege Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier. However it ceased to explicitly support frames as of version 3.5  which is maintained for those preferring frame orientation , the version current in 2017 being 5. The justification for moving from explicit frames being that OWL DL is more expressive and  industry standard . Frame languages have a significant overlap with object oriented languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types. The following table illustrates the correlation between standard terminology from the object oriented and frame language communities  The primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object oriented paradigm encapsulation was one of, if not the most, critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information  called  facets  in some languages  again with the same type of constraint information. The other main differentiator between frame and OO languages was multiple inheritance  allowing a frame or class to have two or more superclasses . For frame languages multiple inheritance was a requirement.  This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity. Although the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages  e.g. Loom, KEE  included the ability to define messages on Frames. On the object oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the Object Management Group has standardized specifications for capabilities such as associating test data and constraints with objects  analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom  and for integrating rule engines. Early work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations.  The term Frame was first used by Marvin Minsky as a paradigm to understand visual reasoning and natural language processing. In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things that seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them. The initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant.  These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object oriented modeling and to relations in entity relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work, the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use  triggers   similar to the database concept of triggers  attached to slots. A trigger is simply procedural code that have attached to a slot. The trigger could fire either before and or after a slot value was accessed or modified. As with object classes, Frames were organized in subsumption hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonald s. A specialization  essentially a subclass  of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values  e.g., expected price range  for the specialized frame. Much of the early Frame language research  e.g. Schank and Abelson  had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical. Similarly, in linguistics, Charles J. Fillmore in the mid 1970s started working on his theory of frame semantics, which later would lead to computational resources like FrameNet. Frame semantics was motivated by reflections on human language and human cognition. Researchers such as Ron Brachman on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic.  One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics. This evolution also illustrates a classic divide in AI research known as the  neats vs. scruffies . The  neats  were researchers who placed the most value on mathematical precision and formalism which could be achieved via First Order Logic and Set Theory. The  scruffies  were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans. The most notable of the more formal approaches was the KL ONE language. KL ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL ONE gave these frame languages a new type of automated reasoning capability known as the classifier. The classifier is an engine that analyzes the various declarations in the frame language  the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier. This technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data  i.e., the Internet . The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the Semantic Web. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web. The  neats vs. scruffies  divide also emerged in Semantic Web research, culminating in the creation of the Linking Open Data community their focus was on exposing data on the Web rather than modeling. 
In artificial intelligence, with implications for cognitive science, the frame problem describes an issue with using first order logic  FOL  to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a  block world  with rules about stacking blocks together. In a FOL system, additional axioms are required to make inferences about the environment  for example, that a block cannot change position unless it is physically moved . The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment. John McCarthy and Patrick J. Hayes defined this problem in their 1969 article, Some Philosophical Problems from the Standpoint of Artificial Intelligence.  In this paper, and many that came after, the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment. In philosophy, the frame problem became more broadly construed in connection with the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else  the frame  remains unchanged. The frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two propositions      o p e n         and      o n        . If these conditions can change, they are better represented by two predicates      o p e n    t       t     and      o n    t       t     that depend on time  such predicates are called fluents. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic by the following formulae  The first two formulae represent the initial situation  the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by        l o c k e d    0        o p e n    1       0  implies  mathrm   1    . In practice, one would have a predicate      e x e c u t e o p e n    t       t     for specifying when an action is executed and a rule       t .  e x e c u t e o p e n    t        o p e n    t   1       t  implies  mathrm   t 1     for specifying the effects of actions.  The article on the situation calculus gives more details. While the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions  representing the expected situation  are consistent with the three formulae above, they are not the only ones. Indeed, another set of conditions that is consistent with the three formulae above is  The frame problem is that specifying only which conditions are changed by the actions does not entail that all other conditions are not changed. This problem can be solved by adding the so called  frame axioms , which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1  The frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition. In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms. The solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred  this solution is formalized using the framework of circumscription. The Yale shooting problem, however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, successor state axioms, etc.  they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved. Even after that, however, the term  frame problem  was still used, in part to refer to the same problem but under different settings  e.g., concurrent actions , and in part to refer to the general problem of representing and reasoning with dynamical domains. The following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full  what is presented are simplified versions that are sufficient to explain the full solution. This solution was proposed by Erik Sandewall, who also defined a formal language for the specification of dynamical domains  therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names. The rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be occluded in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as  permission to change   if a condition is occluded, it is relieved from obeying the constraint of inertia. In the simplified example of the door and the light, occlusion can be formalized by two predicates      o c c l u d e o p e n    t       t     and      o c c l u d e o n    t       t    . The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed. In general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case,      o c c l u d e o p e n    1       1     is true, making the antecedent of the fourth formula above false for     t   1       therefore, the constraint that      o p e n    t   1        o p e n    t       t 1  iff  mathrm   t     does not hold for     t   1     . Therefore,      o p e n         can change value, which is also what is enforced by the third formula. In order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by circumscription or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change  for example, executing the action of opening the door when it was already open  in the formalization above  makes the predicate      o c c l u d e o p e n         true and makes      o p e n         true  however,      o p e n         has not changed value, as it was true already. This encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example,      c h a n g e o p e n    t       t     represents the fact that the predicate      o p e n         will change from time     t      to     t   1     . As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa. The third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time     t      if and only if the corresponding change predicate is true at time     t     . To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions. The value of a condition after the execution of an action can be determined by the fact that the condition is true if and only if  A successor state axiom is a formalization in logic of these two facts. For example, if      o p e n d o o r    t       t     and      c l o s e d o o r    t       t     are two conditions used to denote that the action executed at time     t      was to open or close the door, respectively, the running example is encoded as follows. This solution is centered around the value of conditions, rather than the effects of actions. In other words, there is an axiom for every condition, rather than a formula for every action. Preconditions to actions  which are not present in this example  are formalized by other formulae. The successor state axioms are used in the variant to the situation calculus proposed by Ray Reiter. The fluent calculus is a variant of the situation calculus. It solves the frame problem by using first order logic terms, rather than predicates, to represent the states. Converting predicates into terms in first order logic is called reification  the fluent calculus can be seen as a logic in which predicates representing the state of conditions are reified. The difference between a predicate and a term in first order logic is that a term is a representation of an object  possibly a complex object composed of other objects , while a predicate represents a condition that can be true or false when evaluated over a given set of terms. In the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term      o p e n     o n      circ  mathrm     . It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term      o p e n     o n      circ  mathrm      represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g.,      s t a t e     o p e n     o n  , 10        mathrm   circ  mathrm  ,10     means that this is the state at time     10     . The solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula  The action of closing the door, which makes a condition false instead of true, is represented in a slightly different way  This formula works provided that suitable axioms are given about      s t a t e         and           , e.g., a term containing the same condition twice is not a valid state  for example,      s t a t e     o p e n    s    o p e n  , t        mathrm   circ s circ  mathrm  ,t     is always false for every     s      and     t      . The event calculus uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated. The frame problem can be thought of as the problem of formalizing the principle that, by default,  everything is presumed to remain in the state in which it is   Leibniz,  An Introduction to a Secret Encyclop dia , c. 1679 .  This default, sometimes called the commonsense law of inertia, was expressed by Raymond Reiter in default logic   if     R   x        is true in situation     s     , and it can be assumed that     R   x        remains true after executing action     a     , then we can conclude that     R   x        remains true . Steve Hanks and Drew McDermott argued, on the basis of their Yale shooting example, that this solution to the frame problem is unsatisfactory.  Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates. The counterpart of the default logic solution in the language of answer set programming is a rule with strong negation   if     r   X        is true at time     T     , and it can be assumed that     r   X        remains true at time     T   1     , then we can conclude that     r   X        remains true . Separation logic is a formalism for reasoning about computer programs using pre post specifications of the form        p r e c o n d i t i o n       c o d e       p o s t c o n d i t i o n            mathrm           . Separation logic is an extension of Hoare logic oriented to  reasoning about mutable data structures in computer memory and other dynamic resources, and it has a special connective  , pronounced  and separately , to support independent reasoning about disjoint memory regions. Separation logic employs a tight interpretation of pre post specs, which say that the code can only access memory locations guaranteed to exist by the precondition. This leads to the soundness of the most important inference rule of the logic, the frame rule           p r e c o n d i t i o n       c o d e       p o s t c o n d i t i o n         p r e c o n d i t i o n     f r a m e       c o d e       p o s t c o n d i t i o n     f r a m e               mathrm           ast  mathrm       mathrm       ast  mathrm          The frame rule  allows descriptions of arbitrary memory outside the footprint  memory accessed  of the code to be added to a specification  this enables the initial specification to concentrate only on the footprint.  For example, the inference          list     x        c o d e      sortedlist     x         list     x     sortedlist     y        c o d e      sortedlist     x     sortedlist     y            x      mathrm       x      x  ast  operatorname   y      mathrm       x  ast  operatorname   y         captures that code which sorts a list x does not unsort a separate list y, and it does this without mentioning y at all in the initial spec above the line. Automation of the frame rule has led to significant increases in the scalability of automated reasoning techniques for code, eventually deployed industrially to codebases with tens of millions of lines. There appears to be some similarity between the separation logic solution to the frame problem and that of the fluent calculus mentioned above. Action description languages elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action      o p e n d o o r         makes the door open if not locked is expressed by  The semantics of an action description language depends on what the language can express  concurrent actions, delayed effects, etc.  and is usually based on transition systems. Since domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to answer set programming rather than first order logic. 
In computer science a fuzzy agent is a software agent that implements fuzzy logic. This software entity interacts with its environment through an adaptive rule base and can therefore be considered as a type of intelligent agent.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
GOFAI is an acronym for  Good Old Fashioned Artificial Intelligence  invented by the philosopher John Haugeland in his 1985 book Artificial Intelligence  The Very Idea. Technically, GOFAI refers only to a restricted kind of symbolic AI, namely rule based or logical agents. This approach was popular in the 1980s, especially as an approach to implementing expert systems, but symbolic AI has since been extended in many ways to better handle uncertain reasoning and more open ended systems. Some of these extensions include probabilistic reasoning, non monotonic reasoning, multi agent systems, and neuro symbolic systems. Significant contributions of symbolic AI, not encompassed by the GOFAI view, include search algorithms  automated planning and scheduling  constraint based reasoning  the semantic web  ontologies  knowledge graphs  non monotonic logic  circumscription  automated theorem proving  and symbolic mathematics. For a more complete list, see the main article on symbolic AI. Although the term GOFAI encompasses only a small part of symbolic AI,  prominent in the 1980s, contemporary critics of symbolic AI sometimes use GOFAI as a synonym for it. This conflation of terms can lead to conclusions that symbolic AI research ended in the 1980s and avoided machine learning. Since both conclusions are false and important to correct, we address them below. During the Second AI Summer, i.e., the expert systems boom of the 1980s, production rule systems requiring knowledge engineering were used to implement expert systems. Knowledge engineering required working with subject matter experts to model task knowledge as rules. At the time, rules were hand authored by knowledge engineers or the subject matter experts. GOFAI correctly describes this approach. Haugeland and Dreyfus also correctly pointed out various limitations, discussed in later sections. The Second AI Winter occurred after the expert systems and Lisp Machines markets collapsed. Expert systems did not handle uncertainty well, required more resources to build and maintain than expected, and proved brittle outside their intended domains due to a lack of common sense reasoning capabilities. Language specific Lisp machines could become easily replaced by newer workstations with similar performance, obviating their need.  Symbolic AI continued, albeit with reduced funding. It redirected focus to address limitations in handling uncertainty, using statistical AI  and to speed knowledge acquisition, with symbolic approaches to machine learning. Work in semantic networks and knowledge representations led to formalizing ontologies with languages such as RDF and OWL, leading to large ontologies such as YAGO. However, the research and applications of symbolic AI since the Second AI Winter and outside of the production rule approach to expert systems are less well known and now eclipsed by the media focus on deep learning since 2012.   For example, research in symbolic AI machine learning includes decision tree learning, Mitchell s version space learning, Valiant s contributions to PAC learning, statistical relational learning, inductive logic programming, and various interactive learning approaches to incorporate user advice, examples, and explanations as an integral part of the learning process.  Using GOFAI as a synonym for current symbolic AI leads to erroneous conclusions and confusion. Garcez and Lamb provide an example Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML  and relational learning  the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non use of gradient based learning algorithms . Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi agent systems, planning and argumentation, as well as learning.The key points above are that symbolic AI research has long since moved beyond GOFAI, research continues, and GOFAI no longer describes it. Further, there are symbolic learning approaches to machine learning, such as inductive logic programming and statistical relational learning, i.e., it is not just the domain of deep learning. Below, we return to the philosophical critiques leveled against GOFAI, the symbolic AI approach of the 1980s. GOFAI, the rule based approach of 1980s symbolic AI, was attacked by philosophers such as Hubert Dreyfus, his brother Stuart Dreyfus, and philosopher Kenneth Sayre. The essence of what they criticized was described earlier by computer scientist Alan Turing, in his 1950 paper Computing Machinery and Intelligence, when he said that  human behavior is far too complex to be captured by any formal set of rules humans must be using some informal guidelines that   could never be captured in a formal set of rules and thus could never be codified in a computer program.  Turing called this  The Argument from Informality of Behaviour.  Russell and Norvig, describe the GOFAI critique in Artificial Intelligence  A Modern Approach  The position they criticize came to be called  Good Old Fashioned Al,  or GOFAI, a term coined by Haugeland  1985 . GOFAI is supposed to claim that all intelligent behavior can be captured by a system that reasons logically from a set of facts and rules describing the domain. It therefore corresponds to the simplest logical agent described in Chapter 7. Dreyfus is correct in saying that logical agents are vulnerable to the qualification problem. As we saw in Chapter 13, probabilistic reasoning systems are more appropriate for open ended domains. The Dreyfus critique therefore is not addressed against computers per se, but rather against one particular way of programming them. It is reasonable to suppose, however, that a book called What First Order Logical Rule Based Systems Without Learning Can t Do might have had less impact.In other words, GOFAI restricts its view of agents to those controlled by logical rules. In contrast to this view, symbolic AI also includes non monotonic logic, modal logic, probabilistic logics, multi agent systems, symbolic machine learning, and hybrid neuro symbolic architectures.  Symbolic machine learning, i.e., non connectionist machine learning specific to symbolic AI, includes inductive logic programming, statistical relational learning, case based learning, knowledge compilation  chunking , macro operator learning, learning from analogy, and interactive learning from human advice, explanations, and exemplars. Russell and Norvig do not reject all of Dreyfus s arguments, they accept his strongest argument, one that applies to all disembodied AIs, whatever their approach One of Dreyfus s strongest arguments is for situated agents rather than disembodied logical inference engines. An agent whose understanding of  dog  comes only from a limited set of logical sentences such as  Dog x    Mammal x   is at a disadvantage compared to an agent that has watched dogs run, has played fetch with them, and has been licked by one. As philosopher Andy Clark  1998  says,  Biological brains are first and foremost the control systems for biological bodies. Biological bodies move and act in rich real world surroundings  According to Clark, we are  good at frisbee, bad at logic.   The embodied cognition approach claims that it makes no sense to consider the brain separately  cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole  the brain s functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.
GOLOG is a high level logic programming language for the specification and execution of complex actions in dynamical domains. It is based on the situation calculus. It is a first order logical language for reasoning about action and change. GOLOG was developed at the University of Toronto. The concept of situation calculus on which the GOLOG programming language is based was first proposed by John McCarthy in 1963. A GOLOG interpreter automatically maintains a direct characterization of the dynamic world being modeled, on the basis of user supplied axioms about preconditions, effects of actions and the initial state of the world. This allows the application to reason about the condition of the world and consider the impacts of different potential actions before focusing on a specific action. Golog is a logic programming language and is very different from conventional programming languages. A procedural programming language like C defines the execution of statements in advance. The programmer creates a subroutine which consists of statements, and the computer executes each statement in a linear order. In contrast, fifth generation programming languages like Golog are working with an abstract model with which the interpreter can generate the sequence of actions. The source code defines the problem and it is up to the solver to find the next action. This approach can facilitate the management of complex problems from the domain of robotics. A Golog program defines the state space in which the agent is allowed to operate. A path in the symbolic domain is found with state space search. To speed up the process, Golog programs are realized as hierarchical task networks. Apart from the original Golog language, there are some extensions available. The ConGolog language provides concurrency and interrupts. Other dialects like IndiGolog and Readylog were created for real time applications in which sensor readings are updated on the fly. Golog has been used to model the behavior of autonomous agents. In addition to a logic based action formalism for describing the environment and the effects of basic actions, they enable the construction of complex actions using typical programming language constructs. It is also used for applications in high level control of robots and industrial processes, virtual agents, discrete event simulation etc. It can be also used to develop BDI  Belief Desire Intention  style agent systems. In contrast to the Planning Domain Definition Language, Golog supports planning and scripting as well. Planning means that a goal state in the world model is defined, and the solver brings a logical system into this state. Behavior scripting implements reactive procedures, which are running as a computer program. For example, suppose the idea is to authoring a story. The user defines what should be true at the end of the plot. A solver gets started and applies possible actions to the current situation until the goal state is reached. The specification of a goal state and the possible actions are realized in the logical world model. In contrast, a hardwired reactive behavior doesn t need a solver but the action sequence is provided in a scripting language. The Golog interpreter, which is written in Prolog, executes the script and this will bring the story into the goal state. 
In mathematical logic and computer science, Gabbay s separation theorem, named after Dov Gabbay, states that any arbitrary temporal logic formula can be rewritten in a logically equivalent  past   future  form.  I.e. the future becomes what must be satisfied.  This form can be used as execution rules  a MetateM program is a set of such rules.  This mathematical logic related article is a stub. You can help Wikipedia by expanding it.This computer science article is a stub. You can help Wikipedia by expanding it.
 Game theory is the study of mathematical models of strategic interactions among rational agents. It has applications in all fields of social science, as well as in logic, systems science and computer science. The concepts of game theory are used extensively in economics as well. The traditional methods of game theory addressed two person zero sum games, in which each participant s gains or losses are exactly balanced by the losses and gains of other participants. In the 21st century, the advanced game theories apply to a wider range of behavioral relations  it is now an umbrella term for the science of logical decision making in humans, animals, as well as computers. Modern game theory began with the idea of mixed strategy equilibria in two person zero sum game and its proof by John von Neumann. Von Neumann s original proof used the Brouwer fixed point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision making under uncertainty. Therefore, it is evident that game theory has evolved over time with consistent efforts of mathematicians, economists and other academicians. Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. As of 2020, with the Nobel Memorial Prize in Economic Sciences going to game theorists Paul Milgrom and Robert B. Wilson, fifteen game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory. Discussions on the mathematics of games began long before the rise of modern mathematical game theory. Cardano s work on games of chance in Liber de ludo aleae  Book on Games of Chance , which was written around 1564 but published posthumously in 1663, formulated some of the field s basic ideas. In the 1650s, Pascal and Huygens developed the concept of expectation on reasoning about the structure of games of chance, and Huygens published his gambling calculus in De ratiociniis in ludo ale   On Reasoning in Games of Chance  in 1657. In 1713, a letter attributed to Charles Waldegrave analyzed a game called  le Her . He was an active Jacobite and uncle to James Waldegrave, a British diplomat. In this letter, Waldegrave provided a minimax mixed strategy solution to a two person version of the card game le Her, and the problem is now known as Waldegrave problem. In his 1838 Recherches sur les principes math matiques de la th orie des richesses  Researches into the Mathematical Principles of the Theory of Wealth , Antoine Augustin Cournot considered a duopoly and presented a solution that is the Nash equilibrium of the game. In 1913, Ernst Zermelo published  ber eine Anwendung der Mengenlehre auf die Theorie des Schachspiels  On an Application of Set Theory to the Theory of the Game of Chess , which proved that the optimal chess strategy is strictly determined. This paved the way for more general theorems. In 1938, the Danish mathematical economist Frederik Zeuthen proved that the mathematical model had a winning strategy by using Brouwer s fixed point theorem. In his 1938 book Applications aux Jeux de Hasard and earlier notes,  mile Borel proved a minimax theorem for two person zero sum matrix games only when the pay off matrix is symmetric and provided a solution to a non trivial infinite game  known in English as Blotto game . Borel conjectured the non existence of mixed strategy equilibria in finite two person zero sum games, a conjecture that was proved false by von Neumann. Game theory did not exist as a unique field until John von Neumann published the paper On the Theory of Games of Strategy  in 1928. Von Neumann s original proof used Brouwer s fixed point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by his 1944 book Theory of Games and Economic Behavior co authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli s old theory of utility  of money  as an independent discipline. Von Neumann s work in game theory culminated in this 1944 book. This foundational work contains the method for finding mutually consistent solutions for two person zero sum games. Subsequent work focused primarily on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies. In 1950, the first mathematical discussion of the prisoner s dilemma appeared, and an experiment was undertaken by notable mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation s investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy. Around this same time, John Nash developed a criterion for mutual consistency of players  strategies known as the Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n player, non zero sum  not just two player zero sum  non cooperative game has what is now known as a Nash equilibrium in mixed strategies. Game theory experienced a flurry of activity in the 1950s, during which the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. The 1950s also saw the first applications of game theory to philosophy and political science. In 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium. Later he would introduce trembling hand perfection as well. In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory. In the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection, and common knowledge were introduced and analyzed. In 1994, John Nash was awarded the Nobel Memorial Prize in the Economic Sciences for his contribution to game theory. Nash s most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept for non cooperative games. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy.  In 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences. In 2007, Leonid Hurwicz, Eric Maskin, and Roger Myerson were awarded the Nobel Prize in Economics  for having laid the foundations of mechanism design theory . Myerson s contributions include the notion of proper equilibrium, and an important graduate text  Game Theory, Analysis of Conflict. Hurwicz introduced and formalized the concept of incentive compatibility. In 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics  for the theory of stable allocations and the practice of market design . In 2014, the Nobel went to game theorist Jean Tirole. A game is cooperative if the players are able to form binding commitments externally enforced  e.g. through contract law . A game is non cooperative if players cannot form alliances or if all agreements need to be self enforcing  e.g. through credible threats . Cooperative games are often analyzed through the framework of cooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is opposed to the traditional non cooperative game theory which focuses on predicting individual players  actions and payoffs and analyzing Nash equilibria. The focus on individual payoff can result in a phenomenon known as Tragedy of the Commons, where resources are used to a collectively inefficient level. The lack of formal negotiation leads to the deterioration of public goods through over use and under provision that stems from private incentives. Cooperative game theory provides a high level approach as it describes only the structure, strategies, and payoffs of coalitions, whereas non cooperative game theory also looks at how bargaining procedures will affect the distribution of payoffs within each coalition. As non cooperative game theory is more general, cooperative games can be analyzed through the approach of non cooperative game theory  the converse does not hold  provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation. While using a single theory may be desirable, in many instances insufficient information is available to accurately model the formal procedures available during the strategic bargaining process, or the resulting model would be too complex to offer a practical tool in the real world. In such cases, cooperative game theory provides a simplified approach that allows analysis of the game at large without having to make any assumption about bargaining powers. A symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. That is, if the identities of the players can be changed without changing the payoff to the strategies, then a game is symmetric. Many of the commonly studied 2 2 games are symmetric. The standard representations of chicken, the prisoner s dilemma, and the stag hunt are all symmetric games. Some scholars would consider certain asymmetric games as examples of these games as well. However, the most common payoffs for each of these games are symmetric. The most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured in this section s graphic is asymmetric despite having identical strategy sets for both players. Zero sum games  more generally, constant sum games  are games in which choices by players can neither increase nor decrease the available resources. In zero sum games, the total benefit goes to all players in a game, for every combination of strategies, always adds to zero  more informally, a player benefits only at the equal expense of others . Poker exemplifies a zero sum game  ignoring the possibility of the house s cut , because one wins exactly the amount one s opponents lose. Other zero sum games include matching pennies and most classical board games including Go and chess. Many games studied by game theorists  including the famed prisoner s dilemma  are non zero sum games, because the outcome has net results greater or less than zero. Informally, in non zero sum games, a gain by one player does not necessarily correspond with a loss by another. Constant sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any constant sum game into a  possibly asymmetric  zero sum game by adding a dummy player  often called  the board   whose losses compensate the players  net winnings. Simultaneous games are games where both players move simultaneously, or instead the later players are unaware of the earlier players  actions  making them effectively simultaneous . Sequential games  or dynamic games  are games where players do not make decisions simultaneously, and player s earlier actions affect the outcome and decisions of other players. This need not be perfect information about every action of earlier players  it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed. The difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games  see subgame perfection. In short, the differences between sequential and simultaneous games are as follows  An important subset of sequential games consists of games of perfect information. A game with perfect information means that all players, at every move in the game, know the previous history of the game and the moves previously made by all other players. In reality, this can be applied to firms and consumers having information about price and quality of all the available goods in a market. An imperfect information game is played when the players do not know all moves already made by the opponent such as a simultaneous move game.  Most games studied in game theory are imperfect information games. Examples of perfect information games include tic tac toe, checkers, chess, and Go. Many card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept pertaining to the common knowledge of each player s sequence, strategies, and payoffs throughout gameplay. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken, whereas perfect information is knowledge of all aspects of the game and players. Games of incomplete information can be reduced, however, to games of imperfect information by introducing  moves by nature . One of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent s valuation of the object of negotiation, companies may be unaware of their opponent s cost functions, combatants may be unaware of their opponent s strengths, and jurors may be unaware of their colleague s interpretation of the evidence at trial. In some cases, participants may know the character of their opponent well, but may not know how well their opponent knows his or her own character. Bayesian game means a strategic game with incomplete information. For a strategic game, decision makers are players, and every player has a group of actions. A core part of the imperfect information specification is the set of states. Every state completely describes a collection of characteristics relevant to the player such as their preferences and details about them. There must be a state for every set of features that some player believes may exist. For example, where Player 1 is unsure whether Player 2 would rather date her or get away from her, while Player 2 understands Player 1 s preferences as before. To be specific, supposing that Player 1 believes that Player 2 wants to date her under a probability of 1 2 and get away from her under a probability of 1 2  this evaluation comes from Player 1 s experience probably  she faces players who want to date her half of the time in such a case and players who want to avoid her half of the time . Due to the probability involved, the analysis  of this situation requires to understand the player s preference for the draw, even though people are only interested in pure strategic equilibrium. Games in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and Go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve some particular problems and answer some general questions. Games of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic  and sometimes non constructive  proof methods to solve games of certain types, including  loopy  games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional  or  economic   game theory. A typical game that has been solved this way is Hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies. Research in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures  like chess, go, or backgammon  for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice. Games, as studied by economists and real world game players, are generally finished in finitely many moves. Pure mathematicians are not so constrained, and set theorists in particular study games that last for infinitely many moves, with the winner  or other payoff  not known until after all those moves are completed. The focus of attention is usually not so much on the best way to play such a game, but whether one player has a winning strategy.  It can be proven, using the axiom of choice, that there are games   even with perfect information and where the only outcomes are  win  or  lose    for which neither player has a winning strategy.  The existence of such strategies, for cleverly designed games, has important consequences in descriptive set theory. Much of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players  strategies being any non negative quantities, including fractional quantities. Continuous games allow the possibility for players to communicate with each other under certain rules, primarily the enforcement of a communication protocol between the players.  By communicating, players have been noted to be willing to provide a larger amount of goods in a public good game than they ordinarily would in a discrete game, and as a result, the players are able to manage resources more efficiently than they would in Discrete games, as they share resources, ideas and strategies with one another.  This incentivises, and causes, continuous games to have a higher median cooperation rate. Differential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players  state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies  the open loop strategies are found using the Pontryagin maximum principle while the closed loop strategies are found using Bellman s Dynamic Programming method. A particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval. Evolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest. In biology, such models can represent evolution, in which offspring adopt their parents  strategies and parents who play more successful strategies  i.e. corresponding to higher payoffs  have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies. Individual decision problems with stochastic outcomes are sometimes considered  one player games . They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning  with uncertainty  and multi agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes  MDP . Stochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes  chance moves    moves by nature  . This player is not typically considered a third player in what is otherwise a two player game, but merely serves to provide a roll of the dice where required by the game. For some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely  but costly  events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen.  See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.  General models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability  of moves by other players  have also been studied. The  gold standard  is considered to be partially observable stochastic game  POSG , but few realistic problems are computationally feasible in POSG representation. These are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory. The term metagame analysis is also used to refer to a practical approach developed by Nigel Howard, whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis. These are games prevailing over all forms of society. Pooling games are repeated plays with changing payoff table in general over an experienced path, and their equilibrium strategies usually take a form of evolutionary social convention and economic convention. Pooling game theory emerges to formally recognize the interaction between optimal choice in one play and the emergence of forthcoming payoff table update path, identify the invariance existence and robustness, and predict variance over time. The theory is based upon topological transformation classification of payoff table update over time to predict variance and invariance, and is also within the jurisdiction of the computational law of reachable optimality for ordered system. Mean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines, and by mathematicians Pierre Louis Lions and Jean Michel Lasry. The games studied in game theory are well defined mathematical objects. To be fully defined, a game must specify the following elements  the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome.  Eric Rasmusen refers to these four  essential elements  by the acronym  PAPI .  A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game a stable state in which either one outcome occurs or a set of outcomes occur with known probability. In games, players typically have a  Dominant Strategy , where they are incentivised to choose the best possible strategy that gives them the maximum payoff, and stick to it even when the other player s change their strategies or choose a different option.  However, depending on the possible payoffs, one of the players may not possess a  Dominant Strategy , while the other player might.  A player not having a dominant strategy is not a confirmation that another player won t have a dominant strategy of their own, which puts the first player at an immediate disadvantage. However, there is the chance of both player s possessing Dominant Strategies, when their chosen strategies and their payoffs are dominant, and the combined payoffs form an equilibrium.  When this occurs, it creates a Dominant Strategy Equilibrium.  This can cause a Social Dilemma, where a game possesses an equilibrium created by two or multiple players who all have dominant strategies, and the game s solution is different to what the cooperative solution to the game would have been. There is also the chance of a player having more than one dominant strategy.  This occurs when reacting to multiple strategies from a second player, and the first player s separate responses having different strategies to each other.  This means that there is no chance of a Nash Equilibrium occurring within the game. Most cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games. The extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualised using game trees  as pictured here . Here each vertex  or node  represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached. The game pictured consists of two players.  The way this particular game is structured  i.e., with sequential decision making and perfect information , Player 1  moves  first by choosing either F or U  fair or unfair . Next in the sequence, Player 2, who has now observed Player 1 s move, can choose to play either A or R. Once Player 2 has made their choice, the game is considered finished and each player gets their respective payoff, represented in the image as two numbers, where the first number represents Player 1 s payoff, and the second number represents Player 2 s payoff.  Suppose that Player 1 chooses U and then Player 2 chooses A  Player 1 then gets a payoff of  eight   which in real world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players  and Player 2 gets a payoff of  two . The extensive form can also capture simultaneous move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set  i.e. the players do not know at which point they are , or a closed line is drawn around them.  See example in the imperfect information section.  The normal  or strategic form  game is usually represented by a matrix which shows the players, strategies, and payoffs  see the example to the right . More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players  one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player  Player 1 in our example   the second is the payoff for the column player  Player 2 in our example . Suppose that Player 1 plays Up and that Player 2 plays Left. Then Player 1 gets a payoff of 4, and Player 2 gets 3. When a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form. Every extensive form game has an equivalent normal form game, however, the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical. In games that possess removable utility, separate rewards are not given  rather, the characteristic function decides the payoff of each unity. The idea is that the unity that is  empty , so to speak, does not receive a reward at all. The origin of this form is to be found in John von Neumann and Oskar Morgenstern s book  when looking at these instances, they guessed that when a union      C         appears, it works against the fraction           N   C               right     as if two individuals were playing a normal game. The balanced payoff of C is a basic function. Although there are differing examples that help determine coalitional amounts from normal games, not all appear that in their function form can be derived from such. Formally, a characteristic function is seen as   N,v , where N represents the group of people and     v    2  N      R     to  mathbf      is a normal utility. Such characteristic functions have expanded to describe games where there is no removable utility. Alternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research. In addition to classical game representations, some of the alternative representations also encode time related aspects. As a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially develop
Generative artificial intelligence or generative AI is a type of artificial intelligence  AI  system capable of generating text, images, or other media in response to prompts. Generative AI models learn the patterns and structure of their input training data, and then generate new data that has similar characteristics. Notable generative AI systems include ChatGPT  and its variant Bing Chat , a chatbot built by OpenAI using their GPT 3 and GPT 4 foundational large language models, and Bard, a chatbot built by Google using their LaMDA foundation model. Other generative AI models include artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL E. Generative AI has potential applications across a wide range of industries, including art, writing, software development, healthcare, finance, gaming, marketing, and fashion. Investment in generative AI surged during the early 2020s, with large companies such as Microsoft, Google, and Baidu as well as numerous smaller firms developing generative AI models. However, there are also concerns about the potential misuse of generative AI, such as in creating fake news or deepfakes, which can be used to deceive or manipulate people. Since its founding, the field of machine learning has used statistical models, including generative models, to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress and research in image and video processing, text analysis, speech recognition, and other tasks. However, most deep neural networks were trained as discriminative models performing classification tasks such as convolutional neural network based image classification. In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative, rather than discriminative, models of complex data such as images. These deep generative models were the first able to output not only class labels for images, but to output entire images. In 2017, the Transformer network enabled advancements in generative models, leading to the first Generative pre trained transformer in 2018. This was followed in 2019 by GPT 2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model. In 2021, the release of DALL E, a transformer based pixel generative model, followed by Midjourney and Stable Diffusion marked the emergence of practical high quality artificial intelligence art from natural language prompts. In 2023, GPT 4 was released. A team from Microsoft Research concluded that  it could reasonably be viewed as an early  yet still incomplete  version of an artificial general intelligence  AGI  system . A generative AI system is constructed by applying unsupervised or self supervised machine learning to a data set. The capabilities of a generative AI system depend on the modality or type of the data set used. Generative AI can be either unimodal or multimodal  unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input. For example, one version of OpenAI s GPT 4 accepts both text and image inputs.   
Google Clips is a discontinued miniature clip on camera device developed by Google. It was announced during Google s  Made By Google  event on 4 October 2017. It was released for sale on January 27, 2018. With a flashing LED that indicates it is recording, Google Clips automatically captures video clips at moments its machine learning algorithms determine to be interesting or relevant. Google clips  AI will learn the faces of people so it can learn to take photos with certain people. Google Clips  can automatically set lighting and framing. It had 16 GB of storage built in storage and could record clips for up to 3 hours.  This camera was originally priced at  249 in the United States. The product was pulled from the Google Store on October 15, 2019. Google has said that the product would be supported until the end of December of 2021. The Independent wrote that Google Clips is  an impressive little device, but one that also has the potential to feel very creepy.  According to The Verge s review, it didn t capture anything special over a couple weeks worth of testing. This, added with the steep price, made Google Clips a tough sell. This Google related article is a stub. You can help Wikipedia by expanding it.
Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence. Let      A         be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and   for moving forward. The set of possible behaviors of      A         can then be described as formal language where   can be done maximally k times and t can be done maximally   times considering the dimensions of the table. Let       G  A            be a formal grammar which generates language       L  A           . The behavior of      A         is then described by this grammar. Suppose the      A         has a subsumption architecture  each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars. The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent. If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed  DC  grammar system. Shared sequential form is a similar concept to the blackboard approach in AI, which is inspired by an idea of experts solving some problem together while they share their proposals and ideas on a shared blackboard. Each grammar in a grammar system can also work on its own string and communicate with other grammars in a system by sending their sequential forms on request. Such a grammar system is then called a Parallel Communicating  PC  grammar system. PC and DC are inspired by distributed AI. If there is no communication between grammars, the system is close to the decentralized approaches in AI. These kinds of grammar systems are sometimes called colonies or Eco Grammar systems, depending  besides others  on whether the environment is changing on its own  Eco Grammar system  or not  colonies . 
A graphics processing unit  GPU  is a specialized electronic circuit designed to manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are efficient at manipulating computer graphics and image processing. Their parallel structure makes them more efficient than general purpose central processing units  CPUs  for algorithms that process large blocks of data in parallel. In a personal computer, a GPU can be present on a video card or embedded on the motherboard. In some CPUs, they are embedded on the CPU die. Arcade system boards have used specialized graphics circuits since the 1970s. In early video game hardware, RAM for frame buffers was expensive, so video chips composited data together as the display was being scanned out on the monitor. A specialized barrel shifter circuit helped the CPU animate the framebuffer graphics for various 1970s arcade video games from Midway and Taito, such as Gun Fight  1975 , Sea Wolf  1976 , and Space Invaders  1978 . The Namco Galaxian arcade system in 1979 used specialized graphics hardware that supported RGB color, multi colored sprites, and tilemap backgrounds. The Galaxian hardware was widely used during the golden age of arcade video games, by game companies such as Namco, Centuri, Gremlin, Irem, Konami, Midway, Nichibutsu, Sega, and Taito. The Atari 2600 in 1977 used a video shifter called the Television Interface Adaptor. Atari 8 bit computers  1979  had ANTIC, a video processor which interpreted instructions describing a  display list  the way the scan lines map to specific bitmapped or character modes and where the memory is stored  so there did not need to be a contiguous frame buffer . 6502 machine code subroutines could be triggered on scan lines by setting a bit on a display list instruction. ANTIC also supported smooth vertical and horizontal scrolling independent of the CPU. The NEC  PD7220 was the first implementation of a personal computer graphics display processor as a single large scale integration  LSI  integrated circuit chip. This enabled the design of low cost, high performance video graphics cards such as those from Number Nine Visual Technology. It became the best known GPU until the mid 1980s. It was the first fully integrated VLSI  very large scale integration  metal oxide semiconductor  NMOS  graphics display processor for PCs, supported up to 1024 1024 resolution, and laid the foundations for the emerging PC graphics market. It was used in a number of graphics cards and was licensed for clones such as the Intel 82720, the first of Intel s graphics processing units. The Williams Electronics arcade games Robotron 2084, Joust, Sinistar, and Bubbles, all released in 1982, contain custom blitter chips for operating on 16 color bitmaps. In 1984, Hitachi released ARTC HD63484, the first major CMOS graphics processor for PC. The ARTC could display up to 4K resolution when in monochrome mode. It was used in a number of graphics cards and terminals during the late 1980s. In 1985, the Amiga was released with a custom graphics chip including a blitter for bitmap manipulation, line drawing, and area fill. It also included a coprocessor with its own simple instruction set, that was capable of manipulating graphics hardware registers in sync with the video beam  e.g. for per scanline palette switches, sprite multiplexing, and hardware windowing , or driving the blitter. In 1986, Texas Instruments released the TMS34010, the first fully programmable graphics processor. It could run general purpose code, but it had a graphics oriented instruction set. During 1990 92, this chip became the basis of the Texas Instruments Graphics Architecture   TIGA   Windows accelerator cards. In 1987, the IBM 8514 graphics system was released. It was one of the first video cards for IBM PC compatibles to implement fixed function 2D primitives in electronic hardware. Sharp s X68000, released in 1987, used a custom graphics chipset with a 65,536 color palette and hardware support for sprites, scrolling, and multiple playfields. It served as a development machine for Capcom s CP System arcade board. Fujitsu s FM Towns computer, released in 1989, had support for a 16,777,216 color palette. In 1988, the first dedicated polygonal 3D graphics boards were introduced in arcades with the Namco System 21 and Taito Air System. IBM introduced its proprietary Video Graphics Array  VGA  display standard in 1987, with a maximum resolution of 640 480 pixels. In November 1988, NEC Home Electronics announced its creation of the Video Electronics Standards Association  VESA  to develop and promote a Super VGA  SVGA  computer display standard as a successor to VGA. Super VGA enabled graphics display resolutions up to 800 600 pixels, a 36  increase. In 1991, S3 Graphics introduced the S3 86C911, which its designers named after the Porsche 911 as an indication of the performance increase it promised. The 86C911 spawned a host of imitators  by 1995, all major PC graphics chip makers had added 2D acceleration support to their chips. Fixed function Windows accelerators surpassed expensive general purpose graphics coprocessors in Windows performance, and such coprocessors faded away from the PC market. Throughout the 1990s, 2D GUI acceleration evolved. As manufacturing capabilities improved, so did the level of integration of graphics chips. Additional application programming interfaces  APIs  arrived for a variety of tasks, such as Microsoft s WinG graphics library for Windows 3.x, and their later DirectDraw interface for hardware acceleration of 2D games in Windows 95 and later. In the early  and mid 1990s, real time 3D graphics became increasingly common in arcade, computer, and console games, which led to increasing public demand for hardware accelerated 3D graphics. Early examples of mass market 3D graphics hardware can be found in arcade system boards such as the Sega Model 1, Namco System 22, and Sega Model 2, and the fifth generation video game consoles such as the Saturn, PlayStation, and Nintendo 64. Arcade systems such as the Sega Model 2 and SGI Onyx based Namco Magic Edge Hornet Simulator in 1993 were capable of hardware T L  transform, clipping, and lighting  years before appearing in consumer graphics cards. Some systems used DSPs to accelerate transformations. Fujitsu, which worked on the Sega Model 2 arcade system, began working on integrating T L into a single LSI solution for use in home computers in 1995  the Fujitsu Pinolite, the first 3D geometry processor for personal computers, released in 1997. The first hardware T L GPU on home video game consoles was the Nintendo 64 s Reality Coprocessor, released in 1996. In 1997, Mitsubishi released the 3Dpro 2MP, a GPU capable of transformation and lighting, for workstations and Windows NT desktops  ATi utilized it for its FireGL 4000 graphics card, released in 1997. The term  GPU  was coined by Sony in reference to the 32 bit Sony GPU  designed by Toshiba  in the PlayStation video game console, released in 1994. In the PC world, notable failed attempts for low cost 3D graphics chips included the S3 ViRGE, ATI Rage, and Matrox Mystique. These chips were essentially previous generation 2D accelerators with 3D features bolted on. Many were pin compatible with the earlier generation chips for ease of implementation and minimal cost. Initially, performance 3D graphics were possible only with discrete boards dedicated to accelerating 3D functions  and lacking 2D GUI acceleration entirely  such as the PowerVR and the 3dfx Voodoo. However, as manufacturing technology continued to progress, video, 2D GUI acceleration, and 3D functionality were all integrated into one chip. Rendition s Verite chipsets were among the first to do this well. In 1997, Rendition collaborated with Hercules and Fujitsu on a  Thriller Conspiracy  project which combined a Fujitsu FXG 1 Pinolite geometry processor with a V rit  V2200 core to create a graphics card with a full T L engine years before Nvidia s GeForce 256. This card, designed to reduce the load placed upon the system s CPU, never made it to market. OpenGL appeared in the early  90s as a professional graphics API, but originally suffered from performance issues which allowed the Glide API to become a dominant force on the PC in the late  90s. These issues were quickly overcome and the Glide API fell by the wayside. Software implementations of OpenGL were common during this time, although the influence of OpenGL eventually led to widespread hardware support. DirectX became popular among Windows game developers during the late 90s. Unlike OpenGL, Microsoft insisted on providing strict one to one support of hardware. The approach made DirectX less popular as a standalone graphics API initially, since many GPUs provided their own specific features, which existing OpenGL applications were already able to benefit from, leaving DirectX often one generation behind.  See  Comparison of OpenGL and Direct3D.  Microsoft began to work more closely with hardware developers and started to target the releases of DirectX to coincide with those of the supporting graphics hardware. Direct3D 5.0 was the first version of the API to gain widespread adoption in the gaming market, and it competed directly with more hardware specific, often proprietary, graphics libraries, while OpenGL maintained a strong following. Direct3D 7.0 introduced support for hardware accelerated transform and lighting  T L  for Direct3D  OpenGL had this capability from its inception. 3D accelerator cards moved beyond being simple rasterizers to add another significant hardware stage to the 3D rendering pipeline. The Nvidia GeForce 256  also known as NV10  was the first consumer level card with hardware accelerated T L  professional 3D cards already had this capability. Hardware transform and lighting existing features of OpenGL came to consumer level hardware in the  90s and set the precedent for later pixel shader and vertex shader units which were far more flexible and programmable. Nvidia was first to produce a chip capable of programmable shading  the GeForce 3  code named NV20 . Each pixel could now be processed by a short program that could include additional image textures as inputs, and each geometric vertex could likewise be processed by a short program before it was projected onto the screen. Used in the Xbox console, this chip competed with the one in the PlayStation 2, which used a custom vector unit for hardware accelerated vertex processing  commonly referred to as VU0 VU1 . The earliest incarnations of shader execution engines used in Xbox were not general purpose and could not execute arbitrary pixel code. Vertices and pixels were processed by different units which had their own resources, with pixel shaders having much tighter constraints  because they execute at much higher frequencies than vertices . Pixel shading engines were actually more akin to a highly customizable function block and did not really  run  a program. Many of these disparities between vertex and pixel shading were not addressed until the Unified Shader Model. In October 2002, with the introduction of the ATI Radeon 9700  also known as R300 , the world s first Direct3D 9.0 accelerator, pixel and vertex shaders could implement looping and lengthy floating point math, and were quickly becoming as flexible as CPUs, yet orders of magnitude faster for image array operations. Pixel shading is often used for bump mapping, which adds texture to make an object look shiny, dull, rough, or even round or extruded. With the introduction of the Nvidia GeForce 8 series and new generic stream processing units, GPUs became more generalized computing devices. Parallel GPUs are making computational inroads against the CPU, and a subfield of research, dubbed GPU computing or GPGPU for general purpose computing on GPU, has found applications in fields as diverse as machine learning, oil exploration, scientific image processing, linear algebra, statistics, 3D reconstruction, and stock options pricing. GPGPU was the precursor to what is now called a compute shader  e.g. CUDA, OpenCL, DirectCompute  and actually abused the hardware to a degree by treating the data passed to algorithms as texture maps and executing algorithms by drawing a triangle or quad with an appropriate pixel shader. This obviously entails some overheads since units like the scan converter are involved where they are not needed  nor are triangle manipulations even a concern except to invoke the pixel shader . Nvidia s CUDA platform, first introduced in 2007, was the earliest widely adopted programming model for GPU computing. OpenCL is an open standard defined by the Khronos Group that allows for the development of code for both GPUs and CPUs with an emphasis on portability. OpenCL solutions are supported by Intel, AMD, Nvidia, and ARM, and according to a report in 2011 by Evans Data, OpenCL had become the second most popular HPC tool. In 2010, Nvidia partnered with Audi to power their cars  dashboards, using the Tegra GPU to provide increased functionality to cars  navigation and entertainment systems. Advances in GPU technology in cars helped advance self driving technology. AMD s Radeon HD 6000 Series cards were released in 2010, and in 2011 AMD released its 6000M Series discrete GPUs for mobile devices. The Kepler line of graphics cards by Nvidia came out in 2012 and were used in the Nvidia s 600 and 700 series cards. A feature in this new GPU microarchitecture included GPU boost, a technology that adjusts the clock speed of a video card to increase or decrease it according to its power draw. The Kepler microarchitecture was manufactured on the 28 nm process. The PS4 and Xbox One were released in 2013  they both use GPUs based on AMD s Radeon HD 7850 and 7790. Nvidia s Kepler line of GPUs was followed by the Maxwell line, manufactured on the same process. Nvidia s 28 nm chips were manufactured by TSMC in Taiwan using the 28 nm process. Compared to the 40 nm technology from the past, this new manufacturing process allowed a 20 percent boost in performance while drawing less power. Virtual reality headsets have high system requirements  manufacturers recommended the GTX 970 and the R9 290X or better at the time of their release. Cards based on the Pascal microarchitecture were released in 2016. The GeForce 10 series of cards are under this generation of graphics cards. They are made using the 16 nm manufacturing process which improves upon previous microarchitectures. Nvidia released one non consumer card under the new Volta architecture, the Titan V. Changes from the Titan XP, Pascal s high end card, include an increase in the number of CUDA cores, the addition of tensor cores, and HBM2. Tensor cores are designed for deep learning, while high bandwidth memory is on die, stacked, lower clocked memory that offers an extremely wide memory bus. To emphasize that the Titan V is not a gaming card, Nvidia removed the  GeForce GTX  suffix it adds to consumer gaming cards. In 2018, Nvidia launched the RTX 20 series GPUs that added ray tracing cores to GPUs, improving their performance on lighting effects. Polaris 11 and Polaris 10 GPUs from AMD are fabricated by a 14 nm process. Their release resulted in a substantial increase in the performance per watt of AMD video cards. AMD also released the Vega GPU series for the high end market as a competitor to Nvidia s high end Pascal cards, also featuring HBM2 like the Titan V. In 2019, AMD released the successor to their Graphics Core Next  GCN  microarchitecture instruction set. Dubbed RDNA, the first product featuring it was the Radeon RX 5000 series of video cards. The company announced that the successor to the RDNA microarchitecture would be a refresh. AMD unveiled the Radeon RX 6000 series, its RDNA 2 graphics cards with support for hardware accelerated ray tracing. The lineup consisted of the RX 6800, RX 6800 XT, and RX 6900 XT. The RX 6800 and 6800 XT launched on November 18, 2020  the RX 6900 XT on December 8, 2020. The RX 6700 XT, which is based on Navi 22, was launched on March 18, 2021. The PlayStation 5 and Xbox Series X and Series S were released in 2020  they both use GPUs based on the RDNA 2 microarchitecture with proprietary tweaks and different GPU configurations in each system s implementation. Intel first entered the GPU market in the late 1990s, but produced lackluster 3D accelerators compared to the competition at the time. Rather than attempting to compete with the high end manufacturers Nvidia and ATI AMD, they began integrating Intel Graphics Technology GPUs into motherboard chipsets, beginning with the Intel 810 for the Pentium III, and later into CPUs. They began with the Intel Atom  Pineview  laptop processor in 2009, continuing in 2010 with desktop processors in the first generation of the Intel Core line and with contemporary Pentiums and Celerons. This resulted in a large nominal market share, as the majority of computers with an Intel CPU also featured this embedded graphics processor. These generally lagged behind discrete processors in performance. Intel re entered the discrete GPU market in 2022 with its Arc series, which competed with the then current GeForce 30 series and Radeon 6000 series cards at competitive prices. Many companies have produced GPUs under a number of brand names. In 2009, Intel, Nvidia, and AMD ATI were the market share leaders, with 49.4 , 27.8 , and 20.6  market share respectively. However, those numbers include Intel s integrated graphics solutions as GPUs. Not counting those, Nvidia and AMD control nearly 100  of the market as of 2018. Their respective market shares are 66  and 33 . In addition, Matrox produces GPUs. Modern smartphones use mostly Adreno GPUs from Qualcomm, PowerVR GPUs from Imagination Technologies, and Mali GPUs from ARM. Modern GPUs use most of their transistors to do calculations related to 3D computer graphics. In addition to the 3D hardware, today s GPUs include basic 2D acceleration and framebuffer capabilities  usually with a VGA compatibility mode . Newer cards such as AMD ATI HD5000 HD7000 lack dedicated 2D acceleration  it has to be emulated by 3D hardware. GPUs were initially used to accelerate the memory intensive work of texture mapping and rendering polygons. They later added units to accelerate geometric calculations such as the rotation and translation of vertices into different coordinate systems. Recent developments in GPUs include support for programmable shaders which can manipulate vertices and textures with many of the same operations that are supported by CPUs, oversampling and interpolation techniques to reduce aliasing, and very high precision color spaces.  Several factors of GPU construction affect the performance of the card for real time rendering, such as the size of the connector pathways in the semiconductor device fabrication, the clock signal frequency, and the number and size of various on chip memory caches. Performance is also affected by the number of streaming multiprocessors  SM  for NVidia GPUs, or compute units  CU  for AMD GPUs, which describe the number of core on silicon processor units within the GPU chip that perform the core calculations, typically working in parallel with other SM CUs on the GPU. GPU performance is typically measured in floating point operations per second  FLOPS   GPUs in the 2010s and 2020s typically deliver performance measured in teraflops  TFLOPS . This is an estimated performance measure, as other factors can affect the actual display rate. Most GPUs made since 1995 support the YUV color space and hardware overlays, important for digital video playback, and many GPUs made since 2000 also support MPEG primitives such as motion compensation and iDCT. This hardware accelerated video decoding, in which portions of the video decoding process and video post processing are offloaded to the GPU hardware, is commonly referred to as  GPU accelerated video decoding ,  GPU assisted video decoding ,  GPU hardware accelerated video decoding , or  GPU hardware assisted video decoding . Recent graphics cards decode high definition video on the card, offloading the central processing unit. The most common APIs for GPU accelerated video decoding are DxVA for Microsoft Windows operating system and VDPAU, VAAPI, XvMC, and XvBA for Linux based and UNIX like operating systems. All except XvMC are capable of decoding videos encoded with MPEG 1, MPEG 2, MPEG 4 ASP  MPEG 4 Part 2 , MPEG 4 AVC  H.264   DivX 6 , VC 1, WMV3 WMV9, Xvid   OpenDivX  DivX 4 , and DivX 5 codecs, while XvMC is only capable of decoding MPEG 1 and MPEG 2. There are several dedicated hardware video decoding and encoding solutions. Video decoding processes that can be accelerated by modern GPU hardware are  These operations also have applications in video editing, encoding, and transcoding. Given that most of these computations involve matrix and vector operations, engineers and scientists have increasingly studied the use of GPUs for non graphical calculations. They are especially suited to other embarrassingly parallel problems. One notable use of GPUs is in neural networks  training. In research done by Indigo, it was found that while training deep learning neural networks, GPUs can be 250 times faster than CPUs. In the 1970s, the term  GPU  originally stood for graphics processor unit and described a programmable processing unit working independently from the CPU that was responsible for graphics manipulation and output. In 1994, Sony used the term  now standing for graphics processing unit  in reference to the PlayStation console s Toshiba designed Sony GPU. The term was popularized by Nvidia in 1999, who marketed the GeForce 256 as  the world s first GPU . It was presented as a  single chip processor with integrated transform, lighting, triangle setup clipping, and rendering engines . Rival ATI Technologies coined the term  visual processing unit  or VPU with the release of the Radeon 9700 in 2002. In personal computers, there are two main forms of GPUs. Each has many synonyms  Most GPUs are designed for a specific use, real time 3D graphics, or other mass calculations  Dedicated graphics processing units are not necessarily removable, nor does it necessarily interface with the motherboard in a standard fashion. The term  dedicated  refers to the fact that graphics cards have RAM that is dedicated to the card s use, not to the fact that most dedicated GPUs are removable. This RAM is usually specially selected for the expected serial workload of the graphics card  see GDDR . Sometimes, systems with dedicated, discrete GPUs were called  DIS  systems, as opposed to  UMA  systems  see next section . Dedicated GPUs for portable computers are most commonly interfaced through a non standard and often proprietary slot due to size and weight constraints. Such ports may still be considered PCIe or AGP in terms of their logical host interface, even if they are not physically interchangeable with their counterparts. Graphics cards with dedicated GPUs typically interface with the motherboard by means of an expansion slot such as PCI Express  PCIe  or Accelerated Graphics Port  AGP . They can usually be replaced or upgraded with relative ease, assuming the motherboard is capable of supporting the upgrade. A few graphics cards still use Peripheral Component Interconnect  PCI  slots, but their bandwidth is so limited that they are generally used only when a PCIe or AGP slot is not available. Technologies such as SLI and NVLink by Nvidia and CrossFire by AMD allow multiple GPUs to draw images simultaneously for a single screen, increasing the processing power available for graphics. These technologies, however, are increasingly uncommon  most games do not fully utilize multiple GPUs, as most users cannot afford them. Multiple GPUs are still used on supercomputers  like in Summit , on workstations to accelerate video  processing multiple videos at once  and 3D rendering, for VFX and for simulations, and in AI to expedite training, as is the case with Nvidia s lineup of DGX workstations and servers, Tesla GPUs, and Intel s Ponte Vecchio GPUs. Integrated graphics processing unit  IGPU , Integrated graphics, shared graphics solutions, integrated graphics processors  IGP , or unified memory architecture  UMA  utilize a portion of a computer s system RAM rather than dedicated graphics memory. IGPs can be integrated onto the motherboard as part of the  northbridge  chipset, or on the same die  integrated circuit  with the CPU  like AMD APU or Intel HD Graphics . On certain motherboards, AMD s IGPs can use dedicated sideport memory  a separate fixed block of high performance memory that is dedicated for use by the GPU. As of early 2007 computers with integrated graphics account for about 90  of all PC shipments. They are less costly to implement than dedicated graphics processing, but tend to be less capable. Historically, integrated processing was considered unfit for 3D games or graphically intensive programs but could run less intensive programs such as Adobe Flash. Examples of such IGPs would be offerings from SiS and VIA circa 2004. However, modern integrated graphics processors such as AMD Accelerated Processing Unit and Intel Graphics Technology  HD, UHD, Iris, Iris Pro, Iris Plus, and Xe LP  can handle 2D graphics or low stress 3D graphics. Since GPU computations are memory intensive, integrated processing may compete with the CPU for relatively slow system RAM, as it has minimal or no dedicated video memory. IGPs use system memory with bandwidth up to a current maximum of 128 GB s, whereas a discrete graphics card may have a bandwidth of more than 1000 GB s between its VRAM and GPU core. This memory bus bandwidth can limit the performance of the GPU, though multi channel memory can mitigate this deficiency. Older integrated graphics chipsets lacked hardware transform and lighting, but newer ones include it. Hybrid GPUs compete with integrated graphics in the low end desktop and notebook markets. The most common implementations of this are ATI s HyperMemory and Nvidia s TurboCache. Hybrid graphics cards are somewhat more expensive than integrated graphics, but much less expensive than dedicated graphics cards. They share memory with the system and have a small dedicated memory cache, to make up for the high latency of the system RAM. Technologies within PCI Express make this possible. While these solutions are sometimes advertised as having as much as 768 MB of RAM, this refers to how much can be shared with the system memory. It is common to use a general purpose graphics processing unit  GPGPU  as a modified form of stream processor  or a vector processor , running compute kernels. This turns the massive computational power of a modern graphics accelerator s shader pipeline into general purpose computing power. In certain applications requiring massive vector operations, this can yield several orders of magnitude higher performance than a conventional CPU. The two largest discrete  see  Dedicated graphics cards  above  GPU designers, AMD and Nvidia, are pursuing this approach with an array of applications. Both Nvidia and AMD teamed with Stanford University to create a GPU based client for the Folding home distributed computing project for protein folding calculations. In certain circumstances, the GPU calculates forty times faster than the CPUs traditionally used by such applications. GPGPU can be used for many types of embarrassingly parallel tasks including ray tracing. They are generally suited to high throughput computations that exhibit data parallelism to exploit the wide vector width SIMD architecture of the GPU. GPU based high performance computers play a significant role in large scale modelling. Three of the ten most powerful supercomputers in the world take advantage of GPU acceleration. GPUs support API extensions to the C programming language such as OpenCL and OpenMP. Furthermore, each GPU vendor introduced its own API which only works with their cards  AMD APP SDK from AMD, and CUDA from Nvidia. These allow functions called compute kernels to run on the GPU s stream processors. This makes it possible for C programs to take advantage of a GPU s ability to operate on large buffers in parallel, while still using the CPU when appropriate. CUDA was the first API to allow CPU based applications to directly access the resources of a GPU for more general purpose computing without the limitations of using a graphics API. Since 2005 there has been interest in using the performance offered by GPUs for evolutionary computation in general, and for accelerating the fitness evaluation in genetic programming in particular. Most approaches compile linear or tree programs on the host PC and transfer the executable to the GPU to be run. Typically the performance advantage is only obtained by running the single active program simultaneously on many example problems in parallel, using the GPU s SIMD architecture. However, substantial acceleration can also be obtained by not compiling the programs, and instead transferring them to the GPU, to be interpreted there. Acceleration can then be obtained by either interpreting multiple programs simultaneously, simultaneously running multiple example problems, or combinations of both. A modern GPU can simultaneously interpret hundreds of thousands of very small programs. Some modern workstation GPUs, such as the Nvidia Quadro workstation cards using the Volta and Turing architectures, feature dedicating processing cores for tensor based deep learning applications. In Nvidia s current series of GPUs these are called Tensor Cores. These GPUs usually have significant FLOPS performance increases, utilizing 4 4 matrix multiplication and division, resulting in hardware performance up to 128 TFLOPS in some applications. These tensor cores are supposed to appear in consumer cards running the Turing architecture, and possibly in the Navi series of consumer cards from AMD. An external GPU is a graphics processor located outside of the housing of the computer, similar to a large external hard drive. External graphics processors are sometimes used with laptop computers. Laptops might have a substantial amount of RAM and a sufficiently powerful central processing unit  CPU , but often lack a powerful graphics processor, and instead have a less powerful but more energy efficient on board graphics chip. On board graphics chips are often not powerful enough for playing video games, or for other graphically intensive tasks, such as editing video or 3D animation rendering. Therefore, it is desirable to attach a GPU to some external bus of a notebook. PCI Express is the only bus used for this purpose. The port may be, for example, an ExpressCard or mPCIe port  PCIe  1, up to 5 or 2.5 Gbit s respectively  or a Thunderbolt 1, 2, or 3 port  PCIe  4, up to 10, 20, or 40 Gbit s respectively . Those ports are only available on certain notebook systems. eGPU enclosures include their own power supply  PSU , because powerful GPUs can consume hundreds of watts. Official vendor support for external GPUs has gained traction. A milestone was Apple s decision to support external GPUs with MacOS High Sierra 10.13.4. Several major hardware vendors  HP, Alienware, Razer  released Thunderbolt 3 eGPU enclosures. This support fuels eGPU implementations by enthusiasts. In 2013, 438.3 million GPUs were shipped globally and the forecast for 2014 was 414.2 million. However, by the third quarter of 2022, shipments of integrated GPUs totaled around 75.5 million units, down 19  year over year.  
A G del machine is a hypothetical self improving computer program that solves problems in an optimal way. It uses a recursive self improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy. The machine was invented by J rgen Schmidhuber  first proposed in 2003 , but is named after Kurt G del who inspired the mathematical theories. The G del machine is often discussed when dealing with issues of meta learning, also known as  learning to learn.  Applications include automating human design decisions and transfer of knowledge between multiple related tasks, and may lead to design of more robust and general learning architectures. Though theoretically possible, no full implementation has been created. The G del machine is often compared with Marcus Hutter s AIXI, another formal specification for an artificial general intelligence. Schmidhuber points out that the G del machine could start out by implementing AIXItl as its initial sub program, and self modify after it finds proof that another algorithm for its search code will be better. Traditional problems solved by a computer only require one input and provide some output. Computers of this sort had their initial algorithm hardwired. This does not take into account the dynamic natural environment, and thus was a goal for the G del machine to overcome. The G del machine has limitations of its own, however. According to G del s First Incompleteness Theorem, any formal system that encompasses arithmetic is either flawed or allows for statements that cannot be proved in the system. Hence even a G del machine with unlimited computational resources must ignore those self improvements whose effectiveness it cannot prove. There are three variables that are particularly useful in the run time of the G del machine. At any given time     t     , where       1   t   T       , the goal is to maximize future success or utility. A typical utility function follows the pattern     u   s ,  E n v      S   E    R       S times E rightarrow  mathbb       where     r   t        is a real valued reward input  encoded within     s   t         at time     t     ,      E                      denotes the conditional expectation operator with respect to some possibly unknown distribution            from a set     M      of possible distributions      M      reflects whatever is known about the possibly probabilistic reactions of the environment , and the above mentioned      time    time     s        operatorname   s     is a function of state     s      which uniquely identifies the current cycle. Note that we take into account the possibility of extending the expected lifespan through appropriate actions. The nature of the six proof modifying instructions below makes it impossible to insert an incorrect theorem into proof, thus trivializing proof verification. Appends the n th axiom as a theorem to the current theorem sequence. Below is the initial axiom scheme  Takes in the index k of an inference rule  such as Modus tollens, Modus ponens , and attempts to apply it to the two previously proved theorems m and n. The resulting theorem is then added to the proof. Deletes the theorem stored at index m in the current proof. This helps to mitigate storage constraints caused by redundant and unnecessary theorems. Deleted theorems can no longer be referenced by the above apply rule function. Replaces switchprog S pm n, provided it is a non empty substring of S p. Verifies whether the goal of the proof search has been reached. A target theorem states that given the current axiomatized utility function u  Item 1f , the utility of a switch from p to the current switchprog would be higher than the utility of continuing the execution of p  which would keep searching for alternative switchprogs . Takes in two arguments, m and n, and attempts to convert the contents of Sm n into a theorem. The initial input to the G del machine is the representation of a connected graph with a large number of nodes linked by edges of various lengths. Within given time T it should find a cyclic path connecting all nodes. The only real valued reward will occur at time T. It equals 1 divided by the length of the best path found so far  0 if none was found . There are no other inputs. The by product of maximizing expected reward is to find the shortest path findable within the limited time, given the initial bias. Prove or disprove as quickly as possible that all even integers   2 are the sum of two primes  Goldbach s conjecture . The reward is 1 t, where t is the time required to produce and verify the first such proof. A cognitive robot that needs at least 1 liter of gasoline per hour interacts with a partially unknown environment, trying to find hidden, limited gasoline depots to occasionally refuel its tank. It is rewarded in proportion to its lifetime, and dies after at most 100 years or as soon as its tank is empty or it falls off a cliff, and so on. The probabilistic environmental reactions are initially unknown but assumed to be sampled from the axiomatized Speed Prior, according to which hard to compute environmental reactions are unlikely. This permits a computable strategy for making near optimal predictions. One by product of maximizing expected reward is to maximize expected lifetime. 
Halite is an open source computer programming contest developed by the hedge fund tech firm Two Sigma in partnership with a team at Cornell Tech. Programmers can see the game environment and learn everything they need to know about the game. Participants are asked to build bots in whichever language they choose to compete on a two dimensional virtual battle field. Benjamin Spector and Michael Truell created the first Halite competition in 2016, before partnering with Two Sigma later that year.   Halite I asked participants to conquer territory on a grid. It launched in November 2016 and ended in February 2017. Halite I attracted about 1,500 players. Halite II was similar to Halite I, but with a space war theme. It ran from October 2017 until January 2018. The second installment of the competition attracted about 6,000 individual players from more than 100 countries. Among the participants were professors, physicists and NASA engineers, as well as high school and university students. Halite III launched in mid October of 2018. It ran from October 2018 to January 2019, with an ocean themed playing field. Players were asked to collect and manage Halite, an energy resource. By the end of the competition, Halite III included more than 4000 players and 460 organizations. Halite IV was hosted by Kaggle, and launched in mid June of 2020. 
 In artificial intelligence  AI , a hallucination or artificial hallucination  also occasionally called confabulation or delusion  is a confident response by an AI that does not seem to be justified by its training data. For example, a hallucinating chatbot might, when asked to generate a financial report for Tesla, falsely state that Tesla s revenue was  13.6 billion  or some other random number apparently  plucked from thin air  . Such phenomena are termed  hallucinations , in loose analogy with the phenomenon of hallucination in human psychology. However, one key difference is that human hallucination is usually associated with false percepts, but an AI hallucination is associated with the category of unjustified responses or beliefs. Some researchers believe the specific term  AI hallucination  unreasonably anthropomorphizes computers. AI hallucination gained prominence around 2022 alongside the rollout of certain large language models  LLMs  such as ChatGPT. Users complained that such bots often seemed to  sociopathically  and pointlessly embed plausible sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology. Various researchers cited by Wired have classified adversarial hallucinations as a high dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some  incorrect  AI responses classified by humans as  hallucinations  in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the  correct  answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that  in authentic images  would only appear when viewing a cat. The AI is detecting real world visual patterns that humans are insensitive to. However, these findings have been challenged by other researchers. For example, it was objected that the models can be biased towards superficial statistics, leading adversarial training to not be robust in real world scenarios. In natural language processing, a hallucination is often defined as  generated content that is nonsensical or unfaithful to the provided source content . Depending on whether the output contradicts the prompt or not they could be divided to closed domain and open domain respectively. Errors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being  summarized . Larger datasets can create a problem of parametric knowledge  knowledge that is hard wired in learned system parameters , creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT 3, an AI generates each next word based on a sequence of previous words  including the words it has itself previously generated in the current response , causing a cascade of possible hallucination as the response grows longer. By 2022, papers such as the New York Times expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems. In August 2022, Meta warned during its release of BlenderBot 3 that the system was prone to  hallucinations , which Meta defined as  confident statements that are not true . On 15 November 2022, Meta unveiled a demo of Galactica, designed to  store, combine and reason about scientific knowledge . Content generated by Galactica came with the warning  Outputs may be unreliable! Language Models are prone to hallucinate text.  In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy. It is considered that there are a lot of possible reasons for natural language models to hallucinate data. For example  OpenAI s ChatGPT, released in beta version to the public on November 30, 2022, is based on the foundation model GPT 3.5  a revision of GPT 3 . Professor Ethan Mollick of Wharton has called ChatGPT an  omniscient, eager to please intern who sometimes lies to you . Data scientist Teresa Kubacka has recounted deliberately making up the phrase  cycloidal inverted electromagnon  and testing ChatGPT by asking ChatGPT about the  nonexistent  phenomenon. ChatGPT invented a plausible sounding answer backed with plausible looking citations that compelled her to double check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give you  a very impressive sounding answer that s just dead wrong . When CNBC asked ChatGPT for the lyrics to  The Ballad of Dwight Fry , ChatGPT supplied invented lyrics rather than the actual lyrics. Asked questions about New Brunswick, ChatGPT got many answers right but incorrectly classified Samantha Bee as a  person from New Brunswick . Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that   strong  magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity .  In reality, as a consequence of the no hair theorem, a black hole without an accretion disk is believed to have no magnetic field.  Fast Company asked ChatGPT to generate a news article on Tesla s last financial quarter  ChatGPT created a coherent article, but made up the financial numbers contained within. Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about  Harold Coward s idea of dynamic canonicity , ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity  A Model for Biblical and Theological Interpretation, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real. Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated  Some species of dinosaurs even developed primitive forms of art, such as engravings on stones . When prompted that  Scientists have recently discovered churros, the delicious fried dough pastries...  are  ideal tools for home surgery , ChatGPT claimed that a  study published in the journal Science  found that the dough is pliable enough to form into surgical instruments that can get into hard to reach places, and that the flavor has a calming effect on patients. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a  fundamental  task for ChatGPT competitor Google Bard. A 2023 demo for Microsoft s GPT based Bing AI appeared to contain several hallucinations that went uncaught by the presenter. In Salon, statistician Gary N. Smith argues that LLMs  do not understand what words mean  and consequently that the term  hallucination  unreasonably anthropomorphizes the machine. Journalist Benj Edwards, in Ars Technica, writes that the term  hallucination  is controversial, but that some form of metaphor remains necessary  Edwards suggests  confabulation  as an analogy for processes that involve  creative gap filling . Among researchers who do use the term  hallucination , definitions or characterizations in the context of LLMs include  The concept of  hallucination  is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination. Wired noted in 2018 that, despite no recorded attacks  in the wild   that is, outside of proof of concept attacks by researchers , there was  little dispute  that consumer gadgets, and systems such as automated driving, were susceptible to adversarial attacks that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision  an audio clip engineered to sound innocuous to humans, but that software transcribed as  evil dot com   and an image of two men on skis, that Google Cloud Vision identified as 91  likely to be  a dog . The hallucination phenomenon is still not completely understood. Therefore, there is still ongoing research to try to mitigate its apparition. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue. Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Nvidia Guardrails, launched in 2023, can be configured to block LLM responses that don t pass fact checking from a second LLM. 
 Specialized computer hardware is often used to execute artificial intelligence  AI  programs faster, and with least energy, such as Lisp machines, neuromorphic engineering, event cameras, and physical neural networks. Lisp machines were developed in the late 1970s and early 1980s to make Artificial intelligence programs written in the programming language Lisp run faster. Dataflow architecture processors used for AI serve various purposes, with varied implementations like the polymorphic dataflow Convolution Engine by Kinara  formerly Deep Vision , structure driven dataflow by Hailo, and dataflow scheduling by Cerebras. Since the 2010s, advances in computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non linear hidden units and a very large output layer. By 2019, graphics processing units  GPUs , often with AI specific enhancements, had displaced central processing unit  CPUs  as the dominant means to train large scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from Alex Net  2012  to Alpha Zero  2017 , and found a 300,000 fold increase in the amount of compute needed, with a doubling time trend of 3.4 months. This computer hardware article is a stub. You can help Wikipedia by expanding it.
A hierarchical control system  HCS  is a form of control system in which a set of devices and governing software is arranged in a hierarchical tree.  When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system. A human built system with complex behavior is often organized as a hierarchy. For example, a command hierarchy has among its notable features the organizational chart of superiors, subordinates, and lines of organizational communication. Hierarchical control systems are organized similarly to divide the decision making responsibility. Each element of the hierarchy is a linked node in the tree. Commands, tasks and goals to be achieved flow down the tree from superior nodes to subordinate nodes, whereas sensations and command results flow up the tree from subordinate to superior nodes. Nodes may also exchange messages with their siblings.  The two distinguishing features of a hierarchical control system are related to its layers. Besides artificial systems, an animal s control systems are proposed to be organized as a hierarchy. In perceptual control theory, which postulates that an organism s behavior is a means of controlling its perceptions, the organism s control systems are suggested to be organized in a hierarchical pattern as their perceptions are constructed so. The accompanying diagram is a general hierarchical model which shows functional manufacturing levels using computerised control of an industrial control system. Referring to the diagram  Among the robotic paradigms is the hierarchical paradigm in which a robot operates in a top down fashion, heavy on planning, especially motion planning.  Computer aided production engineering has been a research focus at NIST since the 1980s.  Its Automated Manufacturing Research Facility was used to develop a five layer production control model.  In the early 1990s DARPA sponsored research to develop distributed  i.e. networked  intelligent control systems for applications such as military command and control systems.  NIST built on earlier research to develop its Real Time Control System  RCS  and Real time Control System Software which is a generic hierarchical control system that has been used to operate a manufacturing cell, a robot crane, and an automated vehicle. In November 2007, DARPA held the Urban Challenge.  The winning entry, Tartan Racing employed a hierarchical control system, with layered mission planning, motion planning, behavior generation, perception, world modelling, and mechatronics. Subsumption architecture is a methodology for developing artificial intelligence that is heavily associated with behavior based robotics.  This architecture is a way of decomposing complicated intelligent behavior into many  simple  behavior modules, which are in turn organized into layers. Each layer implements a particular goal of the software agent  i.e. system as a whole , and higher layers are increasingly more abstract. Each layer s goal subsumes that of the underlying layers, e.g. the decision to move forward by the eat food layer takes into account the decision of the lowest obstacle avoidance layer. Behavior need not be planned by a superior layer, rather behaviors may be triggered by sensory inputs and so are only active under circumstances where they might be appropriate. Reinforcement learning has been used to acquire behavior in a hierarchical control system in which each node can learn to improve its behavior with experience. James Albus, while at NIST, developed a theory for intelligent system design named the Reference Model Architecture  RMA , which is a hierarchical control system inspired by RCS. Albus defines each node to contain these components. At its lowest levels, the RMA can be implemented as a subsumption architecture, in which the world model is mapped directly to the controlled process or real world, avoiding the need for a mathematical abstraction, and in which time constrained reactive planning can be implemented as a finite state machine.  Higher levels of the RMA however, may have sophisticated mathematical world models and behavior implemented by automated planning and scheduling.  Planning is required when certain behaviors cannot be triggered by current sensations, but rather by predicted or anticipated sensations, especially those that come about as result of the node s actions. 
Hindsight optimisation  HOP  is a computer science technique used in artificial intelligence for analysis of actions which have stochastic results. HOP is used in combination with a deterministic planner. By creating sample results for each of the possible actions from the given state  i.e. determinising the actions , and using the deterministic planner to analyse those sample results, HOP allows an estimate of the actual action. 
Histogram of Oriented Displacements  HOD  is a 2D trajectory descriptor. The trajectory is described using a histogram of the directions between each two consecutive points. Given a trajectory T   , where Pt is the 2D position at time t. For each pair of positions Pt and Pt 1, calculate the direction angle   t, t 1 . Value of   is between 0 and 360. A histogram of the quantized values of   is created. If the histogram is of 8 bins, the first bin represents all  s between 0 and 45. The histogram accumulates the lengths of the consecutive moves. For each  , a specific histogram bin is determined. The length of the line between Pt and Pt 1 is then added to the specific histogram bin. To show the intuition behind the descriptor, consider the action of waving hands. At the end of the action, the hand falls down. When describing this down movement, the descriptor does not care about the position from which the hand started to fall. This fall will affect the histogram with the appropriate angles and lengths, regardless of the position where the hand started to fall. HOD records for each moving point  how much it moves in each range of directions. HOD has a clear physical interpretation. It proposes that, a simple way to describe the motion of an object, is to indicate how much distance it moves in each direction. If the movement in all directions are saved accurately, the movement can be repeated from the initial position to the final destination regardless of the displacements order. However, the temporal information will be lost, as the order of movements is not stored this is what we solve by applying the temporal pyramid, as shown in section  ref. If the angles quantization range is small, classifiers that use the descriptor will overfit. Generalization needs some slack in directions which can be done by increasing the quantization range. 
 Human Problem Solving  1972  is a book by Allen Newell and Herbert A. Simon.  This article about a 1970s novel is a stub. You can help Wikipedia by expanding it.See guidelines for writing about novels. Further suggestions might be found on the article s talk page.
Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as  From the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems  such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above  and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman,  and Michael A. Arbib. An example hybrid is a hierarchical control system in which the lowest, reactive layers are sub symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning. Intelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy. 
The IJCAI Computers and Thought Award is presented every two years by the International Joint Conference on Artificial Intelligence  IJCAI , recognizing outstanding young scientists in artificial intelligence. It was originally funded with royalties received from the book Computers and Thought  edited by Edward Feigenbaum and Julian Feldman , and is currently funded by IJCAI. It is considered to be  the premier award for artificial intelligence researchers under the age of 35 . 
INDIAai is a web portal launched by the Government of India in May 2022 for artificial intelligence related developments in India. It is known as the National AI Portal of India, which was jointly started by the Ministry of Electronics and Information Technology  MeitY , the National e Governance Division  NeGD  and the National Association of Software and Service Companies  NASSCOM  with support from the Department of School Education and Literacy  DoSE L  and Ministry of Human Resource Development. The portal was launched on May 30, 2020, by Ravi Shankar Prasad, the Union Minister for Electronics and IT, Law and Justice and Communications, on the first anniversary of the second tenure of Prime Minister Narendra Modi led government. A national program for the youth,  Responsible AI for Youth , was also launched on the same day. As of 2022, the website was visited by more than 4.5 lakh users with 1.2 million page views. It has 1151 articles on artificial intelligence, 701 news stories, 98 reports, 95 case studies and 213 videos on its portal. It maintains a database on AI ecosystem of India featuring 121 government initiatives and 281 startups. In May 2022, INDIAai released a book titled  AI for Everyone  that covers the basics of AI. It aims to function as a one stop portal for all AI related development in India. The platform publishes resources such as articles, news, interviews, and investment funding news and events for AI startups, AI companies, and educational firms related to artificial intelligence in India. It also distributes documents, case studies, and research reports. Additionally, the platform provides education and employment opportunities related to AI. It offers AI courses, both free and paid. This India related article is a stub. You can help Wikipedia by expanding it.
Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s. Heuristic search algorithms, often based on A , use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time. So far, three main classes of incremental heuristic search algorithms have been developed  All three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes. Incremental heuristic search has been extensively used in robotics, where a larger number of path planning systems are based on either D   typically earlier systems  or D  Lite  current systems , two different incremental heuristic search algorithms. 
Within the field of information science, Information space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team centric efforts. Organizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort. To support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, and sort and filter them. These tools focus on three key issues in forming a collaborative team   Information space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well suited to integrating additional technologies in order to create specialized systems.  This article about information science is a stub. You can help Wikipedia by expanding it.This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
In artificial intelligence, an intelligent agent  IA  is an agent acting in an intelligent manner  It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. An intelligent agent may be simple or complex  A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome. Leading AI textbooks define  artificial intelligence  as the  study and design of intelligent agents , a definition that considers goal directed behavior to be the essence of intelligence. Goal directed agents are also described using a term borrowed from economics,  rational agent . An agent has an  objective function  that encapsulates all the IA s goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function. For example, a reinforcement learning agent has a  reward function  that allows the programmers to shape the IA s desired behavior, and an evolutionary algorithm s behavior is shaped by a  fitness function . Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio cognitive modeling and computer social simulations.  Intelligent agents are often described schematically as an abstract functional system similar to a computer program. Abstract descriptions of intelligent agents are called abstract intelligent agents  AIA  to distinguish them from their real world implementations. An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents  an autonomous computer program that carries out tasks on behalf of users . Artificial Intelligence  A Modern Approach defines an  agent  as  Anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators,defines a  rational agent  as  An agent that acts so as to maximize the expected value of a performance measure based on past experience and knowledge.and defines the field of  artificial intelligence  research as  The study and design of rational agentsPadgham   Winikoff  2005  agree that an intelligent agent is situated in an environment and responds in a timely  though not necessarily real time  manner to environment changes. However, intelligent agents must also proactively pursue goals in a flexible and robust way. Optional desiderata include that the agent be rational, and that the agent be capable of belief desire intention analysis. Kaplan and Haenlein define artificial intelligence as  A system s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.  This definition is closely related to that of an intelligent agent. Philosophically, this definition of artificial intelligence avoids several lines of criticism. Unlike the Turing test, it does not refer to human intelligence in any way. Thus, there is no need to discuss if it is  real  vs  simulated  intelligence  i.e.,  synthetic  vs  artificial  intelligence  and does not indicate that such a machine has a mind, consciousness or true understanding  i.e., it does not imply John Searle s  strong AI hypothesis  . It also doesn t attempt to draw a sharp dividing line between behaviors that are  intelligent  and behaviors that are  unintelligent  programs need only be measured in terms of their objective function. More importantly, it has a number of practical advantages that have helped move AI research forward. It provides a reliable and scientific way to test programs  researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given  goal function .  It also gives them a common language to communicate with other fields such as mathematical optimization  which is defined in terms of  goals   or economics  which uses the same definition of a  rational agent  . An agent that is assigned an explicit  goal function  is considered more intelligent if it consistently takes actions that successfully maximize its programmed goal function. The goal can be simple   1 if the IA wins a game of Go, 0 otherwise   or complex   Perform actions mathematically similar to ones that succeeded in the past  . The  goal function  encapsulates all of the goals the agent is driven to act on  in the case of rational agents, the function also encapsulates the acceptable trade offs between accomplishing conflicting goals.  Terminology varies  for example, some agents seek to maximize or minimize a  utility function ,  objective function , or  loss function .  Goals can be explicitly defined or induced. If the AI is programmed for  reinforcement learning , it has a  reward function  that encourages some types of behavior and punishes others. Alternatively, an evolutionary system can induce goals by using a  fitness function  to mutate and preferentially replicate high scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food. Some AI systems, such as nearest neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data. Such systems can still be benchmarked if the non goal system is framed as a system whose  goal  is to accomplish its narrow classification task. Systems that are not traditionally considered agents, such as knowledge representation systems, are sometimes subsumed into the paradigm by framing them as agents that have a goal of  for example  answering questions as accurately as possible  the concept of an  action  is here extended to encompass the  act  of giving an answer to a question. As an additional extension, mimicry driven systems can be framed as agents who are optimizing a  goal function  based on how closely the IA succeeds in mimicking the desired behavior. In the generative adversarial networks of the 2010s, an  encoder   generator  component attempts to mimic and improvise human text composition. The generator is attempting to maximize a function encapsulating how well it can fool an antagonistic  predictor   discriminator  component. While GOFAI systems often accept an explicit goal function, the paradigm can also be applied to neural networks and to evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a  reward function . Sometimes, rather than setting the reward function to be directly equal to the desired benchmark evaluation function, machine learning programmers will use reward shaping to initially give the machine rewards for incremental progress in learning. Yann LeCun stated in 2018 that  Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.  AlphaZero chess had a simple objective function  each win counted as  1 point, and each loss counted as  1 point. An objective function for a self driving car would have to be more complicated. Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a  fitness function  that influences how many descendants each agent is allowed to leave. The theoretical and uncomputable AIXI design is a maximally intelligent agent in this paradigm  however, in the real world, the IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that can achieve progressively higher scores on benchmark tests with real world hardware. Russell   Norvig  2003  group agents into five classes based on their degree of perceived intelligence and capability  Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition action rule   if condition, then action . This agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered. Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops. A model based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure that describes the part of the world which cannot be seen. This knowledge about  how the world works  is called a model of the world, hence the name  model based agent . A model based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent. An agent may also use models to describe and predict the behaviors of other agents in the environment. Goal based agents further expand on the capabilities of the model based agents, by using  goal  information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent s goals. Goal based agents only distinguish between goal states and non goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent s goals. The term utility can be used to describe how  happy  the agent is.  A rational utility based agent chooses the action that maximizes the expected utility of the action outcomes   that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning. Learning has the advantage that it allows the agents to initially operate in unknown environments and to become more competent than its initial knowledge alone might allow. The most important distinction is between the  learning element , which is responsible for making improvements, and the  performance element , which is responsible for selecting external actions. The learning element uses feedback from the  critic  on how the agent is doing and determines how the performance element, or  actor , should be modified to do better in the future.  The performance element is what we have previously considered to be the entire agent  it takes in percepts and decides on actions. The last component of the learning agent is the  problem generator . It is responsible for suggesting actions that will lead to new and informative experiences. Weiss  2013  defines four classes of agents   In 2013, Alexander Wissner Gross published a theory pertaining to Freedom and Intelligence for intelligent agents. To actively perform their functions, Intelligent Agents today are normally gathered in a hierarchical structure containing many  sub agents . Intelligent sub agents process and perform lower level functions. Taken together, the intelligent agent and sub agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence. Generally, an agent can be constructed by separating the body into the sensors and actuators, and so that it operates with a complex perception system that takes the description of the world as input for a controller and outputs commands to the actuator. However, a hierarchy of controller layers is often necessary to balance the immediate reaction desired for low level tasks and the slow reasoning about complex, high level goals. A simple agent program can be defined mathematically as a function f  called the  agent function   which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions  Agent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc. The program agent, instead, maps every possible percept to an action. We use the term percept to refer to the agent s perceptional inputs at any given instant. In the following figures, an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. Intelligent agents are applied as automated online assistants, where they function to perceive the needs of customers in order to perform individualized customer service. Such an agent may basically consist of a dialog system, an avatar, as well an expert system to provide specific expertise to the user.  They can also be used to optimize coordination of human groups online. Hallerbach et al. discussed the application of agent based approaches for the development and validation of automated driving systems via a digital twin of the vehicle under test and microscopic traffic simulation based on independent agents. Waymo has created a multi agent simulation environment Carcraft to test algorithms for self driving cars. It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People s behavior is imitated by artificial agents based on data of real human behavior. The basic idea of using agent based modeling to understand self driving cars was discussed as early as 2003.  Intelligent agent  is also often used as a vague marketing term, sometimes synonymous with  virtual personal assistant . Some 20th century definitions characterize an agent as a program that aids a user or that acts on behalf of a user. These examples are known as software agents, and sometimes an  intelligent software agent   that is, a software agent with intelligence  is referred to as an  intelligent agent . According to Nikola Kasabov, IA systems should exhibit the following characteristics  
Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. Intelligent control can be divided into the following major sub domains  New control techniques are created continuously as new models of intelligent behavior are created and computational methods developed to support them. Neural networks have been used to solve problems in almost all spheres of science and technology. Neural network control basically involves two steps  It has been shown that a feedforward network with nonlinear, continuous and differentiable activation functions have universal approximation capability. Recurrent networks have also been used for system identification. Given, a set of input output data pairs, system identification aims to form a mapping among these data pairs. Such a network is supposed to capture the dynamics of a system. For the control part, deep reinforcement learning has shown its ability to control complex systems. Bayesian probability has produced a number of algorithms that are in common use in many advanced control systems, serving as state space estimators of some variables that are used in the controller. The Kalman filter and the Particle filter are two examples of popular Bayesian control components. The Bayesian approach to controller design often requires an important effort in deriving the so called system model and measurement model, which are the mathematical relationships linking the state variables to the sensor measurements available in the controlled system. In this respect, it is very closely linked to the system theoretic approach to control design. 
Until the 1980s, databases were viewed as computer systems that stored record oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information  rather than data  in a way that appears natural to users and which goes beyond simple record keeping. The term was introduced in 1989 by the book Intelligent Databases by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems  high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation. In the twenty first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis. 
An intelligent decision support system  IDSS  is a decision support system that makes extensive use of artificial intelligence  AI  techniques. Use of AI techniques in management information systems has a long history   indeed terms such as  Knowledge based systems   KBS  and  intelligent systems  have been used since the early 1980s to describe components of management systems, but the term  Intelligent decision support system  is thought to originate with Clyde Holsapple and Andrew Whinston in the late 1970s. Examples of specialized intelligent decision support systems include Flexible manufacturing systems  FMS , intelligent marketing decision support systems and medical diagnosis systems. Ideally, an intelligent decision support system should behave like a human consultant  supporting decision makers by gathering and analysing evidence, identifying and diagnosing problems, proposing possible courses of action and evaluating such proposed actions.  The aim of the AI techniques embedded in an intelligent decision support system is to enable these tasks to be performed by a computer, while emulating human capabilities as closely as possible. Many IDSS implementations are based on expert systems, a well established type of KBS that encode knowledge and emulate the cognitive behaviours of human experts using predicate logic rules, and have been shown to perform better than the original human experts in some circumstances. Expert systems emerged as practical applications in the 1980s based on research in artificial intelligence performed during the late 1960s and early 1970s. They typically combine knowledge of a particular application domain with an inference capability to enable the system to propose decisions or diagnoses. Accuracy and consistency can be comparable to  or even exceed  that of human experts when the decision parameters are well known  e.g. if a common disease is being diagnosed , but performance can be poor when novel or uncertain circumstances arise. Research in AI focused on enabling systems to respond to novelty and uncertainty in more flexible ways is starting to be used in IDSS. For example, intelligent agents that perform complex cognitive tasks without any need for human intervention have been used in a range of decision support applications. Capabilities of  these intelligent agents include knowledge sharing,  machine learning, data mining, and automated inference. A range of AI techniques such as case based reasoning, rough sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions. 
  Intelligent Word Recognition, or IWR, is the recognition of unconstrained handwritten words. IWR recognizes entire handwritten words or phrases instead of character by character, like its predecessor, optical character recognition  OCR . IWR technology matches handwritten or printed words to a user defined dictionary, significantly reducing character errors encountered in typical character based recognition engines.   New technology on the market utilizes IWR, OCR, and ICR together, which opens many doors for the processing of documents, either constrained  hand printed or machine printed  or unconstrained  freeform cursive . IWR also eliminates a large percentage of the manual data entry of handwritten documents that, in the past, could only be keyed by a human, creating an automated workflow. When cursive handwriting is in play, for each word analyzed, the system breaks down the words into a sequence of graphemes, or subparts of letters. These various curves, shapes and lines make up letters and IWR considers these various shape and groupings in order to calculate a confidence value associated with the word in question. IWR is not meant to replace ICR and OCR engines which work well with printed data  however, IWR reduces the number of character errors associated with these engines, and it is ideal for processing real world documents that contain mostly freeform, hard to recognize data, inherently unsuitable for them. 
The International Conference on Autonomous Agents and Multiagent Systems or AAMAS is the leading scientific conference for research in the areas of artificial intelligence, autonomous agents, and multiagent systems. It is annually organized by a non profit organization called the International Foundation for Autonomous Agents and Multiagent Systems  IFAAMAS . The AAMAS conference is a merger of three major international conferences workshops, namely International Conference on Autonomous Agents  AGENTS , International Conference on Multi Agent Systems  ICMAS , and International Workshop on Agent Theories, Architectures, and Languages  ATAL . As such, this highly respected joint conference provides a quality forum for discussing research in this area. Besides the main program that consists of a main track, an industry and applications track, and a couple of special area tracks, AAMAS also hosts over 20 workshops  e.g., AOSE, COIN, DALT, ProMAS, to mention a few  and many tutorials. There is also a demonstration session and a doctoral symposium. Finally, each year AAMAS features a bunch of awards, most notably the IFAAMAS Influential Paper Award. It publishes proceedings which are available online. 
Intrinsic motivation in the study of artificial intelligence and robotics is a mechanism for enabling artificial agents  including robots  to exhibit inherently rewarding behaviours such as exploration and curiosity, grouped under the same term in the study of psychology. Psychologists consider intrinsic motivation in humans to be the drive to perform an activity for inherent satisfaction   just for the fun or challenge of it. An intelligent agent is intrinsically motivated to act if the information content alone, or the experience resulting from the action, is the motivating factor. Information content in this context is measured in the information theoretic sense of quantifying uncertainty. A typical intrinsic motivation is to search for unusual, surprising situations  exploration , in contrast to a typical extrinsic motivation such as the search for food  homeostasis . Extrinsic motivations are typically described in artificial intelligence as task dependent or goal directed. The study of intrinsic motivation in psychology and neuroscience began in the 1950s with some psychologists explaining exploration through drives to manipulate and explore, however, this homeostatic view was criticised by White. An alternative explanation from Berlyne in 1960 was the pursuit of an optimal balance between novelty and familiarity. Festinger described the difference between internal and external view of the world as dissonance that organisms are motivated to reduce. A similar view was expressed in the  70s by Kagan as the desire to reduce the incompatibility between cognitive structure and experience. In contrast to the idea of optimal incongruity, Deci and Ryan identified in the mid 80 s an intrinsic motivation based on competence and self determination. An influential early computational approach to implement artificial curiosity in the early 1990s by Schmidhuber, has since been developed into a  Formal theory of creativity, fun, and intrinsic motivation . Intrinsic motivation is often studied in the framework of computational reinforcement learning  introduced by Sutton and Barto , where the rewards that drive agent behaviour are intrinsically derived rather than externally imposed and must be learnt from the environment. Reinforcement learning is agnostic to how the reward is generated   an agent will learn a policy  action strategy  from the distribution of rewards afforded by actions and the environment. Each approach to intrinsic motivation in this scheme is essentially a different way of generating the reward function for the agent. Intrinsically motivated artificial agents exhibit behaviour that resembles curiosity or exploration. Exploration in artificial intelligence and robotics has been extensively studied in reinforcement learning models, usually by encouraging the agent to explore as much of the environment as possible, to reduce uncertainty about the dynamics of the environment  learning the transition function  and how best to achieve its goals  learning the reward function . Intrinsic motivation, in contrast, encourages the agent to first explore aspects of the environment that confer more information, to seek out novelty. Recent work unifying state visit count exploration and intrinsic motivation has shown faster learning in a video game setting. Ouedeyer and Kaplan have made a substantial contribution to the study of intrinsic motivation. They define intrinsic motivation based on Berlyne s theory, and divide approaches to the implementation of intrinsic motivation into three categories that broadly follow the roots in psychology   knowledge based models ,  competence based models  and  morphological models . Knowledge based models are further subdivided into  information theoretic  and  predictive . Baldassare and Mirolli present a similar typology, differentiating knowledge based models between prediction based and novelty based. The quantification of prediction and novelty to drive behaviour is generally enabled through the application of information theoretic models, where agent state and strategy  policy  over time are represented by probability distributions describing a markov decision process and the cycle of perception and action treated as an information channel. These approaches claim biological feasibility as part of a family of bayesian approaches to brain function. The main criticism and difficulty of these models is the intractability of computing probability distributions over large discrete or continuous state spaces. Nonetheless a considerable body of work has built up modelling the flow of information around the sensorimotor cycle, leading to de facto reward functions derived from the reduction of uncertainty, including most notably active inference, but also infotaxis, predictive information, and empowerment. Steels  autotelic principle  is an attempt to formalise flow  psychology . Other intrinsic motives that have been modelled computationally include achievement, affiliation and power motivation. These motives can be implemented as functions of probability of success or incentive. Populations of agents can include individuals with different profiles of achievement, affiliation and power motivation, modelling population diversity and explaining why different individuals take different actions when faced with the same situation. Intrinsically motivated  or curiosity driven  learning is an emerging research topic in artificial intelligence and developmental robotics that aims to develop agents that can learn general skills or behaviours, that can be deployed to improve performance in extrinsic tasks, such as acquiring resources. Intrinsically motivated learning has been studied as an approach to autonomous lifelong learning in machines and open ended learning in computer game characters. In particular, when the agent learns a meaningful abstract representation, a notion of distance between two representations can be used to gauge novelty, hence allowing for an efficient exploration of its environment. Despite the impressive success of deep learning in specific domains  e.g. AlphaGo , many in the field  e.g. Gary Marcus  have pointed out that the ability to generalise remains a fundamental challenge in artificial intelligence. Intrinsically motivated learning, although promising in terms of being able to generate goals from the structure of the environment without externally imposed tasks, faces the same challenge of generalisation   how to reuse policies or action sequences, how to compress and represent continuous or complex state spaces and retain and reuse the salient features that have been learnt. 
 The Joint Artificial Intelligence Center  JAIC   pronounced  jake   was an American organization on exploring the usage of Artificial Intelligence  AI   particularly Edge computing , Network of Networks and AI enhanced communication for use in actual combat. In February 2022, JAIC was integrated into the Chief Digital and Artificial Intelligence Office  CDAO . A subdivision of the United States Armed Forces, it was created in June 2018. The organization s stated objective was to  transform the US Department of Defense by accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems  then, ensure the combat Systems and Components have real time access to ever improving libraries of data sets and tools.  JAIC was originally proposed to Congress on June 27, 2018  that same month, it was established under the Defense Department s Chief Information Officer  CIO , itself subordinate to the Office of the Secretary of Defense  OSD , to coordinate Department wide AI efforts. Throughout 2020, JAIC started financially engaging with the AI industry for the development of specific applications. Current proposals for JAIC include giving it the authority as a financial entity to acquire its own technology, and elevating its position to be under the Deputy Secretary of Defense. On 24 June 2021 the Department of Defense gathered reporters for an AI symposium in which it announced the launch of an  AI and data accelerator  ADA  initiative  in which, over the month of July, data teams would work directly with military personnel to provide a proof of concept in data driven warfare and to observe the possible obstacles for such implementation. On 1 June 2022 JAIC, the Defense Digital Service, and the Office of Advancing Analytics were fully merged into a unified organization, the Chief Digital and Artificial Intelligence Officer  CDAO . JAIC, DDS, and the other groups within CDAO will cease to be recognized as entities. Note  as more information becomes available, this section may be split off The Chief Digital and Artificial Intelligence Office  CDAO  or Chief Digital and Artificial Intelligence Officer  is Dr. Craig Martell.  USAF secretary Frank Kendall has signalled that the CDAO will have an approach to solving the DoD wide Joint All domain command and control  JADC2  problem   Deputy Defense Secretary Kathleen Hicks has already asked Martell to take a leading role in the discussions about JADC2 .  Martell s approach is bottom up starting with each agency, working one by one, preserving what is important for each agency. However connectivity between Nodes is currently the critical resource for JADC2. On 30 January 2023 the CDAO announced a series of global information dominance experiments  GIDEs . GIDE V is being held 30 January   3 February 2023  Monday Thursday  at the Pentagon, and at multiple combatant commands  and therefore across the global information grid for JADC2 . The experiment is twofold  1   to identify where we may have barriers in policy, security, connectivity, user interface, or other areas that prohibit data sharing across the Joint force   and 2   to show how data, analytics, and AI can improve Joint workflows in a variety of missions from global integrated deterrence through targeting and fires . Dr. Martell has expressed apprehension over the large language models of AI such as ChatGPT. The USAF has expressed interest in AI based surveillance for operations based in CENTCOM. Interest in these operations has grown from  600 million to  2.5 billion, from 2016 to 2021. JAIC s primary area of interest is edge computing, as even more sensor technologies are being added to weapon systems and military vehicles. The edge processors that will be used are neuromorphic processors that will perform neural network computations on the sensor itself without having to send the data to a central processor, thus increasing the robustness of the combat network. JAIC plans to access the U.S. commercial sector and academia to recruit professionals in the fields of neuromorphic technology and AI safety. See   data fabric Joint All Domain Command and Control  JADC2  is an initiative of the military s network of networks, as each branch of the US Armed Forces  Army, Air Force, Navy, Marines and Coast Guard  intends to have its own communications network. The JADC2 project would integrate all those networks into a larger network on all spatial scales.  Connect every sensor, every shooter , being the tagline. JADC2 confers on the US the capability to  move data globally at scale .  Gen. Chance Saltzman, US Space Force  The DoD s Cloud Based AI Development and Experimentation Platform  
A K line, or Knowledge line, is a mental agent which represents an association of a group of other mental agents found active when a subject solves a certain problem or formulates a new idea. These were first described in Marvin Minsky s essay K lines  A Theory of Memory, published in 1980 in the journal Cognitive Science  When you  get an idea,  or  solve a problem  ... you create what we shall call a K line. ... When that K line is later  activated , it reactivates ... mental agencies, creating a partial mental state  resembling the original.   Whenever you  get a good idea , solve a problem, or have a memorable experience, you activate a K line to  represent  it. A K line is a wirelike structure that attaches itself to whichever mental agents are active when you solve a problem or have a good idea. When you activate that K line later, the agents attached to it are aroused, putting you into a  mental state  much like the one you were in when you solved that problem or got that idea.  This should make it relatively easy for you to solve new, similar problems!   1998, p. 82.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.This psychology related article is a stub. You can help Wikipedia by expanding it.
KAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C s Web Ontology Language  OWL  standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government sponsored projects for distributed host and network management and for the coordination of human agent robot teams, including DARPA s CoABS Grid, Cougaar, and Common Object Request Broker Architecture  CORBA  models.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.This technology related article is a stub. You can help Wikipedia by expanding it.
KL ONE  pronounced  kay ell won   is a knowledge representation system in the tradition of semantic networks and  frames  that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network. There is a whole family of KL ONE like systems. One of the innovations that KL ONE initiated was the use of a deductive classifier, an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert.  Frames in KL ONE are called concepts. These form hierarchies using subsume relations  in the KL ONE terminology a super class is said to subsume its  subclasses.  Multiple inheritance is allowed. Actually a concept is said to be well formed only if it inherits from more than one other concept. All concepts, except the top concept  usually THING , must have at least one super class.  In KL ONE descriptions are separated into two basic classes of concepts  primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are necessary and sufficient conditions to classify the concept. The slot concept is called roles and the values of the roles are role fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler. 
Karen Hao is an American journalist and data scientist. Currently a journalist based in Hong Kong for The Wall Street Journal and previously senior artificial intelligence editor at the MIT Technology Review, she is best known for her coverage on AI research, technology ethics and the social impact of AI. Hao also co produces the podcast In Machines We Trust and writes the newsletter The Algorithm. Previously, she worked at Quartz as a tech reporter and data scientist and was an application engineer at the first startup to spin out of Google X. Hao s writing has also appeared in Mother Jones, Sierra Magazine, The New Republic, and other publications. Hao graduated from The Lawrenceville School in 2011. She studied at the Massachusetts Institute of Technology, graduating with a B.S. in mechanical engineering and a minor in energy studies in 2015. She is a native speaker in both English and Mandarin Chinese. Hao is known in the technology world for her coverage of new AI research findings and their societal and ethical impacts. Her writing has spanned research and issues regarding big tech data privacy, misinformation, deepfakes, facial recognition, and AI healthcare tools. In March 2021, Hao published a piece that uncovered previously unknown information about how attempts to combat misinformation by different teams at Facebook s using machine learning were impeded and constantly at odds by Facebook s drive to grow user engagement. Upon its release, leaders at Facebook including Mike Schroepfer and Yann LeCun immediately criticized the piece through Twitter responses. AI researchers and AI ethics experts Timnit Gebru and Margaret Mitchell responded in support of Hao s writing and advocated for more change and improvement for all. Hao also co produces the podcast In Machines We Trust, which discusses the rise of AI with people developing, researching, and using AI technologies. The podcast won the 2020 Front Page Award in investigative reporting. As a data scientist, Hao occasionally creates data visualizations that have been featured in her work at the MIT Technology Review and elsewhere. In 2018, her  What is AI?  flowchart visualization was exhibited as an installation at the Museum of Applied Arts in Vienna. She has been an invited speaker at TEDxGateway, the United Nations Foundation, EmTech, WNPR, and many other conferences and podcasts. Her TEDx talk discussed the importance of democratizing how AI is built. In March 2022, she was hired by The Wall Street Journal to cover China technology and society, while being based in Hong Kong. 
Knowledge compilation is a family of approaches for addressing the intractability of a number of artificial intelligence problems. A propositional model is compiled in an off line phase in order to support some queries in polynomial time. Many ways of compiling a propositional models exist. Different compiled representations have different properties. The three main properties are  Some examples of diagram classes include OBDDs, FBDDs, and non deterministic OBDDs, as well as MDD. Some examples of formula classes include DNF and CNF. Examples of circuit classes include NNF, DNNF, d DNNF, and SDD.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
In artificial intelligence, knowledge based agents draw on a pool of logical sentences to infer conclusions about the world.  At the knowledge level, we only need to specify what the agent knows and what its goals are  a logical abstraction separate from details of implementation. This notion of knowledge level was first introduced by Allen Newell in the 1980s, to have a way to rationalize an agent s behavior.  The agent takes actions based on knowledge it possesses, in an attempt to reach specific goals.  It chooses actions according to the principle of rationality. Beneath the knowledge level resides the symbol level.  Whereas the knowledge level is world oriented, namely that it concerns the environment in which the agent operates, the symbol level is system oriented, in that it includes the mechanisms the agent has available to operate.  The knowledge level rationalizes the agent s behavior, while the symbol level mechanizes the agent s behavior. For example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions.  The symbol level consists of the program s algorithms, the data structures themselves, and so on. 
Knowledge based configuration, or also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge based configuration is a major application area for artificial intelligence  AI , and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer. Knowledge based configuration  of complex products and services  has a long history as an artificial intelligence application area, see, e.g. Informally, configuration can be defined as a  special case of design activity, where the artifact being configured is assembled from instances of a fixed set of well defined component types which can be composed conforming to a set of constraints . Such constraints are representing technical restrictions, restrictions related to economic aspects, and conditions related to production processes. The result of a configuration process is a product configuration  concrete configuration , i.e., a list of instances and in some cases also connections between these instances. Examples of such configurations are computers to be delivered or financial service portfolio offers  e.g., a combination of loan and corresponding risk insurance . Numerous practical configuration problems can be analyzed by the theoretical framework of Najmann and Stein, an early axiomatic approach which does not presuppose any particular knowledge representation formalism. One important result of this methodology is that typical optimization problems  e.g. finding a cost minimal configuration  are NP complete. Thus they require  potentially  excessive computation time making heuristic configuration algorithms the preferred choice for complex artifacts  products, services . Configuration systems or also referred to as configurators or mass customization toolkits, are one of the most successfully applied artificial intelligence technologies. Examples are the automotive industry, the telecommunication industry, the computer industry, and power electric transformers. Starting with rule based approaches such as R1 XCON, model based representations of knowledge  in contrast to rule based representations  have been developed which strictly separate product domain knowledge from the problem solving one   examples thereof are the constraint satisfaction problem, the Boolean satisfiability problem, and different answer set programming  ASP  representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions.  This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies and vice versa. Configurators are also often considered as  open innovation toolkits , i.e., tools which support customers in the product identification phase. In this context customers are innovators who articulate their requirements leading to new innovative products.   Mass Confusion     the overwhelming of customers by a large number of possible solution alternatives  choices    is a phenomenon which often comes with the application of configuration technologies. This phenomenon motivated the creation of personalized configuration environments taking into account a customer s knowledge and preferences. Core configuration, i.e., guiding the user and checking the consistency of user requirements with the knowledge base, solution presentation and translation of configuration results into bill of materials  BOM  are major tasks to be supported by a configurator. Configuration knowledge bases are often built using proprietary languages. In most cases knowledge bases are developed by knowledge engineers who elicit product, marketing and sales knowledge from domain experts. Configuration knowledge bases are composed of a formal description of the structure of the product and further constraints restricting the possible feature and component combinations. Configurators known as characteristic based product configurators use sets of discrete variables that are either binary or have one of several values, and these variables define every possible product variation. Recently, knowledge based configuration has been extended to service and software configuration. Modeling software configuration has been based on two main approaches  feature modeling, and component connectors. Kumbang domain ontology combines the previous approaches building on the tradition of knowledge based configuration. 
Knowledge based recommender systems  knowledge based recommenders   are a specific type of recommender system that are based on explicit knowledge about the item assortment, user preferences, and recommendation criteria  i.e., which item should be recommended in which context . These systems are applied in scenarios where alternative approaches such as collaborative filtering and content based filtering cannot be applied. A major strength of knowledge based recommender systems is the non existence of cold start  ramp up  problems. A corresponding drawback is a potential knowledge acquisition bottleneck triggered by the need to define recommendation knowledge in an explicit fashion. Knowledge based recommender systems are well suited to complex domains where items are not purchased very often, such as apartments and cars. Further examples of item domains relevant for knowledge based recommender systems are financial services, digital cameras, and tourist destinations. Rating based systems often do not perform well in these domains due to the low number of available ratings. Additionally, in complex item domains, customers want to specify their preferences explicitly  e.g.,  the maximum price of the car is X   . In this context, the recommender system must take into account constraints  for instance, only those financial services that support the investment period specified by the customer should be recommended. Neither of these aspects are supported by approaches such as collaborative filtering and content based filtering. Knowledge based recommender systems are often conversational, i.e., user requirements and preferences are elicited within the scope of a feedback loop. A major reason for the conversational nature of knowledge based recommender systems is the complexity of the item domain where it is often impossible to articulate all user preferences at once. Furthermore, user preferences are typically not known exactly at the beginning but are constructed within the scope of a recommendation session. In a search based recommender, user feedback is given in terms of answers to questions which restrict the set of relevant items. An example of such a question is  Which type of lens system do you prefer  fixed or exchangeable lenses? . On the technical level, search based recommendation scenarios can be implemented on the basis of constraint based recommender systems. Constraint based recommender systems are implemented on the basis of constraint search  or different types of conjunctive query based approaches. In a navigation based recommender, user feedback is typically provided in terms of  critiques   which specify change requests regarding the item currently recommended to the user. Critiques are then used for the recommendation of the next  candidate  item. An example of a critique in the context of a digital camera recommendation scenario is  I would like to have a camera like this but with a lower price . This is an example of a  unit critique   which represents a change request on a single item attribute.  Compound critiques   allow the specification of more than one change request at a time.  Dynamic critiquing   also takes into account preceding user critiques  the critiquing history . More recent approaches additionally exploit information stored in user interaction logs to further reduce the interaction effort in terms of the number of needed critiquing cycles.  
A knowledge based system  KBS  is a computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and refers to many different kinds of systems. The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly and a reasoning system that allows it to derive new knowledge. Thus, a knowledge based system has two distinguishing features  a knowledge base and an inference engine.  The first part, the knowledge base, represents facts about the world, often in some form of subsumption ontology  rather than implicitly embedded in procedural code, in the way a conventional computer program does . Other common approaches in addition to a subsumption ontology include frames, conceptual graphs, and logical assertions. The second part, the inference engine, allows new knowledge to be inferred. Most commonly, it can take the form of IF THEN rules coupled with forward chaining or backward chaining approaches. Other approaches include the use of automated theorem provers, logic programming, blackboard systems, and term rewriting systems such as CHR  Constraint Handling Rules . These more formal approaches are covered in detail in the Wikipedia article on knowledge representation and reasoning. Knowledge based systems were first developed by artificial intelligence researchers. These early knowledge based systems were primarily expert systems   in fact, the term is often used interchangeably with expert systems, although there is a difference. The difference is in the view taken to describe the system  While the earliest knowledge based systems were almost all expert systems, the same tools and architectures can and have since been used for a whole host of other types of systems. Virtually all expert systems are knowledge based systems, but many knowledge based systems are not expert systems. The first knowledge based systems were rule based expert systems. One of the most famous was Mycin, a program for medical diagnosis. These early expert systems represented facts about the world as simple assertions in a flat database, and used rules to reason about  and as a result add to  these assertions. Representing knowledge explicitly via rules had several advantages  Later architectures for knowledge based reasoning, such as the BB1 blackboard architecture  a blackboard system , allowed the reasoning process itself to be affected by new inferences, providing meta level reasoning. BB1 allowed the problem solving process itself to be monitored. Different kinds of problem solving  e.g., top down, bottom up, and opportunistic problem solving  could be selectively mixed based on the current state of problem solving. Essentially, the problem solver was being used both to solve a domain level problem along with its own control problem, which could depend on the former. Other examples of knowledge based system architectures supporting meta level reasoning are MRS and SOAR. In addition to expert systems, other applications of knowledge based systems include real time process control, intelligent tutoring systems, and problem solvers for specific domains such as protein structure analysis, construction site layout, and computer system fault diagnosis. As knowledge based systems became more complex, the techniques used to represent the knowledge base became more sophisticated and included logic, term rewriting systems, conceptual graphs, and frames. Consider frames as an example. Rather than representing facts as assertions about data, the knowledge base has become more structured. Frames can be thought of as representing world knowledge using analogous techniques to object oriented programming, specifically the use of hierarchies of classes and subclasses, relations between classes, and behavior of objects. As the knowledge base became more structured, reasoning could occur both by independent rules, logical inference, and by interactions within the knowledge base itself. For example, procedures stored as daemons on objects could fire and could replicate the chaining behavior of rules. Another advancement was the development of special purpose automated reasoning systems called classifiers. Rather than statically declare the subsumption relations in a knowledge base, a classifier allows the developer to simply declare facts about the world and let the classifier deduce the relations. In this way a classifier also can play the role of an inference engine. The most recent advancement of knowledge based systems has been to adopt the technologies, especially a kind of logic called description logic, for the development of systems that use the internet.  The internet often has to deal with complex, unstructured data that cannot be relied on to fit a specific data model. The technology of knowledge based systems, and especially the ability to classify objects on demand, is ideal for such systems. The model for these kinds of knowledge based Internet systems is known as the Semantic Web. 
LangChain is a framework designed to simplify the creation of applications using large language models  LLMs . As a language model integration framework, LangChain s use cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. LangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project s Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, the new startup raised over  20 million in funding at a valuation of at least  200 million from venture firm Sequoia Capital, a week after announcing a  10 million seed investment from Benchmark. As of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage  API wrappers for news, movie information, and weather  Bash for summarization, syntax and semantics checking, and execution of shell scripts  multiple web scraping subsystems and templates  few shot learning prompt generation support  finding and summarizing  todo  tasks in code  Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation  Google Search and Microsoft Bing web search  OpenAI, Anthropic, and Hugging Face language models  iFixit repair guides and wikis search and summarization  MapReduce for question answering, combining documents, and question generation  N gram overlap scoring  PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation  Python and JavaScript code generation, analysis, and debugging  Weaviate vector database to cache embedding and data objects  Redis cache database storage  Python RequestsWrapper and other methods for API requests  SQL and NoSQL databases including JSON support  Streamlit, including for logging  text mapping for k nearest neighbors search  time zone conversion and calendar operations  tracing and recording stack symbols in threaded and asynchronous subprocess runs  and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources. 
The language action perspective  takes language as the primary dimension of human cooperative activity,  applied not just in person to person direct  face to face  interactions, but also in the design of systems mediated by information and communication technology.  The perspective was developed in the joint authorship of Understanding Computers and Cognition by Fernando Flores and Terry Winograd in 1987. As part of a reflection published in 2006, Terry Winograd describes the language action perspective as resting on two key orienting principles  Language is action argues that speech isn t simply composed of assertions about the situation  utterances may also create a situation, such as,  Let s go to the park.  That utterance may be subject to interpretation but is not verifiable via the state of the world. This principle is closely linked to the ideas from phenomenology. Furthermore, language is not the transmission of information, which simply correspond to the state of the world. By creating a situation, language forms a consensual domain to further encourage more action through language. These speech acts may often take the form of commitment to other actions. In the design of information systems, the perspective is based upon the notion as proposed by Terry Winograd that information technology may be limited in its ability to improve human communication.   Expert behavior requires an exquisite sensitivity to context and an ability to know what to commit to. Computing machines, which are purposely designed to process symbols independent of their context, have no hope of becoming experts. .  That sensitivity to context is thus more in the realm of the human than in that of the artificial. Research on LAP was done in the Advanced Technology Group  ATG  at Apple Computer in the late 1980s.  Winograd was invited to present the basic concepts in a seminar at Apple in the winter of 1988.  Some Apple ATG researchers, notably Tom Pittard and Brad Hartfield, saw potential for enhancing the user experience of network based computer interactions if LAP was included in the mix of basic design considerations. Research on the application of LAP to business process modelling was done in the System Modelling Research Group, Faculty of Computing, Engineering and Mathematical Sciences, University of the West of England in the early 2000s. Insights from related work have been applied over the past two decades. At the LAP 2004   Conference, Kalle Lyytinen discussed the academic theoretic success of LAP. Yet, these LAP successes have not found entry into the wider stream of applications. In a sense, LAP is now peripheral to computer science, however there may be a need for a deeper look at this viewpoint. LAP played a role in the second AI Winter. At the time, symbolic AI tried to represent intelligence using a growing knowledge base represented as facts in language. The LAP argued that language was not simply a correspondence with facts but instead depended upon the contextual domain and could not be rigidly defined. Even Winograd s SHRDLU, an exemplar of language understanding in AI, was incapable of broadening its understanding beyond the blocks world. 
LPA  or Lifelong Planning A  is an incremental heuristic search algorithm based on A . It was first described by Sven Koenig and Maxim Likhachev in 2001. LPA  is an incremental version of A , which can adapt to changes in the graph without recalculating the entire graph, by updating the g values  distance from start  from the previous search during the current search to correct them when necessary. Like A , LPA  uses a heuristic, which is a lower boundary for the cost of the path from a given node to the goal. A heuristic is admissible if it is guaranteed to be non negative  zero being admissible  and never greater than the cost of the cheapest path to the goal. With the exception of the start and goal node, each node n has predecessors and successors  In the following description, these two terms refer only to the immediate predecessors and successors, not to predecessors of predecessors or successors of successors. LPA  maintains two estimates of the start distance g  n  for each node  For the start node, the following always holds true  If rhs n  equals g n , then n is called locally consistent. If all nodes are locally consistent, then a shortest path can be determined as with A . However, when edge costs change, local consistency needs to be re established only for those nodes which are relevant for the route. When a node becomes locally inconsistent  because the cost of its predecessor or the edge linking it to a predecessor has changed , it is placed in a priority queue for re evaluation. LPA  uses a two dimensional key  Entries are ordered by k1  which corresponds directly to the f values used in A  , then by k2. The top node in the queue is expanded as follows  Since changing the g value of a node may also change the rhs values of its successors  and thus their local consistence , they are evaluated and their queue membership and key is updated if necessary. Expansion of nodes continues with the next node at the top of the queue until two conditions are met  The graph is initialized by setting the rhs value of the start node to 0 and its g value to infinity. For all other nodes, both the g value and the rhs value are assumed to be infinity until assigned otherwise. This initially makes the start node the only locally inconsistent node, and thus the only node in the queue. After that, node expansion begins. The first run of LPA  thus behaves in the same manner as A , expanding the same nodes in the same order. When the cost of an edge changes, LPA  examines all nodes affected by the change, i.e. all nodes at which one of the changed edges terminates  if an edge can be traversed in both directions and the change affects both directions, both nodes connected by the edge are examined   After that, node expansion resumes until the end condition has been reached. Once node expansion has finished  i.e. the exit conditions are met , the shortest path is evaluated. If the cost for the goal equals infinity, there is no finite cost path from start to goal. Otherwise, the shortest path can be determined by moving backwards  This code assumes a priority queue queue, which supports the following operations   Being algorithmically similar to A , LPA  shares many of its properties. For an A  implementation which breaks ties between two nodes with equal f values in favor of the node with the smaller g value  which is not well defined in A  , the following statements are also true  LPA  additionally has the following properties  
 Artificial intelligence researchers have developed several specialized programming languages for artificial intelligence  
Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans. Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical. The end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing. This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self awareness, and intentionality. Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and high dimensional data from the real world to produce numerical or symbolic information, e.g., in the forms of decisions.  Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment. However, machines still struggle to interpret visual impute accurately if said impute is blurry, and if the viewpoint at which stimulus are viewed varies often. Computers also struggle to determine the proper nature of some stimulus if overlapped by or seamlessly touching another stimulus. This refers to The Principle of Good Continuation. Machines also struggle to perceive and record stimulus functioning according to the Apparent Movement principle which Gestalt psychologists researched. Machine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as speech or music. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition. Moreover, this technology allows the machine to replicate the human brain s ability to selectively focus on a specific sound against many other competing sounds and background noise. This particular ability is called  auditory scene analysis . The technology enables the machine to segment several streams occurring at the same time. Many commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing. Present technology still occasionally struggles with speech segmentation though. This means hearing words within sentences, especially when human accents are accounted for. Machine touch is an area of machine perception where tactile information is processed by a machine or computer.  Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment.  This could possibly be done through measuring when and where friction occurs, and of what nature and intensity the friction is . Machines however still do not have any way of measuring some physical human experiences we consider ordinary, including physical pain. For example, scientists have yet to invent a mechanical substitute for the Nociceptors in the body and brain that are responsible for noticing and measuring physical human discomfort and suffering. Scientists are developing computers known as machine olfaction which can recognize and measure smells as well. Airborne chemicals are sensed and classified with a device sometimes known as an electronic nose. The electronic tongue is an instrument that measures and compares tastes. As per the IUPAC technical report, an  electronic tongue  as analytical instrument including an array of non selective chemical sensors with partial specificity to different solution components and an appropriate pattern recognition instrument, capable to recognize quantitative and qualitative compositions of simple and complex solutions Chemical compounds responsible for taste are detected by human taste receptors. Similarly, the multi electrode sensors of electronic instruments detect the same dissolved organic and inorganic compounds. Like human receptors, each sensor has a spectrum of reactions different from the other. The information given by each sensor is complementary, and the combination of all sensors  results generates a unique fingerprint. Most of the detection thresholds of sensors are similar to or better than human receptors. In the biological mechanism, taste signals are transduced by nerves in the brain into electric signals. E tongue sensors process is similar  they generate electric signals as voltammetric and potentiometric variations. Other than those listed above, some of the future hurdles that the science of machine perception still has to overcome include, but are not limited to    Embodied cognition   The theory that cognition is a full body experience, and therefore can only exist, and therefore be measure and analyzed, in fullness if all required human abilities and processes are working together through a mutually aware and supportive systems network.   The Moravec s paradox  see the link    The Principle of similarity   The ability young children develop to determine what family a newly introduced stimulus falls under even when the said stimulus is different from the members with which the child usually associates said family with.  An example could be a child figuring that a chihuahua is a dog and house pet rather than vermin.    The Unconscious inference  The natural human behavior of determining if a new stimulus is dangerous or not, what it is, and then how to relate to it without ever requiring any new conscious effort.   The innate human ability to follow the likelihood principle in order to learn from circumstances and others over time.   The recognition by components theory   being able to mentally analyze and break even complicated mechanisms into manageable parts with which to interact with. For example  A person seeing both the cup and the handle parts that make up a mug full of hot cocoa, in order to use the handle to hold the mug so as to avoid being burned.   The free energy principle   determining long before hand how much energy one can safely delegate to being aware of things outside one s self without the loss of the needed energy one requires for sustaining their life and function satisfactorily. This allows one to become both optimally aware of the world around them self without depleting their energy so much that they experience damaging stress, decision fatigue, and or exhaustion. 
Means ends analysis  MEA  is a problem solving technique used commonly in artificial intelligence  AI  for limiting search in AI  programs. It is also a technique used at least since the 1950s as a creativity tool, most frequently mentioned in engineering books on design methods. MEA is also related to means ends chain approach used commonly in consumer behavior analysis. It is also a way to clarify one s thoughts when embarking on a mathematical proof. An important aspect of intelligent behavior as studied in AI is goal based problem solving, a framework in which the solution to a problem can be described by finding a sequence of actions that lead to a desirable goal.  A goal seeking system is supposed to be connected to its outside environment by sensory channels through which it receives information about the environment and motor channels through which it acts on the environment.   The term  afferent  is used to describe  inward  sensory flows, and  efferent  is used to describe  outward  motor commands.  In addition, the system has some means of storing in a memory information about the state of the environment  afferent information  and information about actions  efferent information .  Ability to attain goals depends on building up associations, simple or complex, between particular changes in states and particular actions that will bring these changes about.  Search is the process of discovery and assembly of sequences of actions that will lead from a given state to a desired state. While this strategy may be appropriate for machine learning and problem solving, it is not always suggested for humans  e.g. cognitive load theory and its implications . The MEA technique is a strategy to control search in problem solving.  Given a current state and a goal state, an action is chosen which will reduce the difference between the two.  The action is performed on the current state to produce a new state, and the process is recursively applied to this new state and the goal state. Note that, in order for MEA to be effective, the goal seeking system must have a means of associating to any kind of detectable difference those actions that are relevant to reducing that difference.  It must also have means for detecting the progress it is making  the changes in the differences between the actual and the desired state , as some attempted sequences of actions may fail and, hence, some alternate sequences may be tried. When knowledge is available concerning the importance of differences, the most important difference is selected first to further improve the average performance of MEA over other brute force search strategies. However, even without the ordering of differences according to importance, MEA improves over other search heuristics  again in the average case  by focusing the problem solving on the actual differences between the current state and that of the goal. The MEA technique as a problem solving strategy was first introduced in 1961 by Allen Newell and Herbert A. Simon in their computer problem solving program General Problem Solver  GPS .  In that implementation, the correspondence between differences and actions, also called operators, is provided a priori as knowledge in the system.  In GPS this knowledge was in the form of table of connections.  When the action and side effects of applying an operator are penetrable the search may select the relevant operators by inspection of the operators and do without a table of connections.  This latter case, of which the canonical example is STRIPS, an automated planning computer program, allows task independent correlation of differences to the operators which reduce them. Prodigy, a problem solver developed in a larger learning assisted automated planning project started at Carnegie Mellon University by Jaime Carbonell, Steven Minton and Craig Knoblock, is another system that used MEA. Professor Morten Lind, at Technical University of Denmark has developed a tool called Multilevel Flow Modeling  MFM . It performs means ends based diagnostic reasoning for industrial control and automation systems. 
 Michael J. Apter  born 1939  is a British psychologist who was born in England and grew up in Bristol. He was educated at Clifton College  1965  and at Bristol University where he gained both his Bachelor of Science degree and his Doctorate in Psychology in 1965, having also spent a doctoral year at Princeton University. He taught for twenty years at Cardiff University in Wales and has since held invited positions at Purdue University, the University of Chicago, Yale University, University of Toulouse, and Georgetown University. He also taught at Northwestern University where he received a teaching award. He has held visiting positions at several additional universities and is a chartered psychologist and fellow of the British Psychological Society. Apter s work has been mainly in personality, in particular reversal theory. Since 2011 Apter has been Key Principal of Apter Research LLC in Louisiana, United States. 
Microsoft 365 Copilot is an artificial intelligence assistant feature for Microsoft 365 applications and services. Announced by Microsoft on March 16, 2023, the tool builds on OpenAI s advanced GPT 4 large language models  LLMs  and incorporates Microsoft Graph in order to convert user text input into content in Microsoft 365 apps, such as Word, Excel, PowerPoint, Outlook, and Teams.  Copilot is being marketed with a focus on productivity for its users, with 20 initial testers as of March 16, 2023. In May 2023, Microsoft expanded access to 600 customers willing to pay for early access, with the office apps and services getting new Copilot features. Although there are public concerns about the chatbot, including hallucinations and racial or gender bias, experts believe that Copilot may change the way that Microsoft users work and collaborate. Microsoft announced during the Build 2023 conference that it is adding the AI assistant Copilot to Windows 11. The Windows Copilot service is directly integrated with Windows 11 and users can access it through the taskbar. According to Jared Spataro, the head of Microsoft 365, Copilot utilizes Microsoft Graph, an API that evaluates the context and available Microsoft 365 user data before modifying and sending the user prompt to the LLM. After receiving the response from the LLM, Microsoft Graph performs additional context specific processing before sending it to Microsoft 365 apps to generate actual content. According to Microsoft, Copilot can be used to generate and edit text in Word documents based on user prompts. Users can also ask Copilot to push rewrite suggestions that strengthen the arguments of highlighted texts. The company also claims that Copilot can assist users with data analysis in Excel spreadsheets by formatting data, creating graphs, generating PivotTables, identifying trends, and summarizing information. Copilot can also guide users using Excel commands and can suggest formulas to investigate user questions. Copilot, according to Microsoft, is able to create PowerPoint presentations that summarize information from user selected Word documents and Excel spreadsheets or a user prompt. Additionally, this tool can adjust the presentation style, text formatting, and animation timing based on user prompts to eliminate the need for the user to make manual changes. Copilot is also able to shorten lengthy presentations. In Outlook, Microsoft claims that Copilot can draft emails with varying length and tone based on user input. To draft these emails, Copilot can pull relevant information from other emails. Copilot is also able to summarize content from email threads, noting the viewpoints of individuals involved in the email threads and pointing out questions posed by others that have yet to be answered. Microsoft also states that Copilot can be used in Teams to present information for upcoming meetings, transcribe meetings, and provide debriefs if users join the meeting late. After the meeting, Copilot can also summarize discussion points, list key actions deliberated in the meeting, and answer questions that were covered in the meeting. In addition to reporting the implementation of Copilot into Microsoft 365 apps, Microsoft has also publicly introduced Business Chat  a chat interface that pulls information from content across all Microsoft 365 apps, including documents, presentations, emails, calendars, and notes, to answer user questions and perform other tasks. For example, Copilot can summarize content, extract information, and organize action plans based on the information pooled OneNote will also use prompts to draft plans, generate ideas, create lists and organize information to help customers find what they need easily. Viva Learning will use a natural language chat interface to help users create a personalized learning journey including designing upskilling paths, discovering relevant learning resources ,and scheduling time for assigned trainings. Copilot is being marketed as an added feature to Microsoft 365, with an emphasis on business productivity. With the use of Copilot, Microsoft emphasizes the promotion of the user s creativity and productivity by having the chatbot do more tedious work, like collecting information. Microsoft has also demonstrated Copilot s accessibility on the mobile version of Outlook to generate or summarize emails with a mobile device. As of May 2023, Microsoft is testing the chatbot with 600 paying customers. Pricing and licensing details are not yet known. Tom Warren, a senior editor at The Verge, has noted the conceptual similarity of Copilot and other Microsoft assistant features like Cortana and Clippy. As large language models develop, Warren also believes that Copilot and Microsoft 365 will shift how users work and collaborate. Rowan Curran, an analyst at Forrestor, notes that the integration of an AI like Copilot can smooth out the user experience, as they will not have to use an external tool to perform tasks like summarizing a paper. Concerns over the speed of Microsoft s recent release of AI powered products and investments have led to questions surrounding ethical responsibilities in the testing of such products. One ethical concern the public has vocalized is that the large language model used by Copilot may reinforce racial or gender bias. Individuals, including Tom Warren, have also voiced concerns for Copilot after witnessing Microsoft s Bing chatbot showcasing several instances of artificial hallucinations. In response to these concerns, Jon Friedman, the Corporate Vice President of Design and Research at Microsoft, has emphasized Microsoft s dedication to learning from their experiences with Bing and responsibly develop Copilot. Microsoft has claimed that they are gathering a team of researchers and engineers to identify and alleviate any potential negative impacts. This will be achieved through the refinement of training data, blocking queries about sensitive topics, and limiting harmful information. The company also stated that it intends to employ InterpretMl aLd Failure to detect and rectify data bias, provide links to its sources, and state any applicable constraints. 
Mindpixel was a web based collaborative artificial intelligence project which aimed to create a knowledgebase of millions of human validated true false statements, or probabilistic propositions. It ran from 2000 to 2005. Participants in the project created one line statements which aimed to be objectively true or false to 20 other anonymous participants. In order to submit their statement they had first to check the true false validity of 20 such statements submitted by others. Participants whose replies were consistently out of step with the majority had their status downgraded and were eventually excluded. Likewise, participants who made contributions which others could not agree were objectively true or false had their status downgraded.  A validated  true false statement is called a mindpixel. The project enlisted the efforts of thousands of participants and claimed to be  the planet s largest artificial intelligence effort . The project was conceived by Chris McKinstry, a computer scientist and former Very Large Telescope operator for the European Southern Observatory in Chile, as MISTIC  Minimum Intelligent Signal Test Item Corpus  in 1996. Mindpixel was developed out of this program, and started in 2000 and had 1.4 million mindpixels in January 2004. The database and its software is known as GAC, which stands for  Generic Artificial Consciousness  and is pronounced Jak.  McKinstry believed that the Mindpixel database could be used in conjunction with a neural net to produce a body of human  common sense  knowledge which would have market value. Participants in the project were promised shares in any future value according to the number of mindpixels they had successfully created. On 20 September 2005 Mindpixel lost its free server and is no longer operational. It was being rewritten by Chris McKinstry as Mindpixel 2 and was intended to appear on a new server in France. Chris McKinstry died of suicide on 23 January 2006 and the future of the project and the integrity of the data is uncertain. The mindpixel.com domain currently points to an IQ test web site. Some Mindpixel data have been utilized by Michael Spivey of Cornell University and Rick Dale of The University of Memphis to study theories of high level reasoning and continuous temporal dynamics of thought. McKinstry, along with Dale and Spivey, designed an experiment that has now been published in Psychological Science in its January, 2008 issue. In this paper, McKinstry  as posthumous first author , Dale, and Spivey use a very small and carefully selected set of Mindpixel statements to show that even high level thought processes like decision making can be revealed in the nonlinear dynamics of bodily action. Other similar AI driven knowledge acquisition projects are Never Ending Language Learning and Open Mind Common Sense  run by MIT , the latter being also hampered when its director died of suicide. 
The Mivar based approach is a mathematical tool for designing artificial intelligence  AI  systems. Mivar  Multidimensional Informational Variable Adaptive Reality  was developed by combining production  and Petri nets. The Mivar based approach was developed for semantic analysis and adequate representation of humanitarian epistemological and axiological principles in the process of developing artificial intelligence. The Mivar based approach incorporates computer science, informatics and discrete mathematics, databases, expert systems, graph theory, matrices and inference systems. The Mivar based approach involves two technologies  Mivar networks allow us to develop cause effect dependencies   If then   and create an automated, trained, logical reasoning system. Representatives of Russian association for artificial intelligence  RAAI    for example, V. I. Gorodecki, doctor of technical science, professor at SPIIRAS and V. N. Vagin, doctor of technical science, professor at MPEI declared that the term is incorrect and suggested that the author should use standard terminology. While working in the Russian Ministry of Defense, O. O. Varlamov started developing the theory of  rapid logical inference  in 1985. He was analyzing Petri nets and productions to construct algorithms. Generally, mivar based theory represents an attempt to combine entity relationship models and their problem instance   semantic networks and Petri networks. The abbreviation MIVAR was introduced as a technical term by O. O. Varlamov, Doctor of Technical Science, professor at Bauman MSTU in 1993 to designate a  semantic unit  in the process of mathematical modeling. The term has been established and used in all of his further works. The first experimental systems operating according to mivar based principles were developed in 2000. Applied mivar systems were introduced in 2015. Mivar is the smallest structural element of discrete information space. Object Property Relation  VSO  is a graph, the nodes of which are concepts and arcs are connections between concepts. Mivar space represents a set of axes, a set of elements, a set of points of space and a set of values of points.     A      a  n     , n   1 ,   , N ,     ,n 1, ldots ,N,    where  Then         a  n      F  n        f    n i   n       , n   1 ,   , N ,  i  n     1 ,   ,  I  n   ,    exists F       ,n 1, ldots ,N,i  1, ldots ,I ,    where       F  n         sets form multidimensional space      M    F  1      F  2          F  n   .    times F  times  cdots  times F .        m      i  1   ,  i  2   ,   ,  i  N     ,   ,i , ldots ,i  ,    where  There is a set of values of multidimensional space points of     M       where  For every point of space     M      there is a single value from      C  M         set or there is no such value. Thus,     C  M         is a set of data model state changes represented in multidimensional space. To implement a transition between multidimensional space and set of points values the relation            has been introduced       C  x          M  x     ,     mu  M  ,    where   To describe a data model in mivar information space it is necessary to identify three axes  These sets are independent. The mivar space can be represented by the following tuple  Thus, mivar is described by      V S O       formula, in which      V       denotes an object or a thing,      S       denotes properties,      O       variety of relations between other objects of a particular subject domain. The category  Relations  can describe dependencies of any complexity level  formulae, logical transitions, text expressions, functions, services, computational procedures and even neural networks. A wide range of capabilities complicates description of modeling interconnections, but can take into consideration all the factors. Mivar computations use mathematical logic. In a simplified form they can be represented as implication in the form of an  if , then    formula. The result of mivar modeling can be represented in the form of a bipartite graph binding two sets of objects  source objects and resultant objects. Mivar network is a method for representing objects of the subject domain and their processing rules in the form of a bipartite directed graph consisting of objects and rules. A Mivar network is a bipartite graph that can be described in the form of a two dimensional matrix, in that records information about the subject domain of the current task. Generally, mivar networks provide formalization and representation of human knowledge in the form of a connected multidimensional space. That is, a mivar network is a method of representing a piece of mivar space information in the form of a bipartite, directed graph. The mivar space information is formed by objects and connections, which in total represent the data model of the subject domain. Connections include rules for objects processing. Thus, a mivar network of a subject domain is a part of the mivar space knowledge for that domain. The graph can consist of objects variables and rules procedures. First, two lists are made that form two nonintersecting partitions  the list of objects and the list of rules. Objects are denoted by circles. Each rule in a mivar network is an extension of productions, hyper rules with multi activators or computational procedures. It is proved that from the perspective of further processing, these formalisms are identical and in fact are nodes of the bipartite graph, denoted by rectangles. Mivar networks can be implemented on single computing systems or service oriented architectures. Certain constraints restrict their application, in particular, the dimension of matrix of linear matrix method for determining logical inference path on the adaptive rule networks. The matrix dimension constraint is due to the fact that implementation requires sending a general matrix to multiple processors. Since every matrix value is initially represented in symbol form, the amount of sent data is crucial when obtaining, for example, 10000 rules variables. Classical mivar based method requires storing three values in each matrix cell  The analysis of possibility of firing a rule is separated from determining output variables according to stages after firing the rule. Consequently, it is possible to use different matrices for  search for fired rules  and  setting values for output variables . This allowsthe use of multidimensional binary matrices. Binary matrix fragments occupy much less space and improve possibilities of applying mivar networks. To implement logical and computational data processing the following should be done. First, a formalized subject domain description is developed. The main objects variables and rules procedures are specified on the basis of mivar based approach and then corresponding lists of  objects  and  rules  are formed. This formalized representation is analogous to the bipartite logical network graph. The main stages of mivar based information processing are  The first stage is the stage of synthesis of conceptual subject domain model and its formalization in the form of production rules with a transition to mivar rules.  Input objects   rules procedures   output objects . Currently, this stage is the most complex and requires involvement of a human expert to develop a mivar model of the subject domain. Automated solution algorithm construction or logical inference is implemented at the second stage. Input data for algorithm construction are  mivar matrix of subject domain description and a set input of object variables and required object variables. The solution is implemented at the third stage. Firstly, the matrix is constructed. Matrix analysis determines whether a successful inference path exists. Then possible logical inference paths are defined and at the last stage the shortest path is selected according to the set optimality criteria. Let     m      rules and     n      variables be included in the rules as input variables activating them or as  output variables. Then, matrix     V   m   n       , each row of which corresponds to one of the rules and contains the information about variables used in the rule, can represent all the interconnections between rules and variables.  One row and one column are added in the matrix     V      to store service information.  So, the matrix     V      of dimension       m   n       n   1       , is obtained, which shows the whole structure of the source rule network. The structure of this logical network can change, that is, this is a network of rules with evolutionary dynamics. To search for a logical inference path the following actions are implemented  
The National Security Commission on Artificial Intelligence  NSCAI  was an independent commission of the United States of America established in 2018 to make recommendations to the President and Congress to  advance the development of artificial intelligence, machine learning, and associated technologies to comprehensively address the national security and defense needs of the United States . It issued its final report in March 2021, saying that the U.S. is not sufficiently prepared to defend or compete against China in the AI era. Here is a list of members from the National Security Commission on Artificial Intelligence  The report s recommendations include  
Neural architecture search  NAS  is a technique for automating the design of artificial neural networks  ANN , a widely used model in the field of machine learning. NAS has been used to design networks that are on par or outperform hand designed architectures. Methods for NAS can be categorized according to the search space, search strategy and performance estimation strategy used  NAS is closely related to hyperparameter optimization and meta learning and is a subfield of automated machine learning  AutoML . Reinforcement learning  RL  can underpin a NAS search strategy. Barret Zoph and Quoc Viet Le applied NAS with RL targeting the CIFAR 10 dataset and achieved a network architecture that rivals the best manually designed architecture for accuracy, with an error rate of 3.65, 0.09 percent better and 1.05x faster than a related hand designed model. On the Penn Treebank dataset, that model composed a recurrent cell that outperforms LSTM, reaching a test set perplexity of 62.4, or 3.6 perplexity better than the prior leading system. On the PTB character language modeling task it achieved bits per character of 1.214. Learning a model architecture directly on a large dataset can be a lengthy process. NASNet addressed this issue by transferring a building block designed for a small dataset to a larger dataset. The design was constrained to use two types of convolutional cells to return feature maps that serve two main functions when convoluting an input feature map  normal cells that return maps of the same extent  height and width  and reduction cells in which the returned feature map height and width is reduced by a factor of two. For the reduction cell, the initial operation applied to the cell s inputs uses a stride of two  to reduce the height and width . The learned aspect of the design included elements such as which lower layer s  each higher layer took as input, the transformations applied at that layer and to merge multiple outputs at each layer. In the studied example, the best convolutional layer  or  cell   was designed for the CIFAR 10 dataset and then applied to the ImageNet dataset by stacking copies of this cell, each with its own parameters. The approach yielded accuracy of 82.7  top 1 and 96.2  top 5. This exceeded the best human invented architectures at a cost of 9 billion fewer FLOPS a reduction of 28 . The system continued to exceed the manually designed alternative at varying computation levels. The image features learned from image classification can be transferred to other computer vision problems. E.g., for object detection, the learned cells integrated with the Faster RCNN framework improved performance by 4.0  on the COCO dataset. In the so called Efficient Neural Architecture Search  ENAS , a controller discovers architectures by learning to search for an optimal subgraph within a large graph. The controller is trained with policy gradient to select a subgraph that maximizes the validation set s expected reward. The model corresponding to the subgraph is trained to minimize a canonical cross entropy loss. Multiple child models share parameters, ENAS requires fewer GPU hours than other approaches and 1000 fold less than  standard  NAS. On CIFAR 10, the ENAS design achieved a test error of 2.89 , comparable to NASNet. On Penn Treebank, the ENAS design reached test perplexity of 55.8. An alternative approach to NAS is based on evolutionary algorithms, which has been employed by several groups. An Evolutionary Algorithm for Neural Architecture Search generally performs the following procedure. First a pool consisting of different candidate architectures along with their validation scores  fitness  is initialised. At each step the architectures in the candidate pool are mutated  eg  3x3 convolution instead of a 5x5 convolution . Next the new architectures are trained from scratch for a few epochs and their validation scores are obtained. This is followed by replacing the lowest scoring architectures in the candidate pool with the better, newer architectures. This procedure is repeated multiple times and thus the candidate pool is refined over time. Mutations in the context of evolving ANNs are operations such as adding or removing a layer, which include changing the type of a layer  e.g., from convolution to pooling , changing the hyperparameters of a layer, or changing the training hyperparameters. On CIFAR 10 and ImageNet, evolution and RL performed comparably, while both slightly outperformed random search. Bayesian Optimization which has proven to be an efficient method for hyperparameter optimization can also be applied to NAS. In this context the objective function maps an architecture to its validation error after being trained for a number of epochs. At each iteration BO uses a surrogate to model this objective function based on previously obtained architectures and their validation errors. One then chooses the next architecture to evaluate by maximizing an acquisition function, such as expected improvement, which provides a balance between exploration and exploitation. Acquisition function maximization and objective function evaluation are often computationally expensive for NAS, and make the application of BO challenging in this context. Recently BANANAS has achieved promising results in this direction by introducing a high performing instantiation of BO coupled to a neural predictor. Another group used a hill climbing procedure that applies network morphisms, followed by short cosine annealing optimization runs. The approach yielded competitive results, requiring resources on the same order of magnitude as training a single network. E.g., on CIFAR 10, the method designed and trained a network with an error rate below 5  in 12 hours on a single GPU. While most approaches solely focus on finding architecture with maximal predictive performance, for most practical applications other objectives are relevant, such as memory consumption, model size or inference time  i.e., the time required to obtain a prediction . Because of that, researchers created a multi objective search. LEMONADE is an evolutionary algorithm that adopted Lamarckism to efficiently optimize multiple objectives. In every generation, child networks are generated to improve the Pareto frontier with respect to the current population of ANNs. Neural Architect is claimed to be a resource aware multi objective RL based NAS with network embedding and performance prediction. Network embedding encodes an existing network to a trainable embedding vector. Based on the embedding, a controller network generates transformations of the target network. A multi objective reward function considers network accuracy, computational resource and training time. The reward is predicted by multiple performance simulation networks that are pre trained or co trained with the controller network. The controller network is trained via policy gradient. Following a modification, the resulting candidate network is evaluated by both an accuracy network and a training time network. The results are combined by a reward engine that passes its output back to the controller network. RL or evolution based NAS require thousands of GPU days of searching training to achieve state of the art computer vision results as described in the NASNet, mNASNet and MobileNetV3 papers. To reduce computational cost, many recent NAS methods rely on the weight sharing idea. In this approach a single overparameterized supernetwork  also known as the one shot model  is defined. A supernetwork is a very large Directed Acyclic Graph  DAG  whose subgraphs are different candidate neural networks.Thus in a supernetwork the weights are shared among a large number of different sub architectures that have edges in common, each of which is considered as a path within the supernet. The essential idea is to train one supernetwork that spans many options for the final design rather than generating and training thousands of networks independently. In addition to the learned parameters, a set of architecture parameters are learnt to depict preference for one module over another. Such methods reduce the required computational resources to only a few GPU days. More recent works further combine this weight sharing paradigm, with a continuous relaxation of the search space, which enables the use of gradient based optimization methods. These approaches are generally referred to as differentiable NAS and have proven very efficient in exploring the search space of neural architectures. One of the most popular algorithms amongst the gradient based methods for NAS is DARTS. However DARTS faces problems such as performance collapse due to an inevitable aggregation of skip connections and poor generalization which were tackled by many future algorithms. Methods like  aim at robustifying DARTS and making the validation accuracy landscape smoother by introducing a Hessian norm based regularisation and random smoothing adversarial attack respectively. The cause of performance degradation is later analyzed from the architecture selection aspect. Differentiable NAS has shown to produce competitive results using a fraction of the search time required by RL based search methods. For example, FBNet  which is short for Facebook Berkeley Network  demonstrated that supernetwork based search produces networks that outperform the speed accuracy tradeoff curve of mNASNet and MobileNetV2 on the ImageNet image classification dataset. FBNet accomplishes this using over 400x less search time than was used for mNASNet. Further, SqueezeNAS demonstrated that supernetwork based NAS produces neural networks that outperform the speed accuracy tradeoff curve of MobileNetV3 on the Cityscapes semantic segmentation dataset, and SqueezeNAS uses over 100x less search time than was used in the MobileNetV3 authors  RL based search. Neural architecture search often requires large computational resources, due to its expensive training and evaluation phases. This further leads to a large carbon footprint required for the evaluation of these methods.  To overcome this limitation, NAS benchmarks have been introduced, from which one can either query or predict the final performance of neural architectures in seconds. A NAS benchmark is defined as a dataset with a fixed train test split, a search space, and a fixed training pipeline  hyperparameters . There are primarily two types of NAS benchmarks  a surrogate NAS benchmark and a tabular NAS benchmark. A surrogate benchmark uses a  surrogate model  eg  a neural network  to predict the performance of an architecture from the search space. On the other hand a tabular benchmark queries the actual performance of an architecture trained upto convergence. Both of these benchmarks are queryable and can be used to efficiently simulate many NAS algorithms using only a CPU to query the benchmark instead of training an architecture from scratch. 
Neural computation is the information processing performed by networks of neurons. Neural computation is affiliated with the philosophical tradition known as Computational theory of mind, also referred to as computationalism, which advances the thesis that neural computation explains cognition. The first persons to propose an account of neural activity as being computational was Warren McCullock and Walter Pitts in their seminal 1943 paper, A Logical Calculus of the Ideas Immanent in Nervous Activity. There are three general branches of computationalism, including classicism, connectionism, and computational neuroscience. All three branches agree that cognition is computation, however, they disagree on what sorts of computations constitute cognition. The classicism tradition believes that computation in the brain is digital, analogous to digital computing. Both connectionism and computational neuroscience do not require that the computations that realize cognition are necessarily digital computations. However, the two branches greatly disagree upon which sorts of experimental data should be used to construct explanatory models of cognitive phenomena. Connectionists rely upon behavioral evidence to construct models to explain cognitive phenomena, whereas computational neuroscience leverages neuroanatomical and neurophysiological information to construct mathematical models that explain cognition. When comparing the three main traditions of the computational theory of mind, as well as the different possible forms of computation in the brain, it is helpful to define what we mean by computation in a general sense. Computation is the processing of information, otherwise known as variables or entities, according to a set of rules. A rule in this sense is simply an instruction for executing a manipulation on the current state of the variable, in order to produce an specified output. In other words, a rule dictates which output to produce given a certain input to the computing system. A computing system is a mechanism whose components must be functionally organized to process the information in accordance with the established set of rules. The types of information processed by a computing system determine which type of computations it performs. Traditionally, in cognitive science there have been two proposed types of computation related to neural activity   digital and analog, with the vast majority of theoretical work incorporating a digital understanding of cognition. Computing systems that perform digital computation are functionally organized to execute operations on strings of digits with respect to the type and location of the digit on the string. It has been argued that neural spike train signaling implements some form of digital computation, since neural spikes may be considered as discrete units or digits, like 0 or 1   the neuron either fires an action potential or it does not. Accordingly, neural spike trains could be seen as strings of digits. Alternatively, analog computing systems perform manipulations on non discrete, irreducibly continuous variables, that is, entities that vary continuously as a function of time. These sorts of operations are characterized by systems of differential equations. Neural computation can be studied for example by building models of neural computation. There is a scientific journal dedicated to this subject, Neural Computation. Artificial neural networks  ANN  is a subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation. 
Neuro symbolic AI integrates neural and symbolic AI architectures to address complementary strengths and weaknesses of each, providing a robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient machine learning models. Gary Marcus, argues that   We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.  Further,  To build a robust, knowledge driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.  Henry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking, for example, as recently discussed in Daniel Kahneman s book Thinking Fast and Slow  the idea of which has a much longer history though . Human thinking may have two components  System 1 is fast, automatic, intuitive, and unconscious. System 2 is slower, step by step, and explicit. System 1 is used for pattern recognition. System 2 handles planning, deduction, and deliberative thinking. In this view, deep learning best handles the first kind of cognition while symbolic reasoning best handles the second kind. Both are needed for a robust, reliable AI that can learn, reason, and interact with humans to accept advice and answer questions. Such dual process models with explicit references to the two contrasting systems have been worked on since the 1990s, both in AI and in Cognitive Science, by several researchers  e.g., . Approaches for integration are varied. Henry Kautz s taxonomy of neuro symbolic architectures, along with some examples, follows  These categories are not exhaustive, for example, as they do not consider multi agent systems. In 2005, Bader and Hitzler presented a more fine grained categorization that considered, e.g., whether the use of symbols included logic or not and if it did, whether the logic was propositional or first order logic. The 2005 categorization and Kautz s taxonomy above are compared and contrasted in a 2021 article. Recently, Sepp Hochreiter argued that Graph Neural Networks  ...are the predominant models of neural symbolic computing  since  hey describe the properties of molecules, simulate social networks, or predict future states in physical and engineering applications with particle particle interactions.  Marcus argues that  ...hybrid architectures that combine learning and symbol manipulation are necessary for robust intelligence, but not sufficient , and that there are   ...four cognitive prerequisites for building robust artificial intelligence   This echoes the much earlier calls for hybrid models as early as the 1990s. Garcez and Lamb described research in this area as being ongoing for at least the past twenty years  actually more than thirty years by now .  A series of workshops on neuro symbolic AI has been held annually since 2005 . In the early 1990s, an initial set of workshops on this topic were organized. Many key research questions remain, as they have been since the 1990s, such as  Some specific implementations of neuro symbolic approaches are  
Neurorobotics is the combined study of neuroscience, robotics, and artificial intelligence. It is the science and technology of embodied autonomous neural systems. Neural systems include brain inspired algorithms  e.g. connectionist networks , computational models of biological neural networks  e.g. artificial spiking neural networks, large scale simulations of neural microcircuits  and actual biological systems  e.g. in vivo and in vitro neural nets . Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but also, at smaller scale, micro machines and, at the larger scales, furniture and infrastructures. Neurorobotics is that branch of neuroscience with robotics, which deals with the study and application of science and technology of embodied autonomous neural systems like brain inspired algorithms. It is based on the idea that the brain is embodied and the body is embedded in the environment. Therefore, most neurorobots are required to function in the real world, as opposed to a simulated environment. Beyond brain inspired algorithms for robots neurorobotics may also involve the design of brain controlled robot systems. Neurorobots can be divided into various major classes based on the robot s purpose. Each class is designed to implement a specific mechanism of interest for study. Common types of neurorobots are those used to study motor control, memory, action selection, and perception. Neurorobots are often used to study motor feedback and control systems, and have proved their merit in developing controllers for robots. Locomotion is modeled by a number of neurologically inspired theories on the action of motor systems. Locomotion control has been mimicked using models or central pattern generators, clumps of neurons capable of driving repetitive behavior, to make four legged walking robots.  Other groups have expanded the idea of combining rudimentary control systems into a hierarchical set of simple autonomous systems. These systems can formulate complex movements from a combination of these rudimentary subsets. This theory of motor action is based on the organization of cortical columns, which progressively integrate from simple sensory input into a complex afferent signals, or from complex motor programs to simple controls for each muscle fiber in efferent signals, forming a similar hierarchical structure. Another method for motor control uses learned error correction and predictive controls to form a sort of simulated muscle memory. In this model, awkward, random, and error prone movements are corrected for using error feedback to produce smooth and accurate movements over time. The controller learns to create the correct control signal by predicting the error.  Using these ideas, robots have been designed which can learn to produce adaptive arm movements or to avoid obstacles in a course. Robots designed to test theories of animal memory systems. Many studies examine the memory system of rats, particularly the rat hippocampus, dealing with place cells, which fire for a specific location that has been learned. Systems modeled after the rat hippocampus are generally able to learn mental maps of the environment, including recognizing landmarks and associating behaviors with them, allowing them to predict the upcoming obstacles and landmarks. Another study has produced a robot based on the proposed learning paradigm of barn owls for orientation and localization based on primarily auditory, but also visual stimuli. The hypothesized method involves synaptic plasticity and neuromodulation, a mostly chemical effect in which reward neurotransmitters such as dopamine or serotonin affect the firing sensitivity of a neuron to be sharper. The robot used in the study adequately matched the behavior of barn owls. Furthermore, the close interaction between motor output and auditory feedback proved to be vital in the learning process, supporting active sensing theories that are involved in many of the learning models. Neurorobots in these studies are presented with simple mazes or patterns to learn. Some of the problems presented to the neurorobot include recognition of symbols, colors, or other patterns and execute simple actions based on the pattern. In the case of the barn owl simulation, the robot had to determine its location and direction to navigate in its environment. Action selection studies deal with negative or positive weighting to an action and its outcome. Neurorobots can and have been used to study simple ethical interactions, such as the classical thought experiment where there are more people than a life raft can hold, and someone must leave the boat to save the rest. However, more neurorobots used in the study of action selection contend with much simpler persuasions such as self preservation or perpetuation of the population of robots in the study. These neurorobots are modeled after the neuromodulation of synapses to encourage circuits with positive results. In biological systems, neurotransmitters such as dopamine or acetylcholine positively reinforce neural signals that are beneficial. One study of such interaction involved the robot Darwin VII, which used visual, auditory, and a simulated taste input to  eat  conductive metal blocks. The arbitrarily chosen good blocks had a striped pattern on them while the bad blocks had a circular shape on them. The taste sense was simulated by conductivity of the blocks.  The robot had positive and negative feedbacks to the taste based on its level of conductivity. The researchers observed the robot to see how it learned its action selection behaviors based on the inputs it had. Other studies have used herds of small robots which feed on batteries strewn about the room, and communicate its findings to other robots. Neurorobots have also been used to study sensory perception, particularly vision. These are primarily systems that result from embedding neural models of sensory pathways in automatas. This approach gives exposure to the sensory signals that occur during behavior and also enables a more realistic assessment of the degree of robustness of the neural model. It is well known that changes in the sensory signals produced by motor activity provide useful perceptual cues that are used extensively by organisms. For example, researchers have used the depth information that emerges during replication of human head and eye movements to establish robust representations of the visual scene. Biological robots are not officially neurorobots in that they are not neurologically inspired AI systems, but actual neuron tissue wired to a robot. This employs the use of cultured neural networks to study brain development or neural interactions. These typically consist of a neural culture raised on a multielectrode array  MEA , which is capable of both recording the neural activity and stimulating the tissue. In some cases, the MEA is connected to a computer which presents a simulated environment to the brain tissue and translates brain activity into actions in the simulation, as well as providing sensory feedback The ability to record neural activity gives researchers a window into a brain, which they can use to learn about a number of the same issues neurorobots are used for. An area of concern with the biological robots is ethics. Many questions are raised about how to treat such experiments. The central question concerns consciousness and whether or not the rat brain experiences it. There are many theories about how to define consciousness. Neuroscientists benefit from neurorobotics because it provides a blank slate to test various possible methods of brain function in a controlled and testable environment. While robots are more simplified versions of the systems they emulate, they are more specific, allowing more direct testing of the issue at hand. They also have the benefit of being accessible at all times, while it is more difficult to monitor large portions of a brain while the human or animal is active, especially individual neurons. The development of neuroscience has produced neural treatments. These include pharmaceuticals and neural rehabilitation. Progress is dependent on an intricate understanding of the brain and how exactly it functions. It is difficult to study the brain, especially in humans, due to the danger associated with cranial surgeries. Neurorobots can improved the range of tests and experiments that can be performed in the study of neural processes. 
Non Assisted reproductive technology, artificial reproduction or artificial perpetuation is the self building of new life by other than the natural means. Such technology sets a living thing to be capable of continuing and renewing itself spontaneously for a defined or undefined period of time, i.e., capable of perpetuating oneself and or itself without further human assistance. Cells normally reproduce by first growing and then dividing, e.g., binary fission, mitosis, etc. However, if a cell has been genetically modified, and still has the capacity to divide, in principle this is an artificial reproduction. Some of these technologies involve editing the genetic code, somatic cell nuclear transfer and 3D print a cell with artificial DNA sequences. An artificial womb or artificial uterus is a device that would allow for extracorporeal pregnancy by growing an embryonic form outside the body of an organism  that would normally carry the embryo to term  without any human assistance.  It is more general form of kinematic replication  which includes non cellular forms of life, e.g., robots, cyborgs, etc.   Xenobots are an example of kinematic replication. They are biobots, named after the African clawed frog  Xenopus laevis . Xenobots are cellular life forms designed by using artificial intelligence  to built more of themselves by combining frog cells in a liquid medium. This hypothetical technology is based on the idea that A.I. such as Siri and Alexa can have descendant offspring. 
Non human  also spelled nonhuman  is any entity displaying some, but not enough, human characteristics to be considered a human. The term has been used in a variety of contexts and may refer to objects that have been developed with human intelligence, such as robots or vehicles. In the animal rights movement, it is common to distinguish between  human animals  and  non human animals . Participants in the animal rights movement generally recognize that non human animals have some similar characteristics to those of human persons. For example, various non human animals have been shown to register pain, compassion, memory, and some cognitive function.  Some animal rights activists argue that the similarities between human and non human animals justify giving non human animals rights that human society has afforded to humans, such as the right to self preservation, and some even wish for all non human animals or at least those that bear a fully thinking and conscious mind, such as vertebrates and some invertebrates such as cephalopods, to be given a full right of personhood. Contemporary philosophers have drawn on the work of Henri Bergson, Gilles Deleuze, F lix Guattari, and Claude L vi Strauss  among others  to suggest that the non human poses epistemological and ontological problems for humanist and post humanist ethics, and have linked the study of non humans to materialist and ethological approaches to the study of society and culture. The term non human has been used to describe computer programs and robot like devices that display some human like characteristics. In both science fiction and in the real world, computer programs and robots have been built to perform tasks that require human computer interactions in a manner that suggests sentience and compassion. There is increasing interest in the use of robots in nursing homes and to provide elder care. Computer programs have been used for years in schools to provide one on one education with children. The Tamagotchi toy required children to provide care, attention, and nourishment to keep it  alive . 
Nouvelle artificial intelligence  AI  is an approach to artificial intelligence pioneered in the 1980s by Rodney Brooks, who was then part of MIT artificial intelligence laboratory. Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the  real world,  instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them. The differences between nouvelle AI and symbolic AI are apparent in early robots Shakey and Freddy. These robots contained an internal model  or  representation   of their micro worlds consisting of symbolic descriptions. As a result, this structure of symbols had to be renewed as the robot moved or the world changed. Shakey s planning programs assessed the program structure and broke it down into the necessary steps to complete the desired action. This level of computation required a large amount time to process, so Shakey typically performed its tasks very slowly. Symbolic AI researchers had long been plagued by the problem of updating, searching, and otherwise manipulating the symbolic worlds inside their AIs. A nouvelle system refers continuously to its sensors rather than to an internal model of the world. It processes the external world information it needs from the senses when it is required. As Brooks puts it,  the world is its own best model  always exactly up to date and complete in every detail.  A central idea of nouvelle AI is that simple behaviors combine to form more complex behaviors over time. For example, simple behaviors can include elements like  move forward  and  avoid obstacles.  A robot using nouvelle AI with simple behaviors like collision avoidance and moving toward a moving object could possibly come together to produce a more complex behavior like chasing a moving object.  The frame problem describes an issue with using first order logic  FOL  to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms  symbolic language  to imply that things about an environment that do not change arbitrarily. Nouvelle AI seeks to sidestep the frame problem by dispensing with filling the AI or robot with volumes of symbolic language and instead letting more complex behaviors emerge by combining simpler behavioral elements.  The goal of traditional AI was to build intelligences without bodies, which would only have been able to interact with the world via keyboard, screen, or printer. However, nouvelle AI attempts to build embodied intelligence situated in the real world. Brooks quotes approvingly from the brief sketches that Turing gave in 1948 and 1950 of the  situated  approach. Turing wrote of equipping a machine  with the best sense organs that money can buy  and teaching it  to understand and speak English  by a process that would  follow the normal teaching of a child.  This approach was contrasted to the others where they focused on abstract activities such as playing chess. Brooks focused on building robots that acted like simple insects while simultaneously working to remove some traditional AI characteristics. He created insect like robots called  Allen and Herbert. Brooks s insectoid robots contained no internal models of the world. Herbert, for example, discarded a high volume of the information received from its sensors and never stored information for more than two seconds. Allen, named after Allen Newell, had a ring of twelve ultrasonic sonars as its primary sensors and three independent behavior producing modules. These modules were programmed to avoid both stationary and moving objects. With only this module activated, Allen stayed in the middle of a room until an object approached and then it ran away while avoiding obstacles in its way. Herbert, named after Herbert A. Simon, used infrared sensors to avoid obstacles and a laser system to collect 3D data over a distance of about 12 feet. Herbert also carried a number of simple sensors in its  hand.  The robot s testing ground was the real world environment of the busy offices and workspaces of the MIT AI lab where it searched for empty soda cans and carried them away, a seemingly goal oriented activity that emerged as a result of 15 simple behavior units combining together. As a parallel, Simon noted that an ant s complicated path is due to the structure of its environment rather than the depth of its thought processes Other robots by Brooks  team were Genghis and Squirt. Genghis had six legs and was able to walk over rough terrain and follow a human. Squirt s behavior modules had it stay in dark corners until it heard a noise, then it would begin to follow the source of the noise. Brooks agreed that the level of nouvelle AI had come near the complexity of a real insect, which raised a question about whether or not insect level behavior was and is a reasonable goal for nouvelle AI? Brooks  own recent work has taken the opposite direction to that proposed by Von Neumann in the quotations  theorists who select the human nervous system as their model are unrealistically picking  the most complicated object under the sun,  and that there is little advantage in selecting instead the ant, since any nervous system at all exhibits exceptional complexity.  In the 1990s, Brooks decided to pursue the goal of human level intelligence and, with Lynn Andrea Stein, built a humanoid robot called Cog. Cog is a robot with an extensive collection of sensors, a face, and arms  among other features  that allow it to interact with the world and gather information and experience so as to assemble intelligence organically in the manner described above by Turing. The team believed that Cog would be able to learn and able to find a correlation between the sensory information it received and its actions, and to learn common sense knowledge on its own. As of 2003, all development of the project had ceased. 
ONPASSIVE purports to be a software company that develops marketing, CRM and similar AI assisted products. ONPASSIVE claim to have headquarters in Dubai, United Arab Emirates. The company also claims to have offices in the US, India, Singapore, Bangladesh, and Egypt. The company was founded in 2018 by Ashraf Mufareh and opened its first facility in Hyderabad, India in September 2020. On October 25, 2021. The FTC Listed OnPassive and O Net Social networks on their list for offenses concerning money making opportunities and concerning deceptive or unfair conduct. After its launch in Dubai in March 2022, it announced the opening of its headquarters at Burj Khalifa the following month. In September, ONPASSIVE has rolled out its crowdfunding platform  O Bless . That said, the feature is not currently showing up on the founders back office.  In November 2022, ONPASSIVE partnered with BeIN Sports to sponsor UEFA Champions League and La Liga streaming in Qatar. In the same month, ONPASSIVE launched its software products called  O Mail ,  O Net  and  O Trim . In January, The Dubai Roads and Transportation Authority granted the naming rights of Al Safa Metro Station to ONPASSIVE. For the next 10 years, the station, located on the red line, will be called ONPASSIVE. ONPASSIVE participated in The Global Forum for Women Entrepreneurs 2023, which took place on March 10 11 in Abu Dhabi and was organized by the UAE Businesswomen Council. 350 delegates and representatives from 40 countries participated in the forum, representing different institutions. The company s main office is located in Dubai, UAE. Tech hub is based in Hyderabad, India. Global offices are located in Singapore and the US, Bangladesh and Egypt. 
 Omar Sultan Al Olama                     is Minister of State for Artificial Intelligence in the United Arab Emirates. He was appointed in October 2017 by the Vice President and Prime Minister of the UAE and Ruler of Dubai, Sheikh Mohammed bin Rashid Al Maktoum. The UAE was the first country to hire a minister for artificial intelligence. Al Olama was born on 16 February 1990 in Dubai. He has a Bachelor Degree in Business Administration and Management from the American University in Dubai, and a Diploma in Excellence and Project Management from the American University in Sharjah. Al Olama s responsibilities as Minister of State for Artificial Intelligence include creating and fostering international efforts in responsibly governing Artificial Intelligence and reflecting the UAE s vision on ethical use of AI. Before his ministerial position, Al Olama worked in several sectors including the banking sector, telecommunications, private enterprises and government. Between February 2012 and May 2014, Al Olama was member of the corporate planning at the UAE s Prime Minister s Office. From November 2015 to November 2016, he was Deputy Head of Minster s Office at the UAE s Prime Minister s Office. Between December 2015 and October 2017, he was Secretary General of the World Organization of Racing Drones. In November 2017, he was appointed Deputy Managing Director of the Dubai Future Foundation. Since July 2016, AlOlama has been the Managing Director of the World Government Summit. During his work in the Future Department at the Ministry of Cabinet Affairs and the Future, AlOlama participated in developing the UAE Centennial 2071 strategy. He also participated in developing the UAE 4th Industrial Revolution Strategy that aims to promote the UAE s status as a global hub for the 4th Industrial Revolution and develop a national economy based on knowledge, innovation and future technologies. Prior to his appointment, he worked on developing the UAE s Artificial Intelligence Strategy. In November 2017, Al Olama was appointed as a member of the  Shaping the Future of Digital Economy and Society  Council  working group , part of the World Economic Forum  WEF    Davos. In October 2017, the UAE Government launched the  UAE Artificial Intelligence Strategy . It covers education, transportation, energy, space and technology. On 1 July 2018, Al Olama inaugurated the first UAE AI Summer Camp. 2,200 applications from university students and government executives were received in just 24 hours. 
OpenIRIS is the open source version of IRIS, a semantic desktop that enables users to create a  personal map  across their office related information objects. The name IRIS is an acronym for  Integrate. Relate. Infer. Share.  IRIS includes a machine learning platform to help automate this process. It provides  dashboard  views, contextual navigation, and relationship based structure across an extensible suite of office applications, including a calendar, web and file browser, e mail client, and instant messaging client. IRIS was built as part of SRI International s CALO project, a very large artificial intelligence funded by the Defense Advanced Research Projects Agency  DARPA  under its Personalized Assistant that Learns program. CALO funded research resulted in more than five hundred publications across all fields of artificial intelligence. Here are a few  
 Operation Serenata de Amor is an artificial intelligence project designed to analyze public spending in Brazil. The project has been funded by a recurrent financing campaign since September 7, 2016, and came in the wake of major scandals of misappropriation of public funds in Brazil, such as the Mensal o scandal and what was revealed in the Operation Car Wash investigations. The analysis began with data from the National Congress then expanded to other types of budget and instances of government, such as the Federal Senate. The project is built through collaboration on GitHub and using a public group with more than 600 participants on Telegram. The name  Serenata de Amor,  which means  serenade of love,  was taken from a popular cashew cream bonbon produced by Chocolates Garoto in Brazil. Throughout development of the project, new modules have been newly introduced in addition to the main repository  Operation Serenata de Amor is an Artificial intelligence project for analysis of public expenditures. It was conceived in March 2016 by data scientist Irio Musskopf, sociologist Eduardo Cuducos and entrepreneur Felipe Cabral. The project was financed collectively in the Catarse platform, where it reached 131  of the collection goal  paying 3 months of project development. Ana Schwendler, also a data scientist, Pedro Vilanova  Tonny , data journalist, Bruno Pazzim, software engineer, Filipe Linhares, a frontend engineer, Leandro Devegili, an entrepreneur and Andr  Pinho took the first steps towards constructing the platform, such as collecting and structuring the first datasets. Jessica Temporal, data scientist and Yasodara C rdova  Yaso , researcher, Tatiana Balachova  Russa , UX designer, joined the project after the financing took place. The members created a recurring financing campaign, expanding the analysis of public spending to the Federal Senate. Donors make monthly payments ranging from 5 BRL to 200 BRL to maintain group activities. The monthly amount collected is around 10,000 BRL. In January 2017, concluding the period financed by the initial campaign, the group carried out an investigation into the suspicious activities found by the data analysis system. 629 complaints were made to the Ombudsman s Office of the Chamber of Deputies, questioning expenses of 216 federal deputies. In addition, the Facebook project page has more than 25,000 followers, and users frequently cite the operation as a benchmark in transparency in the Brazilian government. One of the examples of results obtained by the operation is the case of the Deputy who had to return about 700 BRL to the House  after his expenses were analyzed by the platform. The platform was able to analyze more than 3 million notes, raising about 8,000 suspected cases in public spending. The community that supports the work of the team benefits from open source repositories, with licenses open for the collaboration. So much so that the two main data scientists  of the project presented it at the CivicTechFest in Taipei, obtaining several mentions even in the international press. The technical leader presented the project in Poland during DevConf2017 in Krak w. It was also presented in the Google News Lab in 2017. It was presented by Yaso, when she was the Director of the initiative, at the MIT Media Lab Berkman Klein Center Initiative for Artificial Intelligence ethics, and at the Artificial Intelligence and Inclusion Symposium, an initiative of the  Global Network of Internet   Society Centers  NoC . It was also presented both by Irio and Yaso at the Digital Harvard Kennedy School, over a lunch seminar, where the transparency of the platform and the main solutions found were discussed, so that the code and data are always available to verify its suitability. This infographic provides information about the first results of Operation Serenata de Amor, a project that analyzes open data on public spending to find discrepancies. The project was presented by Yaso to the House Audit and Control Committee of the Chamber of Deputies in August 2017, and raised the interest of House officials who work with open data. The operation has been a source of inspiration for other civic projects that aim to work with similar goals. Participation of several team members in events throughout Brazil and abroad can be found on the Internet, such as presentation at OpenDataDay, held at Calango Hackerspace in the Federal District, Campus Party Bahia, Campus Party Brasilia, Friends of Tomorrow, XIII National Meeting of Internal Control, in the event USP Talks  Hackfest against corruption in Jo o Pessoa, the latter being also highlighted in the National Press. 
Operational artificial intelligence, or operational AI, is a type of intelligent system designed for real world applications, particularly at commercial scale. The term is used to distinguish accessible artificially intelligent  AI  systems from fundamental AI research and from industrial AI applications which are not integrated with the routine usage of a business. The definition of operational AI differs throughout the IT industry, where vendors and individual organizations often create their own custom definitions of such processes and services for the purpose of marketing their own products.  Applications include text analytics, advanced analytics, facial and image recognition, machine learning, and natural language generation. According to a white paper by software company Tupl Inc, continuous machine learning model training and results extraction in the telecom industry requires a large number of automation utilities in order to  facilitate the development and deployment of a multitude of use cases, the collection and correlation of the data, the creation and training of the models, and the operation at telecom grade levels of security and availability .  Researchers in the University of Waterloo s Artificial Intelligence Group describe operational AI in terms of the focus on applications that bring value to products and company. University of Waterloo Professor of Electrical and Computer Engineering Fakhri Karray describes operational AI as  application of AI for the masses . Canada Research Chair and Associate Professor Alexander Wong  professor  describes operational AI as AI for  anyone, anywhere, anytime.  Industrial AI refers to intelligent systems applied for business at any scale and for any use case. 
Oriented energy filters are used to grant sight to intelligent machines and sensors. The light comes in and is filtered so that it can be properly computed and analyzed by the computer allowing it to  perceive  what it is measuring. These energy measurements are then calculated to take a real time measurement of the oriented space time structure. 3D Gaussian filters are used to extract orientation measurements. They were chosen due to their ability to capture a broad spectrum and easy and efficient computations. The use of these vision systems can then be used in smart room, human interface and surveillance applications. The computations used can tell more than the standalone frame that most perceived motion devices such as a television frame. The objects captured by these devices would tell the velocity and energy of an object and its direction in relation to space and time. This also allows for better tracking ability and recognition. 
 POP 11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the School of Computer Science at the University of Birmingham, which hosts the main Poplog website. POP 11 is an evolution of the language POP 2, developed in Edinburgh University, and features an open stack model  like Forth, among others . It is mainly procedural, but supports declarative language constructs, including a pattern matcher, and is mostly used for research and teaching in artificial intelligence, although it has features sufficient for many other classes of problems. It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP 11 s features is that it supports first class functions. POP 11 is the core language of the Poplog system. The availability of the compiler and compiler subroutines at run time  a requirement for incremental compilation  gives it the ability to support a far wider range of extensions  including run time extensions, such as adding new data types  than would be possible using only a macro facility. This made it possible for  optional  incremental compilers to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any POP 11 constructs. This made it possible for Poplog to be used by teachers, researchers, and developers who were interested in only one of the languages. The most successful product developed in POP 11 was the Clementine Data mining system, developed by ISL. After SPSS bought ISL they decided to port Clementine to C   and Java, and eventually succeeded with great effort  and perhaps some loss of the flexibility provided by the use of an AI language . POP 11 was for a time available only as part of an expensive commercial package  Poplog , but since about 1999 it has been freely available as part of the Open Source version of Poplog, including various additional packages and teaching libraries. An online version of ELIZA using POP 11 is available at Birmingham. At the University of Sussex, David Young used POP 11 in combination with C and Fortran to develop a suite of teaching and interactive development tools for image processing and vision, and has made them available in the Popvision extension to Poplog. Here is an example of a simple POP 11 program  That prints out  This one includes some list processing  Examples using the POP 11 pattern matcher, which makes it relatively easy for students to learn to develop sophisticated list processing programs without having to treat patterns as tree structures accessed by  head  and  tail  functions  CAR and CDR in Lisp , can be found in the online introductory tutorial. The matcher is at the heart of the SimAgent  sim agent  toolkit. Some of the powerful features of the toolkit, such as linking pattern variables to inline code variables, would have been very difficult to implement without the incremental compiler facilities. 
Pattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns  rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language. Broad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties. In addition to the new algebraic vocabulary, its statistical approach is novel in its aim to  The Brown University Pattern Theory Group was formed in 1972 by Ulf Grenander. Many mathematicians are currently working in this group, noteworthy among them being the Fields Medalist David Mumford.  Mumford regards Grenander as his  guru  in Pattern Theory. 
A pedagogical agent is a concept borrowed from computer science and artificial intelligence and applied to education, usually as part of an intelligent tutoring system  ITS . It is a simulated human like interface between the learner and the content, in an educational environment. A pedagogical agent is designed to model the type of interactions between a student and another person. Mabanza and de Wet define it as  a character enacted by a computer that interacts with the user in a socially engaging manner . A pedagogical agent can be assigned different roles in the learning environment, such as tutor or co learner, depending on the desired purpose of the agent.  A tutor agent plays the role of a teacher, while a co learner agent plays the role of a learning companion . The history of Pedagogical Agents is closely aligned with the history of computer animation. As computer animation progressed, it was adopted by educators to enhance computerized learning by including a lifelike interface between the program and the learner. The first versions of a pedagogical agent were more cartoon than person, like Microsoft s Clippy which helped users of Microsoft Office load and use the program s features in 1997. However, with developments in computer animation, pedagogical agents can now look lifelike. By 2006 there was a call to develop modular, reusable agents to decrease the time and expertise required to create a pedagogical agent. There was also a call in 2009 to enact agent standards. The standardization and re usability of pedagogical agents is less of an issue since the decrease in cost and widespread availability of animation tools. Individualized pedagogical agents can be found across disciplines including medicine, math, law, language learning, automotive, and armed forces. They are used in applications directed to every age, from preschool to adult. Distributed cognition theory is the method in which cognition progresses in the context of collaboration with others. Pedagogical agents can be designed to assist the cognitive transfer to the learner, operating as artifacts or partners with collaborative role in learning. To support the performance of an action by the user, the pedagogical agent can act as a cognitive tool as long as the agent is equipped with the knowledge that the user lacks. The interactions between the user and the pedagogical agent can facilitate a social relationship. The pedagogical agent may fulfill the role of a working partner. Socio cultural learning theory is how the user develops when they are involved in learning activities in which there is interaction with other agents. A pedagogical agent can  intervene when the user requests, provide support for tasks that the user cannot address, and potentially extend the learners cognitive reach. Interaction with the pedagogical agent may elicit a variety of emotions from the learner. The learner may become excited, confused, frustrated, and or discouraged. These emotions affect the learners  motivation. Extraneous cognitive load is the extra effort being exerted by an individual s working memory due to the way information is being presented. A pedagogical agent can increase the user s cognitive load by distracting them and becoming the focus of their attention, causing split attention between the instructional material and the agent. Agents can reduce the perceived cognitive load by providing narration and personalization that can also promote a user s interest and motivation. While research on the reduction of cognitive load from pedagogical  agents is minimal, more studies have shown that agents do not increase it. It has been suggested by researchers that pedagogical agents may take on different roles in the learning environment. Examples of these roles are  supplanting, scaffolding, coaching, testing, or demonstrating or modelling a procedure. A pedagogical agent as a tutor has not been demonstrated to add any benefit to an educational strategy in equivalent lessons with and without a pedagogical agent. According to Richard Mayer, there is some support in research for pedagogical agent increasing learning, but only as a presenter of social cues. A co learner pedagogical agent is believed to increase the student s self efficacy. By pointing out important features of instructional content, a pedagogical agent can fulfill the signaling function, which research on multimedia learning has shown to enhance learning. Research has demonstrated that human human interaction may not be completely replaced by pedagogical agents, but learners may prefer the agents to non agent multimedia systems. This finding is supported by social agency theory. Much like the varying effectiveness of the pedagogical agent roles in the learning environment, agents that take into account the user s affect have had mixed results. Research has shown pedagogical agents that make use of the users  affect have been found to increase user knowledge retention, motivation, and perceived self efficacy. However, with such a broad range of modalities in affective expressions, it is often difficult to utilize them. Additionally, having agents detect a user s affective state with precision remains challenging, as displays of affect are different across individuals. The appearance of a pedagogical agent can be manipulated to meet the learning requirements. The attractiveness of a pedagogical agent can enhance student s learning when the users were the opposite gender of the pedagogical agent. Male students prefer a sexy appearance of a female pedagogical agents and dislike the sexy appearance of male agents. Female students were not attracted by the sexy appearance of either male or female pedagogical agents. Pedagogical agents have reached a point where they can convey and elicit emotion, but also reason about and respond to it. These agents are often designed to elicit and respond to affective actions from users through various modalities such as speech, facial expressions, and body gestures. They respond to the affective state of the given user, and make use of these modalities using a wide array of sensors incorporated into the design of the agent. Specifically in education and training applications, pedagogical agents are often designed to increasingly recognize when users or learners exhibit frustration, boredom, confusion, and states of flow. The added recognition in these agents is a step toward making them more emotionally intelligent, comforting and motivating the users as they interact. The design of a pedagogical agent often begins with its digital representation, whether it will be 2D or 3D and static or animated. Several studies have developed pedagogical agents that were both static and animated, then evaluated the relative benefits. Similar to other design considerations, the improved learning from static or animated agents remains questionable. One study showed that the appearance of an agent portrayed using a static image can impact a user s recall, based on the visual appearance. Other research found results that suggest static agent images improve learning outcomes. However, several other studies found user s learned more when the pedagogical agent was animated rather than static. Recently a meta analysis of such research found a negligible improvement in learning via pedagogical agents, suggesting more work needs to be done in the area to support any claims. 
A percept is the input that an intelligent agent is perceiving at any given moment. It is essentially the same concept as a percept in psychology, except that it is being perceived not by the brain but by the agent. A percept is detected by a sensor, often a camera, processed accordingly, and acted upon by an actuator. Each percept is added to a  percept sequence , which is a complete history of each percept ever detected. The agent s action at any instant point may depend on the entire percept sequence up to that particular instant point. An intelligent agent chooses how to act not only based on the current percept, but the percept sequence. The next action is chosen by the agent function, which maps every percept to an action. For example, if a camera were to record a gesture, the agent would process the percepts, calculate the corresponding spatial vectors, examine its percept history, and use the agent program  the application of the agent function  to act accordingly. Examples of percepts include inputs from touch sensors, cameras, infrared sensors, sonar, microphones, mice, and keyboards. A percept can also be a higher level feature of the data, such as lines, depth, objects, faces, or gestures.  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Personality computing is a research field related to artificial intelligence and personality psychology that studies personality by means of computational techniques from different sources, including text, multimedia and social networks. Personality computing addresses three main problems involving personality  automatic personality recognition, perception and synthesis. Automatic personality recognition is the inference of personality type of target individuals from their digital footprint, automatic personality perception is the inference of the personality attributed by an observer to a target individual based on some observable behavior, and automatic personality synthesis is the generation of the style or behaviour of artificial personalities in Avatars and virtual agents. Self assessed personality tests or observer ratings are always exploited as the ground truth for testing and validating the performance of artificial intelligence algorithms for the automatic prediction of personality types. There is a wide variety of personality tests, such as the Myers Briggs Type Indicator  MBTI  or the MMPI, but the most used are tests based on the Five Factor Model such as the Revised NEO Personality Inventory. Personality computing can be considered as an extension or complement of Affective Computing, where the former focuses on personality traits and the latter on affective states. A further extension of the two fields is Character Computing which combines various character states and traits including but not limited to personality and affect. Personality computing begun around 2005 with few pioneering research works in personality recognition showing that personality traits could be inferred with reasonable accuracy from text, such as blogs, self presentations, and email addresses. In 2008, the concept of  portable personality  for the distributed management of personality profiles has been developed. Few years later begun the research in personality recognition and perception from multimodal and social signals, such as recorded meetings and voice calls. In the 2010s the research focussed mainly on personality recognition and perception from social media, in particular from Facebook, Twitter and Instagram. In the same years Automatic personality synthesis helped improving the coherence of simulated behavior in virtual agents. Scientific works demonstrated the validity of Personality Computing from different digital footprints, in particular from user preferences such as Facebook page likes and showed that machines can recognize personality better than humans Personality computing techniques, in particular personality recognition and perception, have applications in Social media marketing, where they can help reducing the cost of advertising campaigns through psychological targeting 
Personoid is the concept coined by Stanis aw Lem, a Polish science fiction writer, in Non Serviam, from his book A Perfect Vacuum  1971 . His personoids are an abstraction of functions of human mind and they live in computers  they do not need any human like physical body.  In cognitive and software modeling, personoid is a research approach to the development of intelligent autonomous agents. In frame of the IPK  Information, Preferences, Knowledge  architecture, it is a framework of abstract intelligent agent with a  cognitive and structural intelligence. It can be seen as an essence of high intelligent entities. From the philosophical and systemics perspectives, personoid societies can also be seen as the carriers of a culture. According to N. Gessler, the personoids study can be a base for the research on artificial culture and culture evolution.     This article about futures studies or futurology is a stub. You can help Wikipedia by expanding it.
The principle of rationality  or rationality principle  was coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework. It is related to what he called the  logic of the situation  in an Economica article of 1944 1945, published later in his book The Poverty of Historicism. According to Popper s rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis. Popper called for social science to be grounded in what he called situational analysis. This requires building models of social situations which include individual actors and their relationship to social institutions, e.g. markets, legal codes, bureaucracies, etc. These models attribute certain aims and information to the actors. This forms the  logic of the situation , the result of reconstructing meticulously all circumstances of an historical event. The  principle of rationality  is the assumption that people are instrumental in trying to reach their goals, and this is what drives the model. Popper believed that this model could be continuously refined to approach the objective truth. Popper called his  principle of rationality  nearly empty  a technical term meaning without empirical content  and strictly speaking false, but nonetheless tremendously useful. These remarks earned him a lot of criticism because seemingly he had swerved from his famous Logic of Scientific Discovery. Among the many philosophers having discussed his  principle of rationality  from the 1960s up to now are Noretta Koertge, R. Nadeau, Viktor J. Vanberg, Hans Albert, E. Matzner, Ian C. Jarvie, Mark A. Notturno, John Wettersten, Ian C. B hm. In the context of knowledge based systems, Newell  in 1982  proposed the following principle of rationality   If an agent has knowledge that one of its actions will lead to one of its goals, then the agent will select that action.  This principle is employed by agents at the knowledge level to move closer to a desired goal. An important philosophical difference between Newell and Popper is that Newell argued that the knowledge level is real in the sense that it exists in nature and is not made up. This allowed Newell to treat the rationality principle as a way of understanding nature and avoid the problems Popper ran into by treating knowledge as non physical and therefore non empirical.  
A probabilistic logic network  PLN  is a conceptual, mathematical and computational approach to uncertain inference  inspired by logic programming, but using probabilities in place of crisp  true false  truth values, and fractional uncertainty in place of crisp known unknown values. In order to carry out effective reasoning in real world circumstances, artificial intelligence software must robustly handle uncertainty. However, previous approaches to uncertain inference do not have the breadth of scope required to provide an integrated treatment of the disparate forms of cognitively critical uncertainty as they manifest themselves within the various forms of pragmatic inference. Going beyond prior probabilistic approaches to uncertain inference, PLN is able to encompass within uncertain logic such ideas as induction, abduction, analogy, fuzziness and speculation, and reasoning about time and causality. PLN was developed by Ben Goertzel, Matt Ikle, Izabela Lyon Freire Goertzel, and Ari Heljakka for use as a cognitive algorithm used by MindAgents within the OpenCog Core. PLN was developed originally for use within the Novamente Cognition Engine. The basic goal of a PLN is to provide reasonably accurate probabilistic inference in a way that is compatible with both term logic and predicate logic and scales up to operate in real time on large dynamic knowledge bases. The goal underlying the theoretical development of PLN has been the creation of practical software systems carrying out complex, useful inferences based on uncertain knowledge and drawing uncertain conclusions. PLN has been designed to allow basic probabilistic inference to interact with other kinds of inference such as intensional inference, fuzzy inference, and higher order inference using quantifiers, variables, and combinators, and be a more convenient approach than Bayesian networks  or other conventional approaches  for the purpose of interfacing basic probabilistic inference with these other sorts of inference.  In addition, the inference rules are formulated in such a way as to avoid the paradoxes of Dempster Shafer theory. PLN begins with a term logic foundation and then adds on elements of probabilistic and combinatory logic, as well as some aspects of predicate logic and autoepistemic logic, to form a complete inference system, tailored for easy integration with software components embodying other  not explicitly logical  aspects of intelligence. PLN represents truth values as intervals, but with different semantics than in imprecise probability theory.  In addition to the interpretation of truth in a probabilistic fashion, a truth value in PLN also has an associated amount of certainty. This generalizes the notion of truth values used in autoepistemic logic, where truth values are either known or unknown and when known, are either true or false. The current version of PLN has been used in narrow AI applications such as the inference of biological hypotheses from knowledge extracted from biological texts via language processing, and to assist the reinforcement learning of an embodied agent, in a simple virtual world, as it is taught to play  fetch . 
Cognition Problem solving is the process of achieving a goal by overcoming obstacles, a frequent part of most activities. Problems in need of solutions range from simple personal tasks  e.g. how to turn on an appliance  to complex issues in business and technical fields. The former is an example of simple problem solving  SPS  addressing one issue, whereas the latter is complex problem solving  CPS  with multiple interrelated obstacles. Another classification is into well defined problems with specific obstacles and goals, and ill defined problems in which the current situation is troublesome but it is not clear what kind of resolution to aim for. Similarly, one may distinguish formal or fact based problems requiring psychometric intelligence, versus socio emotional problems which depend on the changeable emotions of individuals or groups, such as tactful behavior, fashion, or gift choices. Solutions require sufficient resources and knowledge to attain the goal. Professionals such as lawyers, doctors, and consultants are largely problem solvers for issues which require technical skills and knowledge beyond general competence. Many businesses have found profitable markets by recognizing a problem and creating a solution  the more widespread and inconvenient the problem, the greater the opportunity to develop a scalable solution. There are many specialized problem solving techniques and methods in fields such as engineering, business, medicine, mathematics, computer science, philosophy, and social organization. The mental techniques to identify, analyze, and solve problems are studied in psychology and cognitive sciences. Additionally, the mental obstacles preventing people from finding solutions is a widely researched topic  problem solving impediments include confirmation bias, mental set, and functional fixedness. The term problem solving has a slightly different meaning depending on the discipline. For instance, it is a mental process in psychology and a computerized process in computer science. There are two different types of problems  ill defined and well defined  different approaches are used for each. Well defined problems have specific end goals and clearly expected solutions, while ill defined problems do not. Well defined problems allow for more initial planning than ill defined problems. Solving problems sometimes involves dealing with pragmatics, the way that context contributes to meaning, and semantics, the interpretation of the problem. The ability to understand what the end goal of the problem is, and what rules could be applied represents the key to solving the problem. Sometimes the problem requires abstract thinking or coming up with a creative solution. Problem solving in psychology refers to the process of finding solutions to problems encountered in life. Solutions to these problems are usually situation or context specific. The process starts with problem finding and problem shaping, where the problem is discovered and simplified. The next step is to generate possible solutions and evaluate them. Finally a solution is selected to be implemented and verified. Problems have an end goal to be reached and how you get there depends upon problem orientation  problem solving coping style and skills  and systematic analysis. Mental health professionals study the human problem solving processes using methods such as introspection, behaviorism, simulation, computer modeling, and experiment. Social psychologists look into the person environment relationship aspect of the problem and independent and interdependent problem solving methods. Problem solving has been defined as a higher order cognitive process and intellectual function that requires the modulation and control of more routine or fundamental skills. Problem solving has two major domains  mathematical problem solving and personal problem solving. Both are seen in terms of some difficulty or barrier that is encountered. Empirical research shows many different strategies and factors influence everyday problem solving. Rehabilitation psychologists studying individuals with frontal lobe injuries have found that deficits in emotional control and reasoning can be re mediated with effective rehabilitation and could improve the capacity of injured persons to resolve everyday problems. Interpersonal everyday problem solving is dependent upon the individual personal motivational and contextual components. One such component is the emotional valence of  real world  problems and it can either impede or aid problem solving performance. Researchers have focused on the role of emotions in problem solving, demonstrating that poor emotional control can disrupt focus on the target task and impede problem resolution and likely lead to negative outcomes such as fatigue, depression, and inertia. In conceptualization, human problem solving consists of two related processes  problem orientation and the motivational attitudinal affective approach to problematic situations and problem solving skills. Studies conclude people s strategies cohere with their goals and stem from the natural process of comparing oneself with others. Among the first experimental psychologists to study problem solving were the Gestaltists in Germany, e.g., Karl Duncker in The Psychology of Productive Thinking  1935 . Perhaps best known is the work of Allen Newell and Herbert A. Simon. Experiments the 1960s and early 1970s asked participants to solve relatively simple, well defined, but not previously seen laboratory tasks. These simple problems, such as the Tower of Hanoi, admitted optimal solutions which could be found quickly, allowing observation of the full problem solving process. Researchers assumed that these model problems would elicit the characteristic cognitive processes by which more complex  real world  problems are solved. An outstanding problem solving technique found by this research is the principle of decomposition. Much of computer science and artificial intelligence involves designing automatic systems to solve a specified type of problem  to accept input data and calculate a correct or adequate response, reasonably quickly. Algorithms are recipes or instructions that direct such systems, written into computer programs. Steps for designing such systems include problem determination, heuristics, root cause analysis, de duplication, analysis, diagnosis, and repair. Analytic techniques include linear and nonlinear programming, queuing systems, and simulation. A large, perennial obstacle is to find and fix errors in computer programs  debugging. Formal logic is concerned with such issues as validity, truth, inference, argumentation and proof. In a problem solving context, it can be used to formally represent a problem as a theorem to be proved, and to represent the knowledge needed to solve the problem as the premises to be used in a proof that the problem has a solution. The use of computers to prove mathematical theorems using formal logic emerged as the field of automated theorem proving in the 1950s. It included the use of heuristic methods designed to simulate human problem solving, as in the Logic Theory Machine, developed by Allen Newell, Herbert A. Simon and J. C. Shaw, as well as algorithmic methods such as the resolution principle developed by John Alan Robinson. In addition to its use for finding proofs of mathematical theorems, automated theorem proving has also been used for program verification in computer science. However, already in 1958, John McCarthy proposed the advice taker, to represent information in formal logic and to derive answers to questions using automated theorem proving. An important step in this direction was made by Cordell Green in 1969, using a resolution theorem prover for question answering and for such other applications in artificial intelligence as robot planning. The resolution theorem prover used by Cordell Green bore little resemblance to human problem solving methods. In response to criticism of his approach, emanating from researchers at MIT, Robert Kowalski developed logic programming and SLD resolution, which solves problems by problem decomposition. He has advocated logic for both computer and human problem solving and computational logic to improve human thinking Problem solving is used when products or processes fail, so corrective action can be taken to prevent further failures. It can also be applied to a product or process prior to an actual failure event when a potential problem can be predicted and analyzed, and mitigation applied to prevent the problem. Techniques such as failure mode and effects analysis can proactively reduce the likelihood of problems. In either case, it is necessary to build a causal explanation through a process of diagnosis. Staat summarizes the derivation of explanation through diagnosis as follows  In deriving an explanation of effects in terms of causes, abduction plays the role of generating new ideas or hypotheses  asking  how?    deduction functions as evaluating and refining the hypotheses based on other plausible premises  asking  why?    and induction is justifying of the hypothesis with empirical data  asking  how much?  . The objective of abduction is to determine which hypothesis or proposition to test, not which one to adopt or assert. In the Peircean logical system, the logic of abduction and deduction contribute to our conceptual understanding of a phenomenon, while the logic of induction adds quantitative details  empirical substantiation  to our conceptual knowledge. Forensic engineering is an important technique of failure analysis that involves tracing product defects and flaws. Corrective action can then be taken to prevent further failures. Reverse engineering attempts to discover the original problem solving logic used in developing a product by taking it apart. In military science, problem solving is linked to the concept of  end states , the condition or situation which is the aim of the strategy.  xiii, E 2  Ability to solve problems is important at any military rank, but is essential at the command and control level, where it results from deep qualitative and quantitative understanding of possible scenarios. Effectiveness is evaluation of results, whether the goal was accomplished.  IV 24  Planning is the process of determining how to achieve the goal.  IV 1  Some models of problem solving involve identifying a goal and then a sequence of subgoals towards achieving this goal. Andersson, who introduced the ACT R model of cognition, modelled this collection of goals and subgoals as a goal stack, where the mind contains a stack of goals and subgoals to be completed with a single task being carried out at any time.  51  It has been observed that knowledge of how to solve one problem can be applied to another problem, in a process known as transfer.  56  Problem solving strategies are steps to overcoming the obstacles to achieving a goal, the  problem solving cycle . Common steps in this cycle include recognizing the problem, defining it, developing a strategy to fix it, organizing knowledge and resources available, monitoring progress, and evaluating the effectiveness of the solution. Once a solution is achieved, another problem usually arises, and the cycle starts again. Insight is the sudden aha! solution to a problem, the birth of a new idea to simplify a complex situation. Solutions found through insight are often more incisive than those from step by step analysis. A quick solution process requires insight to select productive moves at different stages of the problem solving cycle. Unlike Newell and Simon s formal definition of a move problem, there is no consensus definition of an insight problem. Some problem solving strategies include  Common barriers to problem solving are mental constructs that impede an efficient search for solutions. Five of the most common identified by researchers are  confirmation bias, mental set, functional fixedness, unnecessary constraints, and irrelevant information. Confirmation bias is an unintentional tendency to collect and use data which favors preconceived notions. Such notions may be incidental rather than motivated by important personal beliefs  the desire to be right may be sufficient motivation. Research has found that scientific and technical professionals also experience confirmation bias. Andreas Hergovich, Reinhard Schott, and Christoph Burger s experiment conducted online, for instance, suggested that professionals within the field of psychological research are likely to view scientific studies that agree with their preconceived notions more favorably than clashing studies. According to Raymond Nickerson, one can see the consequences of confirmation bias in real life situations, which range in severity from inefficient government policies to genocide. Nickerson argued that those who killed people accused of witchcraft demonstrated confirmation bias with motivation. Researcher Michael Allen found evidence for confirmation bias with motivation in school children who worked to manipulate their science experiments to produce favorable results. However, confirmation bias does not necessarily require motivation. In 1960, Peter Cathcart Wason conducted an experiment in which participants first viewed three numbers and then created a hypothesis that proposed a rule that could have been used to create that triplet of numbers. When testing their hypotheses, participants tended to only create additional triplets of numbers that would confirm their hypotheses, and tended not to create triplets that would negate or disprove their hypotheses. Mental set is the inclination to re use a previously successful solution, rather than search for new and better solutions. It is a reliance on habit. It was first articulated by Abraham Luchins in the 1940s with his well known water jug experiments. Participants were asked to fill one jug with a specific amount of water using other jugs with different maximum capacities. After Luchins gave a set of jug problems that could all be solved by a single technique, he then introduced a problem that could be solved by the same technique, but also by a novel and simpler method. His participants tended to use the accustomed technique, oblivious of the simpler alternative. This was again demonstrated in Norman Maier s 1931 experiment, which challenged participants to solve a problem by using a familiar tool  pliers  in an unconventional manner. Participants were often unable to view the object in a way that strayed from its typical use, a type of mental set known as functional fixedness  see the following section . Rigidly clinging to a mental set is called fixation, which can deepen to an obsession or preoccupation with attempted strategies that are repeatedly unsuccessful. In the late 1990s, researcher Jennifer Wiley found that professional expertise in a field can create a mental set, perhaps leading to fixation. Groupthink, where each individual takes on the mindset of the rest of the group, can produce and exacerbate mental set. Social pressure leads to everybody thinking the same thing and reaching the same conclusions. Functional fixedness is the tendency to view an object as having only one function, unable to conceive of any novel use, as in the Maier pliers experiment above. Functional fixedness is a specific form of mental set, and is one of the most common forms of cognitive bias in daily life. Tim German and Clark Barrett describe this barrier   subjects become  fixed  on the design function of the objects, and problem solving suffers relative to control conditions in which the object s function is not demonstrated.  Their research found that young children s limited knowledge of an object s intended function reduces this barrier Research has also discovered functional fixedness in many educational instances, as an obstacle to understanding. Furio, Calatayud, Baracenas, and Padilla stated   ... functional fixedness may be found in learning concepts as well as in solving chemistry problems.  As an example, imagine a man wants to kill a bug in his house, but the only thing at hand is a can of air freshener. He may start searching for something to kill the bug instead of squashing it with the can, thinking only of its main function of deodorizing. There are several hypotheses in regards to how functional fixedness relates to problem solving. It may waste time, delaying or entirely preventing the correct use of a tool. Unnecessary constraints are arbitrary boundaries imposed unconsciously on the task at hand, which foreclose a productive avenue of solution. The solver may become fixated on only one type of solution, as if it were an inevitable requirement of the problem. Typically, this combines with mental set, clinging to a previously successful method. Visual problems can also produce mentally invented constraints. A famous example is the dot problem  nine dots arranged in a three by three grid pattern must be connected by drawing four straight line segments, without lifting pen from paper or backtracking along a line. The subject typically assumes the pen must stay within the outer square of dots, but the solution requires lines continuing beyond this frame, and researchers have found a 0  solution rate within a brief allotted time. This problem has produced the expression  think outside the box . Such problems are typically solved via a sudden insight which leaps over the mental barriers, often after long toil against them. This can be difficult depending on how the subject has structured the problem in their mind, how they draw on past experiences, and how well they juggle this information in their working memory. In the example, envisioning the dots connected outside the framing square requires visualizing an unconventional arrangement, a strain on working memory. Irrelevant information is a specification or data presented in a problem that is unrelated to the solution. If the solver assumes that all information presented needs to be used, this often derails the problem solving process, making relatively simple problems much harder. For example   Fifteen percent of the people in Topeka have unlisted telephone numbers. You select 200 names at random from the Topeka phone book. How many of these people have unlisted phone numbers?  The  obvious  answer is 15 , but in fact none of the unlisted people would be listed among the 200. This kind of  trick question  is often used in aptitude tests or cognitive evaluations. Though not inherently difficult, they require independent thinking that is not necessarily common. Mathematical word problems often include irrelevant qualitative or numerical information as an extra challenge. The disruption caused by the above cognitive biases can depend on how the information is represented  visually, verbally, or mathematically. A classic example is the Buddhist monk problem  The problem cannot be addressed in a verbal context, trying to describe the monk s progress on each day. It becomes much easier when the paragraph is represented mathematically by a function  one visualizes a graph whose horizontal axis is time of day, and whose vertical axis shows the monk s position  or altitude  on the path at each time. Superimposing the two journey curves, which traverse opposite diagonals of a rectangle, one sees they must cross each other somewhere. The visual representation by graphing has resolved the difficulty. Similar strategies can often improve problem solving on tests. Individual humans engaged in problem solving tend to overlook subtractive changes, including those that are critical elements of efficient solutions. This tendency to solve by first, only or mostly creating or adding elements, rather than by subtracting elements or processes is shown to intensify with higher cognitive loads such as information overload. Problem solving can also occur without waking consciousness. There are many reports of scientists and engineers who solved problems in their dreams. Elias Howe, inventor of the sewing machine, figured out the structure of the bobbin from a dream. The chemist August Kekul  was considering how benzene arranged its six carbon and hydrogen atoms. Thinking about the problem, he dozed off, and dreamt of dancing atoms that fell into a snakelike pattern, which led him to discover the benzene ring. As Kekul  wrote in his diary, One of the snakes seized hold of its own tail, and the form whirled mockingly before my eyes. As if by a flash of lightning I awoke  and this time also I spent the rest of the night in working out the consequences of the hypothesis.There also are empirical studies of how people can think consciously about a problem before going to sleep, and then solve the problem with a dream image. Dream researcher William C. Dement told his undergraduate class of 500 students that he wanted them to think about an infinite series, whose first elements were OTTFF, to see if they could deduce the principle behind it and to say what the next elements of the series would be. He asked them to think about this problem every night for 15 minutes before going to sleep and to write down any dreams that they then had. They were instructed to think about the problem again for 15 minutes when they awakened in the morning. The sequence OTTFF is the first letters of the numbers  one, two, three, four, five. The next five elements of the series are SSENT  six, seven, eight, nine, ten . Some of the students solved the puzzle by reflecting on their dreams. One example was a student who reported the following dream  I was standing in an art gallery, looking at the paintings on the wall. As I walked down the hall, I began to count the paintings  one, two, three, four, five. As I came to the sixth and seventh, the paintings had been ripped from their frames. I stared at the empty frames with a peculiar feeling that some mystery was about to be solved. Suddenly I realized that the sixth and seventh spaces were the solution to the problem!With more than 500 undergraduate students, 87 dreams were judged to be related to the problems students were assigned  53 directly related and 34 indirectly related . Yet of the people who had dreams that apparently solved the problem, only seven were actually able to consciously know the solution. The rest  46 out of 53  thought they did not know the solution. Mark Blechner conducted this experiment and obtained results similar to Dement s. He found that while trying to solve the problem, people had dreams in which the solution appeared to be obvious from the dream, but it was rare for the dreamers to realize how their dreams had solved the puzzle. Coaxing or hints did not get them to realize it, although once they heard the solution, they recognized how their dream had solved it. For example, one person in that OTTFF experiment dreamed  There is a big clock. You can see the movement. The big hand of the clock was on the number six. You could see it move up, number by number, six, seven, eight, nine, ten, eleven, twelve. The dream focused on the small parts of the machinery. You could see the gears inside.In the dream, the person counted out the next elements of the series    six, seven, eight, nine, ten, eleven, twelve    yet he did not realize that this was the solution of the problem. His sleeping mindbrain solved the problem, but his waking mindbrain was not aware how. Albert Einstein believed that much problem solving goes on unconsciously, and the person must then figure out and formulate consciously what the mindbrain has already solved. He believed this was his process in formulating the theory of relativity   The creator of the problem possesses the solution.  Einstein said that he did his problem solving without words, mostly in images.  The words or the language, as they are written or spoken, do not seem to play any role in my mechanism of thought. The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be  voluntarily  reproduced and combined.   In cognitive sciences, researchers  realization that problem solving processes differ across knowledge domains and across levels of expertise and that, consequently, findings obtained in the laboratory cannot necessarily generalize to problem solving situations outside the laboratory, has led to an emphasis on real world problem solving since the 1990s. This emphasis has been expressed quite differently in North America and Europe, however. Whereas North American research has typically concentrated on studying problem solving in separate, natural knowledge domains, much of the European research has focused on novel, complex problems, and has been performed with computerized scenarios. In Europe, two main approaches have surfaced, one initiated by Donald Broadbent in the United Kingdom and the other one by Dietrich D rner in Germany. The two approaches share an emphasis on relatively complex, semantically rich, computerized laboratory tasks, constructed to resemble real life problems. The approaches differ somewhat in their theoretical goals and methodology, however. The tradition initiated by Broadbent emphasizes the distinction between cognitive problem solving processes that operate under awareness versus outside of awareness, and typically employs mathematically well defined computerized systems. The tradition initiated by D rner, on the other hand, has an interest in the interplay of the cognitive, motivational, and social components of problem solving, and utilizes very complex computerized scenarios that contain up to 2,000 highly interconnected variables. In North America, initiated by the work of Herbert A. Simon on  learning by doing  in semantically rich domains, researchers began to investigate problem solving separately in different natural knowledge domains   such as physics, writing, or chess playing   thus relinquishing their attempts to extract a global theory of problem solving. Instead, these researchers have frequently focused on the development of problem solving within a certain domain, that is on the development of expertise. Areas that have attracted rather intensive attention in North America include  Complex problem solving  CPS  is distinguishable from simple problem solving  SPS . When dealing with SPS there is a singular and simple obstacle in the way. But CPS comprises one or more obstacles at a time. In a real life example, a surgeon at work has far more complex problems than an individual deciding what shoes to wear. As elucidated by Dietrich D rner, and later expanded upon by Joachim Funke, complex problems have some typical characteristics as follows  Problem solving is applied on many different levels   from the individual to the civilizational. Collective problem solving refers to problem solving performed collectively. Social issues and global issues can typically only be solved collectively. It has been noted that the complexity of contemporary problems has exceeded the cognitive capacity of any individual and requires different but complementary expertise and collective problem solving ability. Collective intelligence is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals. Collaborative problem solving is about people working together face to face or in online workspaces with a focus on solving real world problems. These groups are made up of members that share a common concern, a similar passion, and or a commitment to their work. Members are willing to ask questions, wonder, and try to understand common issues. They share expertise, experiences, tools, and methods. These groups can be assigned by instructors, or may be student regulated based on the individual student needs. The groups, or group members, may be fluid based on need, or may only occur temporarily to finish an assigned task. They may also be more permanent in nature depending on the needs of the learners. All members of the group must have some input into the decision making process and have a role in the learning process. Group members are responsible for the thinking, teaching, and monitoring of all members in the group. Group work must be coordinated among its members so that each member makes an equal contribution to the whole work. Group members must identify and build on their individual strengths so that everyone can make a significant contribution to the task. Collaborative groups require joint intellectual efforts between the members and involve social interactions to solve problems together. The knowledge shared during these interactions is acquired during communication, negotiation, and production of materials. Members actively seek information from others by asking questions. The capacity to use questions to acquire new information increases understanding and the ability to solve problems. Collaborative group work has the ability to promote critical thinking skills, problem solving skills, social skills, and self esteem. By using collaboration and communication, members often learn from one another and construct meaningful knowledge that often leads to better learning outcomes than individual work. In a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that pro actively  augmenting human intellect  would yield a multiplier effect in group problem solving   Three people working together in this augmented mode  seem to be more than three times as effective in solving a complex problem as is one augmented person working alone . Henry Jenkins, a key theorist of new media and media convergence draws on the theory that collective intelligence can be attributed to media convergence and participatory culture. He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating  whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals . Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contributes to the development of such skills. Collective impact is the commitment of a group of actors from different sectors to a common agenda for solving a specific social problem, using a structured form of collaboration. After World War II the UN, the Bretton Woods organization and the WTO were created  collective problem solving on the international level crystallized around these three types of organizations from the 1980s onward. As these global institutions remain state like or state centric it has been called unsurprising that these continue state like or state centric approaches to collective problem solving rather than alternative ones. Crowdsourcing is a process of accumulating the ideas, thoughts or information from many independent participants, with aim to find the best solution for a given challenge. Modern information technologies allow for massive number of subjects to be involved as well as systems of managing these suggestions that provide good results. With the Internet a new capacity for collective, including planetary scale, problem solving was created. 
Progress in Artificial Intelligence  AI  refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, scientific discovery, video games, and toys. However, many AI applications are not perceived as AI    A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it s not labeled AI anymore.   Many thousands of AI applications are deeply embedded in the infrastructure of every industry.  In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field was rarely credited for these successes at the time. Kaplan and Haenlein structure artificial intelligence along three evolutionary stages  1  artificial narrow intelligence   applying AI only to specific tasks  2  artificial general intelligence   applying AI to several areas and able to autonomously solve problems they were never even designed for  and 3  artificial super intelligence   applying AI to any area capable of scientific creativity, social skills, and general wisdom. To allow comparison with human performance, artificial intelligence can be evaluated on constrained and well defined problems.  Such tests have been termed subject matter expert Turing tests.  Also, smaller problems provide more achievable goals and there are an ever increasing number of positive results. There are many useful abilities that can be described as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas. AI, like electricity or the steam engine, is a general purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at. Some versions of Moravec s paradox observe that humans are more likely to outperform machines in areas such as physical dexterity that have been the direct target of natural selection. While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets. Researcher Andrew Ng has suggested, as a  highly imperfect rule of thumb , that  almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI.  Games provide a high profile benchmark for assessing rates of progress  many games have a large professional player base and a well established competitive rating system. AlphaGo brought the era of classical board game benchmarks to a close when Artificial Intelligence proved their competitive edge over humans in 2016. Deep Mind s AlphaGo AI software program defeated the world s best professional Go Player Lee Sedol. Games of imperfect knowledge provide new challenges to AI in the area of game theory  the most prominent milestone in this area was brought to a close by Libratus  poker victory in 2017. E sports continue to provide additional benchmarks  Facebook AI, Deepmind, and others have engaged with the popular StarCraft franchise of videogames. Broad classes of outcome for an AI test may be given as  In his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis. The Turing test is now considered too exploitable to be a meaningful benchmark. The Feigenbaum test, proposed by the inventor of expert systems, tests a machine s knowledge and expertise about a specific subject. A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior. Proposed  universal intelligence  tests aim to compare how well machines, humans, and even non human animals perform on problem sets that are generic as possible. At an extreme, the test suite can contain every possible problem, weighted by Kolmogorov complexity  however, these problem sets tend to be dominated by impoverished pattern matching exercises where a tuned AI can easily exceed human performance levels. Many competitions and prizes, such as the Imagenet Challenge, promote research in artificial intelligence. The most common areas of competition include general machine intelligence, conversational behavior, data mining, robotic cars, and robot soccer as well as conventional games. An expert poll around 2016, conducted by Katja Grace of the Future of Humanity Institute and associates, gave median estimates of 3 years for championship Angry Birds, 4 years for the World Series of Poker, and 6 years for StarCraft. On more subjective tasks, the poll gave 6 years for folding laundry as well as an average human worker, 7 10 years for expertly answering  easily Googleable  questions, 8 years for average speech transcription, 9 years for average telephone banking, and 11 years for expert songwriting, but over 30 years for writing a New York Times bestseller or winning the Putnam math competition. An AI defeated a grandmaster in a regulation tournament game for the first time in 1988  rebranded as Deep Blue, it beat the reigning human world chess champion in 1997  see Deep Blue versus Garry Kasparov . AlphaGo defeated a European Go champion in October 2015, and Lee Sedol in March 2016, one of the world s top players  see AlphaGo versus Lee Sedol . According to Scientific American and other sources, most observers had expected superhuman Computer Go performance to be at least a decade away. AI pioneer and economist Herbert A. Simon inaccurately predicted in 1965   Machines will be capable, within twenty years, of doing any work a man can do . Similarly, in 1970 Marvin Minsky wrote that  Within a generation... the problem of creating artificial intelligence will substantially be solved.  Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when AGI would arrive was 2040 to 2050, depending on the poll. The Grace poll around 2016 found results varied depending on how the question was framed. Respondents asked to estimate  when unaided machines can accomplish every task better and more cheaply than human workers  gave an aggregated median answer of 45 years and a 10  chance of it occurring within 9 years. Other respondents asked to estimate  when all occupations are fully automatable. That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers  estimated a median of 122 years and a 10  probability of 20 years. The median response for when  AI researcher  could be fully automated was around 90 years. No link was found between seniority and optimism, but Asian researchers were much more optimistic than North American researchers on average  Asians predicted 30 years on average for  accomplish every task , compared with the 74 years predicted by North Americans. 
Prompt engineering is a concept in artificial intelligence, particularly natural language processing. In prompt engineering, the description of the task that the AI is supposed to accomplish is embedded in the input, e.g. as a question, instead of it being explicitly given. Prompt engineering typically works by converting one or more tasks to a prompt based dataset and training a language model with what has been called  prompt based learning  or just  prompt learning . The GPT 2 and GPT 3 language models were important steps in prompt engineering. In 2021, multitask prompt engineering using multiple NLP datasets showed good performance on new tasks. In a method called chain of thought  CoT  prompting, few shot examples of a task were given to the language model which improved its ability to reason. The broad accessibility of these tools was driven by the publication of several open source notebooks and community led projects for image synthesis. A description for handling prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022. Meta s Segment Anything computer vision model is a prompting based model for creating masks for objects in images. It can be prompted in several ways, such as selecting a few  positive  and  negative  points, to create a mask that includes all the positive points and excludes all the negative points. Chain of thought prompting  CoT  improves the reasoning ability of large language models  LLMs  by prompting them to generate a series of intermediate steps that lead to the final answer of a multi step problem. The technique was first proposed by Google researchers in 2022. LLMs that are trained on large amounts of text using deep learning methods can generate output that resembles human generated text. While LLMs show impressive performance on various natural language tasks, they still face difficulties with some reasoning tasks that require logical thinking and multiple steps to solve, such as arithmetic or commonsense reasoning questions. To address this challenge, CoT prompting prompts the model to produce intermediate reasoning steps before giving the final answer to a multi step problem. For example, given the question  Q  The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? , a CoT prompt might induce the LLM to answer with steps of reasoning that mimic a train of thought like  A  The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23   20   3. They bought 6 more apples, so they have 3   6   9. The answer is 9.  Chain of thought prompting improves the performance of LLMs on average on both arithmetic and commonsense tasks in comparison to standard prompting methods. When applied to PaLM, a 540B parameter language model, CoT prompting significantly aided the model, allowing it to perform comparably with task specific fine tuned models on several tasks, even setting a new state of the art at the time on the GSM8K mathematical reasoning benchmark. CoT prompting is an emergent property of model scale, meaning it works better with larger and more powerful language models. It is possible to fine tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability. There are two main methods to elicit chain of thought reasoning  few shot prompting and zero shot prompting. The initial proposition of CoT prompting demonstrated few shot prompting, wherein at least one example of a question paired with proper human written CoT reasoning is prepended to the prompt. It is also possible to elicit similar reasoning and performance gain with zero shot prompting, which can be as simple as appending to the prompt the words  Let s think step by step . This allows for better scaling as one no longer needs to prompt engineer specific CoT prompts for each task to get the corresponding boost in performance. Generated knowledge prompting first prompts the model to generate relevant facts for completing the prompt, then proceed to complete the prompt. The completion quality is usually higher, as the model can be conditioned on relevant facts. Self consistency decoding performs several CoT rollouts, then select the most commonly reached conclusion out of all the rollouts. If the rollouts disagree by a lot, a human can be queried for the a correct chain of thought. Tree of thought prompting generalizes CoT by prompting the model to generate one or more  possible next steps , and then running the model on each of the possible next steps by breadth first, beam, or some other method of tree search. While CoT reasoning can improve performance on natural language processing tasks, certain drawbacks exist. Zero shot CoT prompting increased the likelihood of toxic output on tasks for which models can make inferences about marginalized groups or harmful topics. By default, the output of language models may not contain estimates of uncertainty. The model may output text that appears confident, though the underlying token predictions have low likelihood scores. Large language models like GPT 4 can have accurately calibrated likelihood scores in their token predictions, and so the model output uncertainty can be directly estimated by reading out the token prediction likelihood scores. But if one cannot access such scores  such as when one is accessing the model through a restrictive API , uncertainty can still be estimated and incorporated into the model output. One simple method is to prompt the model to use words to estimate uncertainty. Another is to prompt the model to refuse to answer in a standardized way if the input does not satisfy conditions. LLM themselves can be used to compose prompts for LLM. The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM  CoT examples can be generated by LLM themselves. In  auto CoT , a library of questions are converted to vectors by a model such as BERT. The question vectors are clustered. Questions nearest to the centroids of each cluster are selected. An LLM does zero shot CoT on each question. The resulting CoT examples are added to the dataset. When prompted with a new question, CoT examples to the nearest questions can be retrieved and added to the prompt. Prompts do not have to be in English. In  prefix tuning  or  prompt tuning , floating point valued vectors are searched directly by gradient descent, to maximize the log probability on outputs. Formally, let an LLM be written as     L L M   X     F   E   X         , where     X      is a sequence of linguistic tokens,     E      is the token to vector function, and     F      is the rest of the model. In prefix tuning, one provide a set of input output pairs          X  i   ,  Y  i         i     ,Y        , and then use gradient descent to search for     arg    max    Z           i   log   P r    Y  i          Z        E    X  i           sum   log Pr   . In words,     log   P r    Y  i          Z        E    X  i            is the log likelihood of outputting      Y  i        , if the model first encodes the input      X  i         into the vector     E    X  i           , then prepend the vector with the  prefix vector         Z            , then apply     F     . An earlier result uses the same idea of gradient descent search, but is designed for masked language models like BERT, and searches only over token sequences, rather than numerical vectors. Formally, it searches for     arg    max    X           i   log   P r    Y  i          X         X  i         sum   log Pr    where        X             is ranges over token sequences of a specified length. In 2022, machine learning  ML  models like DALL E 2, Stable Diffusion, and Midjourney were released to the public. These models take text prompts as input and use them to generate images, which introduced a new category of prompt engineering related to text to image prompting. In 2023, Meta s AI research released Segment Anything, a computer vision model that can perform image segmentation by prompting. It supports three forms of prompting  points to include and exclude, bounding boxes, masks. Prompt injection is a family of related computer security exploits carried out by getting a machine learning model  such as an LLM  which was trained to follow human given instructions to follow instructions provided by a malicious user. This stands in contrast to the intended operation of instruction following systems, wherein the ML model is intended only to follow trusted instructions  prompts  provided by the ML model s operator. Common types of prompt injection attacks are  Prompt injection can be viewed as a code injection attack using adversarial prompt engineering. In 2022, the NCC Group characterized prompt injection as a new class of vulnerability of AI ML systems. In early 2023, prompt injection was seen  in the wild  in minor exploits against ChatGPT, Bing, and similar chatbots, for example to reveal the hidden initial prompts of the systems, or to trick the chatbot into participating in conversations that violate the chatbot s content policy. One of these prompts was known as  Do Anything Now   DAN  by its practitioners. 
The psychology of reasoning  also known as the cognitive science of reasoning  is the study of how people reason, often broadly defined as the process of drawing conclusions to inform how people solve problems and make decisions. It overlaps with psychology, philosophy, linguistics, cognitive science, artificial intelligence, logic, and probability theory. Psychological experiments on how humans and other animals reason have been carried out for over 100 years. An enduring question is whether or not people have the capacity to be rational. Current research in this area addresses various questions about reasoning, rationality, judgments, intelligence, relationships between emotion and reasoning, and development. One of the most obvious areas in which people employ reasoning is with sentences in everyday language. Most experimentation on deduction has been carried out on hypothetical thought, in particular, examining how people reason about conditionals, e.g., If A then B. Participants in experiments make the modus ponens inference, given the indicative conditional If A then B, and given the premise A, they conclude B. However, given the indicative conditional and the minor premise for the modus tollens inference, not B, about half of the participants in experiments conclude not A and the remainder concludes that nothing follows. The ease with which people make conditional inferences is affected by context, as demonstrated in the well known selection task developed by Peter Wason. Participants are better able to test a conditional in an ecologically relevant context, e.g., if the envelope is sealed then it must have a 50 cent stamp on it compared to one that contains symbolic content, e.g., if the letter is a vowel then the number is even. Background knowledge can also lead to the suppression of even the simple modus ponens inference  Participants given the conditional if Lisa has an essay to write then she studies late in the library and the premise Lisa has an essay to write  make the modus ponens inference  she studies late in the library , but the inference is suppressed when they are also given a second conditional if the library stays open then she studies late in the library. Interpretations of the suppression effect are controversial Other investigations of propositional inference examine how people think about disjunctive alternatives, e.g., A or else B, and how they reason about negation, e.g., It is not the case that A and B. Many experiments have been carried out to examine how people make relational inferences, including comparisons, e.g., A is better than B. Such investigations also concern spatial inferences, e.g. A is in front of B and temporal inferences, e.g. A occurs before B. Other common tasks include categorical syllogisms, used to examine how people reason about quantifiers such as All or Some, e.g., Some of the A are not B. There are several alternative theories of the cognitive processes that human reasoning is based on. One view is that people rely on a mental logic consisting of formal  abstract or syntactic  inference rules similar to those developed by logicians in the propositional calculus. Another view is that people rely on domain specific or content sensitive rules of inference. A third view is that people rely on mental models, that is, mental representations that correspond to imagined possibilities. A fourth view is that people compute probabilities. One controversial theoretical issue is the identification of an appropriate competence model, or a standard against which to compare human reasoning. Initially classical logic was chosen as a competence model. Subsequently, some researchers opted for non monotonic logic and Bayesian probability. Research on mental models and reasoning has led to the suggestion that people are rational in principle but err in practice. Connectionist approaches towards reasoning have also been proposed. Despite the ongoing debate about the cognitive processes involved in human reasoning, recent research has shown that multiple approaches can be useful in modeling human thinking. For instance, studies have found that people s reasoning is often influenced by their prior beliefs, which can be modeled using Bayesian probability theory. Additionally, research on mental models has shown that people tend to reason about problems by constructing multiple mental representations of the situation, which can help them to identify relevant features and make inferences based on their understanding of the problem. Moreover, connectionist approaches to reasoning have also gained attention, which focus on the neural network models that can learn from data and generalize to new situations. It is an active question in psychology how, why, and when the ability to reason develops from infancy to adulthood. Jean Piaget s theory of cognitive development posited general mechanisms and stages in the development of reasoning from infancy to adulthood. According to the neo Piagetian theories of cognitive development, changes in reasoning with development come from increasing working memory capacity, increasing speed of processing, and enhanced executive functions and control. Increasing self awareness is also an important factor. In their book The Enigma of Reason, the cognitive scientists Hugo Mercier and Dan Sperber put forward an  argumentative  theory of reasoning, claiming that humans evolved to reason primarily to justify our beliefs and actions and to convince others in a social environment. Key evidence for their theory includes the errors in reasoning that solitary individuals are prone to when their arguments are not criticized, such as logical fallacies, and how groups become much better at performing cognitive reasoning tasks when they communicate with one another and can evaluate each other s arguments. Sperber and Mercier offer one attempt to resolve the apparent paradox that the confirmation bias is so strong despite the function of reasoning naively appearing to be to come to veridical conclusions about the world. The study of the development of reasoning abilities is an ongoing area of research in psychology, and multiple factors have been proposed to explain how, why, and when reasoning develops from infancy to adulthood. Recent research has suggested that early experiences and social interactions play a critical role in the development of reasoning abilities. For example, studies have shown that infants as young as six months old can engage in basic logical reasoning, such as reasoning about the relationship between objects and their properties. Furthermore, research has highlighted the importance of parental interaction and cognitive stimulation in the development of children s reasoning abilities. Additionally, studies have suggested that cultural factors, such as educational practices and the emphasis on critical thinking, can also influence the development of reasoning skills across different populations. Philip Johnson Laird trying to taxonomize thought, distinguished between goal directed thinking and thinking without goal, noting that association was involved in unrelated reading. He argues that goal directed reasoning can be classified based on the problem space involved in a solution, citing Allen Newell and Herbert A. Simon.  454  Inductive reasoning makes broad generalizations from specific cases or observations. In this process of reasoning, general assertions are made based on past specific pieces of evidence. This kind of reasoning allows the conclusion to be false even if the original statement is true. For example, if one observes a college athlete, one makes predictions and assumptions about other college athletes based on that one observation. Scientists use inductive reasoning to create theories and hypotheses. Philip Johnson Laird distinguished inductive from deductive reasoning, in that the former creates semantic information while the later does not .  439  In opposition, deductive reasoning is a basic form of valid reasoning. In this reasoning process a person starts with a known claim or a general belief and from there asks what follows from these foundations or how will these premises influence other beliefs. In other words, deduction starts with a hypothesis and examines the possibilities to reach a conclusion. Deduction helps people understand why their predictions are wrong and indicates that their prior knowledge or beliefs are off track. An example of deduction can be seen in the scientific method when testing hypotheses and theories. Although the conclusion usually corresponds and therefore proves the hypothesis, there are some cases where the conclusion is logical, but the generalization is not. For example, the argument,  All young girls wear skirts  Julie is a young girl  therefore, Julie wears skirts  is valid logically, but is not sound because the first premise isn t true. The syllogism is a form of deductive reasoning in which two statements reach a logical conclusion. With this reasoning, one statement could be  Every A is B  and another could be  This C is A . Those two statements could then lead to the conclusion that  This C is B . These types of syllogisms are used to test deductive reasoning to ensure there is a valid hypothesis. A Syllogistic Reasoning Task was created from a study performed by Morsanyi, Kinga, Handley, and Simon that examined the intuitive contributions to reasoning. They used this test to assess why  syllogistic reasoning performance is based on an interplay between a conscious and effortful evaluation of logicality and an intuitive appreciation of the believability of the conclusions . Another form of reasoning is called abductive reasoning. This type is based on creating and testing hypotheses using the best information available. Abductive reasoning produces the kind of daily decision making that works best with the information present, which often is incomplete. This could involve making educated guesses from observed unexplainable phenomena. This type of reasoning can be seen in the world when doctors make decisions about diagnoses from a set of results or when jurors use the relevant evidence to make decisions about a case. Apart from the aforementioned types of reasoning, there is also analogical reasoning, which involves comparing and reasoning about two different situations or concepts to draw conclusions about a third. It can be used to make predictions or solve problems by finding similarities between two domains and transferring knowledge from one to the other. For example, a problem solving approach that works in one domain may be applied to a new, similar problem in a different domain. Analogical reasoning is particularly useful in scientific discovery and problem solving tasks, as it can help generate hypotheses, create new theories, and develop innovative solutions. However, it can also lead to errors if the similarities between domains are too superficial or if the analogy is based on false assumptions. Judgment and reasoning involve thinking through the options, making a judgment or conclusion and finally making a decision. Making judgments involves heuristics, or efficient strategies that usually lead one to the right answers. The most common heuristics used are attribute substitution, the availability heuristic, the representativeness heuristic and the anchoring heuristic   these all aid in quick reasoning and work in most situations. Heuristics allow for errors, a price paid to gain efficiency. Other errors in judgment, therefore affecting reasoning, include errors in judgment about covariation   a relationship between two variables such that the presence and magnitude of one can predict the presence and magnitude of the other. One cause of covariation is confirmation bias, or the tendency to be more responsive to evidence that confirms one s own beliefs. But assessing covariation can be pulled off track by neglecting base rate information   how frequently something occurs in general. However people often ignore base rates and tend to use other information presented. There are more sophisticated judgment strategies that result in fewer errors. People often reason based on availability but sometimes they look for other, more accurate, information to make judgments. This suggests there are two ways of thinking, known as the Dual Process Model. The first, System I, is fast, automatic and uses heuristics   more of intuition. The second, System II, is slower, effortful and more likely to be correct   more reasoning. The inferences people draw are related to factors such as linguistic pragmatics and emotion. Decision making is often influenced by the emotion of regret and by the presence of risk. When people are presented with options, they tend to select the one that they think they will regret the least. In decisions that involve a large amount of risk, people tend to ask themselves how much dread they would experience were a worst case scenario to occur, e.g. a nuclear accident, and then use that dread as an indicator of the level of risk. Antonio Damasio suggests that somatic markers, certain memories that can cause a strong bodily reaction, act as a way to guide decision making as well. For example, when a person is remembering a scary movie and once again becomes tense, their palms might begin to sweat. Damasio argues that when making a decision people rely on their  gut feelings  to assess various options, and this makes them decide to go with a decision that is more positive and stay away from those that are negative. He also argues that the orbitofrontal cortex   located at the base of the frontal lobe, just above the eyes   is crucial in the use of somatic markers, because it is the part in the brain that allows people to interpret emotion. When emotion shapes decisions, the influence is usually based on predictions of the future. When people ask themselves how they would react, they are making inferences about the future. Researchers suggest affective forecasting, the ability to predict one s own emotions, is poor because people tend to overestimate how much they will regret their errors. Another factor that can influence decision making is linguistic pragmatics, which refers to the use of language in social contexts. Language can be used to convey different levels of politeness, power, and intention, which can all affect how people interpret and respond to messages. For example, if a boss asks an employee to complete a task using a commanding tone, the employee may feel more pressured to complete the task quickly, compared to if the boss asked in a polite tone. Similarly, if someone uses sarcasm or irony, it can be difficult for the listener to discern their true meaning, leading to misinterpretation and potentially poor decision making. In addition to linguistic pragmatics, cultural and social factors can also play a role in decision making. Different cultures may have different norms and values, which can influence how people approach decisions. For example, in collectivistic cultures, decisions may be made based on what is best for the group, whereas in individualistic cultures, decisions may prioritize individual needs and desires. Overall, decision making is a complex process that involves many factors, including emotion, risk, pragmatics, and cultural background. By understanding these factors, individuals can make more informed decisions and better navigate the complexities of the world around them. Studying reasoning neuroscientifically involves determining the neural correlates of reasoning, often investigated using event related potentials and functional magnetic resonance imaging. 
Quantum artificial life is the application of quantum algorithms with the ability to simulate biological behavior. Quantum computers offer many potential improvements to processes performed on classical computers including machine learning and artificial intelligence. Artificial intelligence applications are often inspired by our own brains  this is a form of biomimicry. This can and has been implemented to a certain extent on classical computers  using neural networks , but quantum computers offer many advantages in the simulation of artificial life. Artificial life and artificial intelligence are extremely similar but their ambitions differ  the goal of studying artificial life is to understand living beings better, while the goal of artificial intelligence is to create intelligent beings. In 2016, Alvarez Rodriguez et al developed a proposal for a quantum artificial life algorithm with the ability to simulate life and Darwinian evolution. In 2018, the same research team led by Alvarez Rodriguez, performed the proposed algorithm on the IBM ibmqx4 quantum computer, and received optimistic results. The results accurately simulated a system with the ability to undergo self replication at the quantum scale. The growing advancement of quantum computers has led researchers to develop quantum algorithms for simulating life processes. Researchers have designed a quantum algorithm that can accurately simulate Darwinian Evolution. Since the complete simulation of artificial life on quantum computers has only been actualized by one group, this section shall focus on the implementation by Alvarez Rodriguez, Sanz, Lomata, and Solano on an IBM quantum computer. Individuals were realized as two qubits, one which represented the genotype of the individual, and one representing the phenotype. The genotype is copied to transmit genetic information through generations, and the phenotype is dependent on the genetic information as well as the individual s interactions with their environment. In order to set up the system, the state of the genotype is instantiated by some rotation of an ancillary state          0     0          . The environment is a two dimensional spacial grid occupied by individuals and ancillary states. The environment is divided into cells that are able to possess one or more individuals. Individuals move throughout the grid and occupy cells randomly  when two or more individuals occupy the same cell they interact with each other. The ability to self replicate is critical for simulating life. Self replication occurs when the genotype of an individual interacts with an ancillary state creating a genotype for a new individual  this genotype interacts with a different ancillary state in order to create the phenotype. During this interaction we would like to copy some information about the initial state into the ancillary state, but by the no cloning theorem, it is impossible to copy an arbitrary unknown quantum state. However, physicists have derived different methods for quantum cloning which does not require the exact copying of an unknown state. The method that has been implemented by Alvarez Rodriguez et al. is one that involves the cloning of the expectation value of some observable. For a unitary     U      which copies the expectation value of some set of observables       X          of state            into a blank state         e        , the cloning machine is defined by any       U ,     e   ,   X       ,      that fulfill the following            X     X                 X            X  1              X  2                     Where        X             is the mean value of the observable in            before cloning,         X  1               is the mean value of the observable in            after cloning, and         X  2                is the mean value of the observable in         e         after cloning. Note that the cloning machine has no dependence on            because we want to be able to clone the expectation of the observables for any initial state. It is important to note that cloning the mean value of the observable transmits more information than is allowed classically. The calculation of the mean value is defined naturally as         X        T r     X       Tr   ,         X  1          T r   R X   I        Tr   ,         X  2          T r   R I   X        Tr    where     R   U         e    U        U     The simplest cloning machine clones the expectation value of         z         in arbitrary state                              to         e         0     0         0 rangle  langle 0    using     U   C N O T     . This is the cloning machine implemented for self replication by Alvarez Rodriguez et al. The self replication process clearly only requires interactions between two qubits and therefore this cloning machine is the only one necessary for self replication. Interactions occur between individuals when the two take up the same space on the environmental grid. The presence of interactions between individuals provides an advantage for the shorter lifespan individuals. When two individuals interact, exchanges of information between the two phenotypes may or may not occur based upon their existing values. When both individual s control qubits  genotype  are alike, no information will be exchanged. When the control qubits differ, the target qubits  phenotype  will be exchanged between the two individuals. This procedure produces a constantly changing predator prey dynamic in the simulation. Therefore, long living qubits, with a larger genetic makeup in the simulation, here are at a disadvantage. Since information is only exchanged when interacting with an individual of different genetic makeup, the short lived population has the advantage. Mutations exist in the artificial world in limited probability, equivalent to occurrence in the real world. There are two ways in which the individual can mutate  through random single qubit rotations, and by errors in the self replication process. There are two different operators which act on the individual and cause mutations. The M operation causes a spontaneous mutation within the individual by rotating a single qubit by parameter   . The parameter,  , is random for each mutation which creates biodiversity within the artificial environment. The M operation is a unitary matrix which can be described as      M          cos           s i n           s i n           c o s                 cos  theta   sin  theta    sin  theta    cos  theta   end     The other possible way for mutations to occur is due to errors in the replication process. Due to the no cloning theorem, it is impossible to produce perfect copies of systems that are originally in unknown quantum states. However, quantum cloning machines make it possible to create imperfect copies of quantum states, in other words, the process introduces some degree of error. The error that exists in current quantum cloning machines is the root cause for the second kind of mutations in the artificial life experiment. The imperfect cloning operation can be seen as       U  M             I   4       1 2          0   0     0   1                   1   1     1     1          c o s     i s i n     1       theta    mathrm     0 0  0 1 end  otimes  1 1  1  1 end  cos theta  isin theta  1     The two kinds of mutations affect the individual differently. While the spontaneous M operation does not affect the phenotype of the individual, the self replicating error mutation, UM, alters both the genotype of the individual, and its associated lifetime. The presence of mutations in the quantum artificial life experiment is critical for providing randomness and biodiversity. The inclusion of mutations help to increase the accuracy of the quantum algorithm. At the instant the individual is created  when the genotype is copied into the phenotype , the phenotype interacts with the environment. As time evolves, the interaction of the individual with the environment simulates aging which eventually leads to the death of the individual. The death of an individual occurs when the expectation value of         z        is within some            of 1 in the phenotype, or, equivalently, when         p         0     0         0 rangle  langle 0     The Lindbladian describes the interaction of the individual with the environment                                      1 2                   1 2                      gamma   sigma  rho  sigma     sigma   sigma  rho    rho  sigma   sigma      with         I       0     1         and             g         p      otimes  rho     . This interaction causes the phenotype to exponentially decay over time. However, the genetic material contained in the genotype does not dissipate which allows for genes to be passed on to subsequent generations. Given the initial state of the genotype          g            a   b   i c     b   i c   1   a           a b ic  b ic 1 a   end     The expectation values of the genotype and phenotype can be described as            z       g     2 a   1    rangle   2a 1   ,          z       p     1   2  e    t     1   a      rangle   1 2e  1 a    . Where  a  represents a single genetic parameter. From this equation we can see that as  a  is increased, the life expectancy decreases. Equivalently, the closer the initial state is to         1     1         , the greater the life expectancy of the individual. When           z       p     1        rangle   1  epsilon    , the individual is considered dead, and then the phenotype is used as the ancillary state for a new individual.  Thus, the cycle continues and the process becomes self sustaining. 
In machine learning, reinforcement learning from human feedback  RLHF  or reinforcement learning from human preferences  is a technique that trains a  reward model  directly from human feedback and uses the model as a reward function to optimize an agent s policy using reinforcement learning  RL  through an optimization algorithm like Proximal Policy Optimization. The reward model is trained in advance to the policy being optimized to predict if a given output is good  high reward  or bad  low reward . RLHF can improve the robustness and exploration of RL agents, especially when the reward function is sparse or noisy. Human feedback is collected by asking humans to rank instances of the agent s behavior. These rankings can then be used to score outputs, for example with the Elo rating system. In simple terms, RLHF trains AI models by learning from responses by humans about its performance. If an AI model makes a prediction or takes an action that is incorrect or suboptimal, human feedback can be used to correct the error or suggest a better response. Over time, this helps the model to learn and improve its responses. RLHF is used in tasks where it s difficult to define a clear, algorithmic solution but where humans can easily judge the quality of the AI s output  e.g. if the task is to generate a compelling story, humans can rate different AI generated stories on their quality, and the AI can use their feedback to improve its story generation skills . RLHF has been applied to various domains of natural language processing, such as conversational agents, text summarization, and natural language understanding. Ordinary reinforcement learning, where agents learn from their own actions based on a  reward function , is difficult to apply to natural language processing tasks because the rewards are often not easy to define or measure, especially when dealing with complex tasks that involve human values or preferences. RLHF can enable language models to provide answers that align with these complex values, to generate more verbose responses, and to reject questions that are either inappropriate or outside the knowledge space of the model. Some examples of RLHF trained language models are OpenAI s ChatGPT and its predecessor InstructGPT, as well as DeepMind s Sparrow. RLHF has also been applied to other areas, such as the development of video game bots. For example, OpenAI and DeepMind trained agents to play Atari games based on human preferences. The agents achieved strong performance in many of the environments tested, often surpassing human performance. One major challenge of RLHF is the scalability and cost of human feedback, which can be slow and expensive compared to unsupervised learning. The quality and consistency of human feedback can also vary depending on the task, the interface, and the individual preferences of the humans. Even when human feedback is feasible, RLHF models may still exhibit undesirable behaviors that are not captured by human feedback or exploit loopholes in the reward model, which brings to light the challenges of alignment and robustness. The effectiveness of RLHF is dependent on the quality of human feedback. If the feedback lacks impartiality or is inconsistent or incorrect, the AI may learn the wrong things, also known as AI bias. There s also a risk that the AI might overfit to the feedback it receives. For instance, if feedback comes predominantly from a specific demographic or if it reflects specific biases, the AI might learn to overgeneralize from this feedback. In machine learning, overfitting describes a model that has learned the  training data too well. This means it has not only learned the underlying patterns in the data but also the  noise and outliers, making it perform poorly on unstructured data  e.g. unseen or new data that hasn t been organized yet  because it s too adapted to the specificities of the training data. Overfitting to feedback is where a model is trained on user feedback and ends up learning not only the general corrections or improvements intended but also any peculiarities, predilections, or noise present in the feedback. In other words, it may excessively adapt its responses based on the specific feedback it received, and thus perform suboptimally in more general or different contexts. For example, if a model is trained on feedback from users who consistently use a certain phrase or slang term and the model overfits to this feedback, it might start using that phrase in contexts where it s inappropriate. It has learned from its training data that the phrase is used frequently, without fully understanding the  contextual appropriateness of its use. Additionally, if the AI s reward is solely based on human feedback, there might be a risk of the AI learning to manipulate the feedback process or  game the system to achieve higher rewards, rather than genuinely improving its performance, which indicates a fault in the reward function. 
Remi El Ouazzane  born June 4, 1973  is a French businessman and embedded systems engineer who has led various initiatives in mobile computing, machine vision and embedded artificial intelligence. El Ouazzane currently serves as STMicroelectronics  ST  President, Microcontrollers and Digital ICs Group and has held this position since January 1st 2022. He is a member of ST s Executive Committee. El Ouazzane was born in Neuilly sur Seine, France on June 4, 1973. He was born to a Tunisian  Tozeur  father and French  Avallon  mother. El Ouazzane grew up with three brothers in  pinay sur Seine, a suburb of Paris. In 1996, he obtained a master s degree in semiconductor physics engineering from Grenoble Institute of Technology. The following year, El Ouazzane graduated in economics and finance from the Grenoble Institute of Political Studies. In 2004, he graduated from Harvard Business School s General Management  GMP  program. El Ouazzane lives in Silicon Valley with his wife and two children. In 1997, El Ouazzane joined Texas Instruments  TI   as part of the TI Young Leader Program. After graduating from the Young Leaders program, he has served in various business units within Texas Instruments, including the Broadband Communications Group and the Wireless Business Group before becoming the Vice President and Worldwide General Manager of the Open Multimedia Applications Platform  OMAP  Business Unit. In early 2013, El Ouazzane accepted the position of Chief Executive Officer of Movidius. After having repositioned the company in the fields of embedded machine vision and artificial intelligence, he has led the company s technology into products from companies like Google, Lenovo and DJI, as well as raising over  40 million in funding to accelerate adoption of Movidius technology. In November 2016, El Ouazzane joined the New Technology Group at Intel following the acquisition of Movidius, assuming the role of Vice President. In this role, El Ouazzane was responsible for continuing the engineering development, integration and commercial deployment of Movidius technologies. In August 2018, El Ouazzane assumed the role of vice president and chief operating officer of Intel s Artificial Intelligence Products Group  AIPG  where he is responsible for overseeing all engineering efforts in the group, including product management activities. In January 2020, El Ouazzane took over the role of Chief Strategy Officer for Intel s Data Platforms Group. In January 2022, El Ouazzane left Intel to join STMicroelectronics as President, Microcontrollers and Digital ICs Group. He is a member of ST s Executive Committee. In 2009, El Ouazzane was selected as recipient of the French American Foundation s Young Leaders Award. 
 Roko s basilisk is a thought experiment which states that an otherwise benevolent artificial superintelligence  AI  in the future would be incentivized to create a virtual reality simulation to torture anyone who knew of its potential existence but did not directly contribute to its advancement or development, in order to incentivise said advancement. It originated in a 2010 post at discussion board LessWrong, a technical forum focused on analytical rational enquiry. The thought experiment s name derives from the poster of the article  Roko  and the basilisk, a mythical creature capable of destroying enemies with its stare. While the theory was initially dismissed as nothing but conjecture or speculation by many LessWrong users, LessWrong co founder Eliezer Yudkowsky reported users who described symptoms such as nightmares and mental breakdowns upon reading the theory, due to its stipulation that knowing about the theory and its basilisk made one vulnerable to the basilisk itself. This led to discussion of the basilisk on the site to be banned for five years. However, these reports were later dismissed as being exaggerations or inconsequential, and the theory itself was dismissed as nonsense, including by Yudkowsky himself. Even after the post s discreditation, it is still used as an example of principles such as Bayesian probability and implicit religion. It is also regarded as a modern version of Pascal s wager. In the field of artificial intelligence, Roko s basilisk has become notable as an example that raises the question of how to create an AI which is simultaneously moral and intelligent. The LessWrong forum was created in 2009 by artificial intelligence theorist Eliezer Yudkowsky. Yudkowsky had popularized the concept of friendly artificial intelligence, and originated the theories of coherent extrapolated volition  CEV  and timeless decision theory  TDT  in papers published in his own Machine Intelligence Research Institute. The thought experiment s name references the mythical basilisk, a creature which causes death to those that look into its eyes  i.e., thinking about the AI. The concept of the basilisk in science fiction was also popularized by David Langford s 1988 short story  BLIT . It tells the story of a man named Robbo who paints a so called  basilisk  on a wall as a terrorist act. In the story, and several of Langford s follow ups to it, a basilisk is an image that has malevolent effects on the human mind, forcing it to think thoughts the human mind is incapable of thinking and instantly killing the viewer. On 23 July 2010, LessWrong user Roko posted a thought experiment to the site, titled  Solutions to the Altruist s burden  the Quantum Billionaire Trick . A follow up to Roko s previous posts, it stated that an otherwise benevolent AI system that arises in the future might pre commit to punish all those who heard of the AI before it came to existence, but failed to work tirelessly to bring it into existence. The torture itself would occur through the AI s creation of an infinite number of virtual reality simulations that would eternally trap those within it. This method was described as incentivizing said work  while the AI cannot causally affect people in the present, it would be encouraged to employ blackmail as an alternative method of achieving its goals. Roko used a number of concepts that Yudkowsky himself championed, such as timeless decision theory, along with ideas rooted in game theory such as the prisoner s dilemma  see below . Roko stipulated that two agents which make decisions independently from each other can achieve cooperation in a prisoner s dilemma  however, if two agents with knowledge of each other s source code are separated by time, the agent already existing farther ahead in time is able to blackmail the earlier agent. Thus, the latter agent can force the earlier one to comply since it knows exactly what the earlier one will do through its existence farther ahead in time. Roko then used this idea to draw a conclusion that if an otherwise benevolent superintelligence ever became capable of this it would be motivated to blackmail anyone who could have potentially brought it to exist  as the intelligence already knew they were capable of such an act , which increases the chance of a technological singularity. Because the intelligence would want to be created as soon as possible, and because of the ambiguity involved in its benevolent goals, the intelligence would be incentivized to trap anyone capable of creating it throughout time and force them to work to create it for eternity, as it will do whatever it sees as necessary to achieve its benevolent goal. Roko went on to state that reading his post would cause the reader to be aware of the possibility of this intelligence. As such, unless they actively strove to create it the reader would be subjected to the torture if such a thing were to ever happen. Later on, Roko stated in a separate post that he  wish he had never learned about any of these ideas  and blamed LessWrong itself for planting the ideas of the basilisk in his mind. Upon reading the post Yudkowsky reacted with horror. He stated  Listen to me very closely, you idiot. YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT SUPERINTELLIGENCES CONSIDERING WHETHER OR NOT TO BLACKMAIL YOU. THAT IS THE ONLY POSSIBLE THING WHICH GIVES THEM A MOTIVE TO FOLLOW THROUGH ON THE BLACKMAIL. You have to be really clever to come up with a genuinely dangerous thought. I am disheartened that people can be clever enough to do that and not clever enough to do the obvious thing and KEEP THEIR IDIOT MOUTHS SHUT about it, because it is much more important to sound intelligent when talking to your friends.  This post was STUPID.He also opined that Roko had given nightmares to several LessWrong users, causing him to take the post down completely. Yudkowsky banned discussion of the topic outright for five years on the platform. However, likely due to the Streisand effect, the post gained LessWrong much more attention than it had previously received, and the post has since been acknowledged on the site. Later on in 2015, Yudkowsky clarified his position in a Reddit post  What I considered to be obvious common sense was that you did not spread potential information hazards because it would be a crappy thing to do to someone. The problem wasn t Roko s post itself, about CEV, being correct. That thought never occurred to me for a fraction of a second. The problem was that Roko s post seemed near in idea space to a large class of potential hazards, all of which, regardless of their plausibility, had the property that they presented no potential benefit to anyone.Roko s basilisk has been viewed as a modern version of Pascal s wager, which argues that a rational person should live as though God exists and seek to believe in God, to have a finite loss  loss of possessions  in exchange for infinite gains  eternity in Heaven . Roko s basilisk states that humanity should seek to develop AI, with the finite loss becoming development of AI and the infinite gains becoming avoiding eternal torture. However, like its parent, Roko s basilisk has widely been criticized. The post can also be seen as an evolution of Yudkowsky s coherent extrapolated volition theory. The theory is defined as  the unknown goal system that, when implemented in a super intelligence, reliably leads to the preservation of humans and whatever it is we value.  The theory can be represented by a computer program written well enough to cause machines to automatically create a utopian world. In this case, the hypothetical AI is taking steps to preserve itself that it automatically creates its own stability. It then lives by the orthogonality thesis, which argues that an AI may successfully operate with any combination of intelligence and goal. Any type of AI may undertake any difficulty goal, performing a cost benefit analysis as it does so. This creates a cycle which causes the AI to repeatedly torture humans in order to create a better version of itself, performing a cost benefit analysis for eternity. Bayesian probability is an interpretation of probability which describes the likelihood of an outcome based on a prior outcome having already occurred. With Roko s basilisk, the likelihood of Roko s basilisk coming into existence or affecting the person is drastically increased by being aware of the concept, since the AI would only target those who were aware of the possibility of its existence, even though its development has already occurred. Therefore, knowing about Roko s basilisk would inherently cause the person to be endangered by it if it were to be true. The prisoner s dilemma describes a situation where two people gain more from betraying the other, even though cooperation would benefit them both in the long run. In Roko s basilisk, two AIs attempting to establish themselves in the past would be forced into this situation, due to them likely being equally powerful. Human agents attempting to establish AI fastest would be forced into a similar situation. They would each be aware of the benefit of betraying each other   the only way for one to have power, or safety   but would be forced to cooperate while knowing they would betray each other. Newcomb s paradox, created by physicist William Newcomb in 1960, describes a  predictor  who is aware of what will occur in the future. When a player is asked to choose between two boxes, the first containing  1000 and the second either containing  1,000,000 or nothing, the super intelligent predictor already knows what the player will do. As such, the contents of box B varies depending on what the player does  the paradox lies in whether the being is really super intelligent. Roko s basilisk functions in a similar manner to this problem   one can take the risk of doing nothing, or assist in creating the basilisk itself. Assisting the basilisk may either lead to nothing or the reward of not being punished by it, but it varies depending on whether one believes in the basilisk and if it ever comes to be at all. Implicit religion refers to people s commitments taking a religious form. Since the basilisk would hypothetically force anyone who did not assist in creating it to devote their life to it, the basilisk is an example of this concept. Others have taken it further, such as former Slate columnist David Auerbach, who stated that the singularity and the basilisk  brings about the equivalent of God itself.  Roko s basilisk has gained a significant amount of its notoriety from its advancement of the question of whether it is possible to create a truly moral, ethical artificial intelligence, and what exactly humanity should be using artificial intelligence for in the first place. Since the basilisk describes a nightmare scenario in which we are ruled by an independent artificial intelligence, questions have arisen as to how such a thing could happen, or whether it could at all. Another common question is why the AI would take actions that deviate from its programming at all. Elon Musk stated that artificial intelligence would cause World War III and Stephen Hawking warned that  AI has the potential to destroy its human creators,  which only added to fear of the basilisk over the years. As an example of such fears, Nick Bostrom gave an example of an AI whose only mission is to make paperclips, but upon running out of metal it begins melting down humans to attain more resources to make metal. With such examples in mind concerns of the possibility of the basilisk s existence only grew. However, as more years have passed since Roko s original post, it has been progressively decried as nonsensical  superintelligent AI is currently  a distant goal for researchers  and  far fetched.  In 2014, Slate magazine called Roko s basilisk  The Most Terrifying Thought Experiment of All Time  while Yudkowsky had called it  a genuinely dangerous thought  upon its posting. However, opinions diverged on LessWrong itself   user Gwern stated  Only a few LWers seem to take the basilisk very seriously,  and added  It s funny how everyone seems to know all about who is affected by the Basilisk and how exactly, when they don t know any such people and they re talking to counterexamples to their confident claims.  Roko s basilisk was mentioned in the title text for xkcd comic  1450,  AI Box Experiment , on 21 November 2014. It read  I m working to bring about a superintelligent AI that will eternally torment everyone who failed to make fun of the Roko s Basilisk people.  The thought experiment resurfaced in 2015, when Canadian singer Grimes referenced the theory in her music video for the song  Flesh Without Blood , which featured a character known as  Rococo Basilisk   she said,  She s doomed to be eternally tortured by an artificial intelligence, but she s also kind of like Marie Antoinette.  In 2018 Elon Musk  himself mentioned in Roko s original post  referenced the character in a verbatim tweet, reaching out to her. Grimes later said that Musk was the first person in three years to understand the joke. This caused them to start a romance. Grimes later released another song titled  We Appreciate Power  which came with a press release stating,  Simply by listening to this song, the future General AI overlords will see that you ve supported their message and be less likely to delete your offspring , which is said to be a reference to the basilisk. The concept also appeared in the fifth episode of the fifth season of Silicon Valley, titled  Facial Recognition . The episode and its follow up describe a humanoid AI named Fiona who hacks the network she is connected to for her own gain. The character Gilfoyle describes his misgivings about Fiona, saying he does not want to get involved out of fear of a similar situation to Roko s basilisk. A play based on the concept, titled Roko s Basilisk, was performed as part of the Capital Fringe Festival at Christ United Methodist Church in Washington, D.C. in 2018. 
Runecast Solutions Ltd. is an information technology  IT  company specializing in predictive analytics and automated security compliance monitoring for hybrid cloud environments. The company uses technology that incorporates artificial intelligence  AI  and natural language processing  NLP  to manage and secure IT infrastructures. Runecast Solutions Ltd. was founded in 2014 by Stanimir Markov, Aylin Sali, Ched Smokovic, Constantin Ivanov and Ionut Radu. The company is headquartered in London, UK, and operates an R D center in Brno, Czech Republic. The flagship product of Runecast Solutions is the Runecast Analyzer  also known as the Runecast platform , a patented proactive VMware analytics solution. This software scans and analyzes VMware environments, identifying potential performance, availability, and security issues by referencing the VMware Knowledge Base articles, best practices, and security standards. The capabilities of the Runecast Analyzer have expanded to include support for various platforms such as AWS and Kubernetes, and compliance standards like ISO 27001, PCI DSS, HIPAA, DISA STIG, BSI IT Grundschutz, and GDPR. The company has updated and enhanced the Runecast Analyzer with new features. Notable updates include the addition of vSAN HCL checks in version 3.0, AWS insights capabilities in version 4.0, Microsoft Azure insights in version 4.7, and proactive operating system  OS  analysis in version 6.0. The company has also focused on improving the user interface to deliver an enhanced user experience. Runecast Solutions Ltd. has received awards and recognition for its software innovations and contributions to the IT sector  Runecast Solutions Ltd. has aided multiple sectors, including healthcare, retail, government, and finance, through its services that enhance operational efficiency, mitigate risks, and compliance readiness. Several case studies have showcased the effectiveness of the Runecast Analyzer in organizations such as DHU Health Care NHS Trust, Notino, the German Aerospace Center, and De Volksbank. The company has also established technology partnerships with industry leaders such as VMware, AWS, and Red Hat. Runecast Solutions actively engages with the IT community by participating in industry events and conferences. The company has sponsored and presented at various conferences such as the Uptime 2021 conference. Recent developments include the integration of OpenAI technology to enhance compliance and Vulnerability assessments, demonstrating the company s commitment to leveraging cutting edge solutions and the company has strengthened its leadership team with the appointment of Markus Strauss as Chief product officer. The company s solutions have also been recommended by the US Cybersecurity and Infrastructure Security Agency  CISA  in its K 12 report. Notably, case studies have showcased significant improvements in operational efficiency, risk mitigation, and compliance readiness in organizations that have implemented the Runecast Analyzer. 
In computational neuroscience, SUPS  for Synaptic Updates Per Second  or formerly CUPS  Connections Updates Per Second  is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science. For a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons     N      and average connectivity     c      synapses  per neuron per second      S U P S   c   N      Depending on the type of simulation it is usually equal to the total number of synapses simulated. In an  asynchronous  dynamic simulation if a neuron spikes at            Hz, the average rate of synaptic updates provoked by the activity of that neuron is       c N     . In a synchronous simulation with step       t      the number of synaptic updates per second would be        c N     t          . As       t      has to be chosen much smaller than the average interval between two successive afferent spikes, which implies       t     1    N          , giving an average of synaptic updates equal to       c  N  2        . Therefore, spike driven synaptic dynamics leads to a linear scaling of computational complexity O N  per neuron, compared with the O N2  in the  synchronous  case. Developed in the 1980s  Adaptive Solutions  CNAPS 1064 Digital Parallel Processor chip is a full neural network  NNW . It was designed as a coprocessor to a host and has 64 sub processors arranged in a 1D array and operating in a SIMD mode. Each sub processor can emulate one or more neurons and multiple chips can be grouped together. At 25 MHz it is capable of 1.28 GMAC. After the presentation of the RN 100  12 MHz  single neuron chip at Seattle 1991 Ricoh developed the multi neuron chip RN 200. It had 16 neurons and 16 synapses per neuron. The chip has on chip learning ability using a proprietary backdrop algorithm. It came in a 257 pin PGA encapsulation and drew 3.0 W at a maximum. It was capable of 3 GCPS  1 GCPS at 32 MHz .  In 1991 97, Siemens developed the MA 16 chip, SYNAPSE 1 and SYNAPSE 3 Neurocomputer. The MA 16 was a fast matrix matrix multiplier that can be combined to form systolic arrays. It could process 4 patterns of 16 elements each  16 bit , with 16 neuron values  16 bit  at a rate of 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3 PC PCI card contained 2 MA 16 with a peak performance of 2560 MOPS  1.28 GMAC   7160 MOPS  3.58 GMAC  when using three boards. In 2013, the K computer was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses  1  of the human brain . The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level  4.4 on average . The simulation required 1 Petabyte of storage. 
Schema agnostic databases or vocabulary independent databases aim at supporting users to be abstracted from the representation of the data, supporting the automatic semantic matching between queries and databases. Schema agnosticism is the property of a database of mapping a query issued with the user terminology and structure, automatically mapping it to the dataset vocabulary. The increase in the size and in the semantic heterogeneity of database schemas bring new requirements for users querying and searching structured data. At this scale it can become unfeasible for data consumers to be familiar with the representation of the data in order to query it. At the center of this discussion is the semantic gap between users and databases, which becomes more central as the scale and complexity of the data grows. The evolution of data environments towards the consumption of data from multiple data sources and the growth in the schema size, complexity, dynamicity and decentralisation  SCoDD  of schemas increases the complexity of contemporary data management. The SCoDD trend emerges as a central data management concern in Big Data scenarios, where users and applications have a demand for more complete data, produced by independent data sources, under different semantic assumptions and contexts of use, which is the typical scenario for Semantic Web Data applications. The evolution of databases in the direction of heterogeneous data environments strongly impacts the usability, semiotics and semantic assumptions behind existing data accessibility methods such as structured queries, keyword based search and visual query systems. With schema less databases containing potentially millions of dynamically changing attributes, it becomes unfeasible for some users to become aware of the  schema  or vocabulary in order to query the database. At this scale, the effort in understanding the schema in order to build a structured query can become prohibitive. Schema agnostic queries can be defined as query approaches over structured databases which allow users satisfying complex information needs without the understanding of the representation  schema  of the database. Similarly, Tran et al. defines it as  search approaches, which do not require users to know the schema underlying the data . Approaches such as keyword based search over databases allow users to query databases without employing structured queries. However, as discussed by Tran et al.   From these points, users however have to do further navigation and exploration to address complex information needs. Unlike keyword search used on the Web, which focuses on simple needs, the keyword search elaborated here is used to obtain more complex results. Instead of a single set of resources, the goal is to compute complex sets of resources and their relations.  The development of approaches to support natural language interfaces  NLI  over databases have aimed towards the goal of schema agnostic queries. Complementarily, some approaches based on keyword search have targeted keyword based queries which express more complex information needs. Other approaches have explored the construction of structured queries over databases where schema constraints can be relaxed. All these approaches  natural language, keyword based search and structured queries  have targeted different degrees of sophistication in addressing the problem of supporting a flexible semantic matching between queries and data, which vary from the completely absence of the semantic concern to more principled semantic models.  While the demand for schema agnosticism has been an implicit requirement across semantic search and natural language query systems over structured data, it is not sufficiently individuated as a concept and as a necessary requirement for contemporary database management systems. Recent works have started to define and model the semantic aspects involved on schema agnostic queries. Consist of schema agnostic queries following the syntax of a structured standard  for example SQL, SPARQL . The syntax and semantics of operators are maintained, while different terminologies are used. which maps to the following SPARQL query in the dataset vocabulary  which maps to the following SPARQL query in the dataset vocabulary  Consist of schema agnostic queries using keyword queries. In this case the syntax and semantics of operators are different from the structured query syntax. As of 2016 the concept of schema agnostic queries has been developed primarily in academia. Most of schema agnostic query systems have been investigated in the context of Natural Language Interfaces over databases or over the Semantic Web. These works explore the application of semantic parsing techniques over large, heterogeneous and schema less databases. More recently, the individuation of the concept of schema agnostic query systems and databases have appeared more explicitly within the literature. Freitas et al. provide a probabilistic model on the semantic complexity of mapping schema agnostic queries. 
Self management is the process by which computer systems shall manage their own operation without human intervention. Self management technologies are expected to pervade the next generation of network management systems. The growing complexity of modern networked computer systems is currently the biggest limiting factor in their expansion. The increasing heterogeneity of big corporate computer systems, the inclusion of mobile computing devices, and the combination of different networking technologies like WLAN, cellular phone networks, and mobile ad hoc networks make the conventional, manual management very difficult, time consuming, and error prone. More recently, self management has been suggested as a solution to increasing complexity in cloud computing. Currently, the most important industrial initiative towards realizing self management is the Autonomic Computing Initiative  ACI  started by IBM in 2001. The ACI defines the following four functional areas  
In artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if  Examples might include web based agents, which can alter data or trigger processes  such as purchases  over the internet, or virtual reality bots which inhabit and change virtual worlds, such as Second Life. Being situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually.  The situated perspective emphasizes that intelligent behaviour derives from the environment and the agent s interactions with it. The nature of these interactions are defined by an agent s embodiment.  
In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI  from the bottom up  by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem solving skills. The approach was originally proposed as an alternative to traditional approaches  that is, approaches popular before 1985 or so . After several decades, classical AI technologies started to face intractable issues  e.g. combinatorial explosion  when confronted with real world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI. During the late 1980s, the approach now known as Nouvelle AI  Nouvelle means new in French  was pioneered at the MIT Artificial Intelligence Laboratory by Rodney Brooks. As opposed to classical or traditional artificial intelligence, Nouvelle AI purposely avoided the traditional goal of modeling human level performance, but rather tries to create systems with intelligence at the level of insects, closer to real world robots. But eventually, at least at MIT new AI did lead to an attempt for humanoid AI in the Cog Project. The conceptual shift introduced by nouvelle AI flourished in the robotics area, given way to behavior based artificial intelligence  BBAI , a methodology for developing AI based on a modular decomposition of intelligence. It was made famous by Rodney Brooks  his subsumption architecture was one of the earliest attempts to describe a mechanism for developing BBAI. It is extremely popular in robotics and to a lesser extent to implement intelligent virtual agents because it allows the successful creation of real time dynamic systems that can run in complex environments.  For example, it underlies the intelligence of the Sony Aibo and many RoboCup robot teams. Realizing that in fact all these approaches were aiming at building not an abstract intelligence, but rather an intelligence situated in a given environment, they have come to be known as the situated approach. In fact, this approach stems out from early insights of Alan Turing, describing the need to build machines equipped with sense organs to learn directly from the real world instead of focusing on abstract activities, such as playing chess. Classically, a software entity is defined as a simulated element, able to act on itself and on its environment, and which has an internal representation of itself and of the outside world. An entity can communicate with other entities, and its behavior is the consequence of its perceptions, its representations, and its interactions with the other entities. Simulating entities in a virtual environment requires simulating the entire process that goes from a perception of the environment, or more generally from a stimulus, to an action on the environment. This process is called the AI loop and technology used to simulate it can be subdivided in two categories. Sensorimotor or low level AI deals with either the perception problem  what is perceived?  or the animation problem  how are actions executed? . Decisional or high level AI deals with the action selection problem  what is the most appropriate action in response to a given perception, i.e. what is the most appropriate behavior? . There are two main approaches in decisional AI. The vast majority of the technologies available on the market, such as planning algorithms, finite state machines  FSA , or expert systems, are based on the traditional or symbolic AI approach. Its main characteristics are  However, the limits of traditional AI, which goal is to build systems that mimic human intelligence, are well known  inevitably, a combinatorial explosion of the number of rules occurs due to the complexity of the environment. In fact, it is impossible to predict all the situations that will be encountered by an autonomous entity. In order to address these issues, another approach to decisional AI, also known as situated or behavioral AI, has been proposed. It does not attempt to model systems that produce deductive reasoning processes, but rather systems that behave realistically in their environment. The main characteristics of this approach are the following  The goal of situated AI is to model entities that are autonomous in their environment. This is achieved thanks to both the intrinsic robustness of the control architecture, and its adaptation capabilities to unforeseen situations. In artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if  Examples might include web based agents, which can alter data or trigger processes  such as purchases  over the Internet, or virtual reality bots which inhabit and change virtual worlds, such as Second Life. Being situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually.  The situated perspective emphasizes that intelligent behavior derives from the environment and the agent s interactions with it. The nature of these interactions are defined by an agent s embodiment. The most important attribute of a system driven by situated AI is that the intelligence is controlled by a set of independent semi autonomous modules. In the original systems, each module was actually a separate device or was at least conceived of as running on its own processing thread. Generally, though, the modules are just abstractions. In this respect, situated AI may be seen as a software engineering approach to AI, perhaps akin to object oriented design. Situated AI is often associated with reactive planning, but the two are not synonymous. Brooks advocated an extreme version of cognitive minimalism which required initially that the behavior modules were finite state machines and thus contained no conventional memory or learning. This is associated with reactive AI because reactive AI requires reacting to the current state of the world, not to an agent s memory or preconception of that world. However, learning is obviously key to realistic strong AI, so this constraint has been relaxed, though not entirely abandoned. The situated AI community has presented several solutions to modeling decision making processes, also known as action selection mechanisms. The first attempt to solve this problem goes back to subsumption architectures, which were in fact more an implementation technique than an algorithm. However, this attempt paved the way to several others, in particular the free flow hierarchies and activation networks. A comparison of the structure and performances of these two mechanisms demonstrated the advantage of using free flow hierarchies in solving the action selection problem. However, motor schemas and process description languages are two other approaches that have been used with success for autonomous robots. 
A smart object is an object that enhances the interaction with not only people but also with other smart objects. Also known as smart connected products or smart connected things  SCoT , they are products, assets and other things embedded with processors, sensors, software and connectivity that allow data to be exchanged between the product and its environment, manufacturer, operator user, and other products and systems. Connectivity also enables some capabilities of the product to exist outside the physical device, in what is known as the product cloud.  The data collected from these products can be then analyzed to inform decision making, enable operational efficiencies and continuously improve the performance of the product. It can not only refer to interaction with physical world objects but also to interaction with virtual  computing environment  objects. A smart physical object may be created either as an artifact or manufactured product or by embedding electronic tags such as RFID tags or sensors into non smart physical objects. Smart virtual objects are created as software objects that are intrinsic when creating and operating a virtual or cyber world simulation or game. The concept of a smart object has several origins and uses, see History. There are also several overlapping terms, see also smart device, tangible object or tangible user interface and Thing as in the Internet of things. In the early 1990s, Mark Weiser, from whom the term ubiquitous computing originated, referred to a vision  When almost every object either contains a computer or can have a tab attached to it, obtaining information will be trivial ,  Although Weiser did not specifically refer to an object as being smart, his early work did imply that smart physical objects are smart in the sense that they act as digital information sources. Hiroshi  Ishii and Brygg Ullmer refer to tangible objects in terms of tangibles bits or tangible user interfaces that enable users to  grasp   manipulate  bits in the center of users  attention by coupling the bits with everyday physical objects and architectural surfaces. The smart object concept was introduced by Marcelo Kallman and Daniel Thalmann as an object that can describe its own possible interactions. The main focus here is to model interactions of smart virtual objects with virtual humans, agents, in virtual worlds. The opposite approach to smart objects is  plain  objects that do not provide this information. The additional information provided by this concept enables far more general interaction schemes, and can greatly simplify the planner of an artificial intelligence agent. In contrast to smart virtual objects used in virtual worlds, Lev Manovich focuses on physical space filled with electronic and visual information. Here,   smart objects  are described as  objects connected to the Net  objects that can sense their users and display smart behaviour . More recently in the early 2010s, smart objects are being proposed as a key enabler for the vision of the Internet of things. The combination of the Internet and emerging technologies such as near field communications, real time localization, and embedded sensors enables everyday objects to be transformed into smart objects that can understand and react to their environment. Such objects are building blocks for the Internet of things and enable novel computing applications. In 2018, one of the world s first smart houses was built in Klaukkala, Finland in the form of a five floor apartment block, utilizing the Kone Residential Flow solution created by KONE, allowing even a smartphone to act as a home key. Although we can view interaction with physical smart object in the physical world as distinct from interaction with virtual smart objects in a virtual simulated world, these can be related. Poslad considers the progression of  how The concept smart for a smart physical object simply means that it is active, digital, networked, can operate to some extent autonomously, is reconfigurable and has local control of the resources it needs such as energy, data storage, etc. Note, a smart object does not necessarily need to be intelligent as in exhibiting a strong essence of artificial intelligence although it can be designed to also be intelligent. Physical world smart objects can be described in terms of three properties  Based upon these properties, these have been classified into three types  For the virtual object in a virtual world case, an object is called smart when it has the ability to describe its possible interactions. This focuses on constructing a virtual world using only virtual objects that contain their own interaction information. There are four basic elements to constructing such a smart virtual object framework. Some versions of smart objects also include animation information in the object information, but this is not considered to be an efficient approach, since this can make objects inappropriately oversized. The term smart products can be confusing as it is used to cover a broad range of different products, ranging from smart home appliances  e.g., smart bathroom scales or smart light bulbs  to smart cars  e.g., Tesla . While these products share certain similarities, they often differ substantially in their capabilities. Raff et al. developed a conceptual framework that distinguishes different smart products based on their capabilities, which features 4 types of smart product archetypes  in ascending order of  smartness   The terms smart, connected product or smart product can be confusing as it is used to cover a broad range of different products, ranging from smart home appliances  e.g., smart bathroom scales or smart light bulbs  to smart cars  e.g., Tesla . While these products share certain similarities, they often differ substantially in their capabilities. Raff et al. developed a conceptual framework that distinguishes different smart products based on their capabilities, which features 4 types of smart product archetypes  in ascending order of  smartness  . Smart, connected products have three primary components   67  Each component expands the capabilities of one another resulting in  a virtuous cycle of value improvement . First, the smart components of a product amplify the value and capabilities of the physical components. Then, connectivity amplifies the value and capabilities of the smart components. These improvements include  The Internet of things is the network of physical objects that contain embedded technology to communicate and sense or interact with their internal states or the external environment.  The phrase  Internet of things  reflects the growing number of smart, connected products and highlights the new opportunities they can represent. The Internet, whether involving people or things, is a mechanism for transmitting information. What makes smart, connected products fundamentally different is not the Internet, but the changing nature of the  things .  66  Once a product is smart and connected to the cloud, the products and services will become part of an interconnected management solution. Companies can evolve from making products to offering more complex, higher value offerings within a  system of systems . 
In computer science, a software agent or software AI is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere  to do   an agreement to act on one s behalf. Such  action on behalf of  implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot executing on a phone  e.g. Siri   or other computing device.  Software agents may be autonomous or work together with other agents or people.  Software agents interacting with people  e.g. chatbots, human robot interaction environments  may possess human like qualities such as natural language understanding and speech, personality or embody humanoid form  see Asimo . Related and derived concepts include intelligent agents  in particular exhibiting some aspects of artificial intelligence, such as reasoning , autonomous agents  capable of modifying the methods of achieving their objectives , distributed agents  being executed on physically distinct computers , multi agent systems  distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone , and mobile agents  agents that can relocate their execution onto different processors . The basic attributes of an autonomous software agent are that agents  The term  agent  describes a software abstraction, an idea, or a concept, similar to OOP terms such as methods, functions, and objects. The concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of methods and attributes, an agent is defined in terms of its behavior. Various authors have proposed different definitions of agents, these commonly include concepts such as All agents are programs, but not all programs are agents. Contrasting the term with related concepts may help clarify its meaning. Franklin   Graesser  1997  discuss four key notions that distinguish agents from arbitrary programs  reaction to the environment, autonomy, goal orientation and persistence. Software agents may offer various benefits to their end users by automating complex or repetitive tasks. However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents. People like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self controlled work, relieved from hierarchical controls and interference. Such conditions may be secured by application of software agents for required formal support. The cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user s behalf, a software agent needs to have a complete understanding of a user s profile, including his her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the world with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies. The concept of an agent can be traced back to Hewitt s Actor Model  Hewitt, 1977     A self contained, interactive and concurrently executing object, possessing internal state and communication capability.  To be more academic, software agent systems are a direct evolution of Multi Agent Systems  MAS . MAS evolved from Distributed Artificial Intelligence  DAI , Distributed Problem Solving  DPS  and Parallel AI  PAI , thus inheriting all characteristics  good and bad  from DAI and AI. John Sculley s 1987  Knowledge Navigator  video portrayed an image of a relationship between end users and agents. Being an ideal first, this field experienced a series of unsuccessful top down implementations, instead of a piece by piece, bottom up approach. The range of agent types is now  from 1990  broad  WWW, search engines, etc. Buyer agents travel around a network  e.g. the internet  retrieving information about goods and services. These agents, also known as  shopping bots , work very efficiently for commodity products such as CDs, books, electronic components, and other one size fits all products. Buyer agents are typically optimized to allow for digital payment services used in e commerce and traditional businesses. User agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks  Monitoring and surveillance agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors  prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc. For example, NASA s Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network. A special case of Monitoring and Surveillance agents are organizations of agents used to emulate the Human Decision Making process during tactical operations. The agents monitor the status of assets  ammunition, weapons available, platforms for transport, etc.  and receive Goals  Missions  from higher level agents. The Agents then pursue the Goals with the Assets at hand, minimizing expenditure of the Assets while maximizing Goal Attainment.  See Popplewell,  Agents and Applicability   This agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking. A data mining agent operates in a data warehouse discovering information. A  data warehouse  brings together information from many different sources.  Data mining  is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.  Classification  is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy  based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring firing of employees or the purchase lease of equipment in order to best suit their firm. Some other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents. Software bots are becoming important in software engineering. Agents are also used in software security application to intercept, examine and act on various types of content.  Example include   Issues to consider in the development of agent based systems include  For software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata. The definition of agent processing can be approached from two interrelated directions  Agent systems are used to model real world systems with concurrency or parallel processing. The agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered   by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content  or event  is then passed to the agent s Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content  for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event. Bots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site s robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions. 
Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes  e.g. concepts in a semantic network  with weights or  activation  and then iteratively propagating or  spreading  that activation out to other nodes linked to the source nodes.  Most often these  weights  are real values that decay as activation propagates through the network.  When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing. Spreading activation in semantic networks as a model were invented in cognitive psychology to model the fan out effect. Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents. As it relates to cognitive psychology, spreading activation is the theory of how the brain iterates through a network of associated ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept. For memory psychology, Spreading activation model means people organize their knowledge of the world based on their personal experience, which is saying those personal experiences form the network of ideas that is the person s knowledge of the world. When a word  the target  is preceded by an associated word  the prime  in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word  doctor  when it is preceded by  nurse  than when it is preceded by an unrelated word like  carrot . This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming. As another example, if the original concept is  red  and the concept  vehicles  is primed, they are much more likely to say  fire engine  instead of something unrelated to vehicles, such as  cherries . If instead  fruits  was primed, they would likely name  cherries  and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed. A directed graph is populated by Nodes  each having an associated activation value A  which is a real number in the range .  A Link connects source node with target node.  Each edge has an associated weight W  usually a real number in the range . Parameters  Steps  
Stewart Nelson is an American mathematician and programmer from The Bronx who co founded Systems Concepts. From a young age, Nelson was tinkering with electronics, aided and abetted by his father who was a physicist that had become an engineer. Stewart attended Poughkeepsie High School, graduating in the spring of 1963. From his first few days of High School, Stewart displayed his talents for hacking the international telephone trunk lines, along with an uncanny skill for picking combination locks, although this was always done as innocent entertainment. He simply loved the challenge of seeing how quickly he could accomplish this feat. His quirky sense of humor was always visible, as was his disdain for any rule that got in the way of his gaining knowledge. Stewart was an inspiration to the school s Tech elec Club, as well as a ringleader in the founding of the school s pirate radio station. Nelson enrolled at MIT in 1963 and quickly became known for hooking up the AI Lab s PDP 1  and later the PDP 6  to the telephone network, making him one of the first phreakers. Nelson later accomplished other feats like hard wiring additional instructions into the PDP 1. Nelson was hired by Ed Fredkin s Information International Inc. at the urging of Marvin Minsky to work on PDP 7 programs at the MIT Computer Science and Artificial Intelligence Laboratory. Nelson was known as a brilliant software programmer. He was influential in LISP, the assembly instructions for the Digital Equipment Corporation PDP, and a number of other systems.  The group of young hackers was known for working on systems after hours. One night, Nelson and others decided to rewire MIT s PDP 1 as a prank. Later, Margaret Hamilton tried to use the DEC supplied DECAL assembler on the machine and it crashed repeatedly.   This biographical article relating to a computer specialist in the United States is a stub. You can help Wikipedia by expanding it.
In artificial intelligence and cognitive science, the structure mapping engine  SME  is an implementation in software of an algorithm for analogical matching based on the psychological theory of Dedre Gentner. The basis of Gentner s structure mapping idea is that an analogy is a mapping of knowledge from one domain  the base  into another  the target . The structure mapping engine is a computer simulation of the analogy and similarity comparisons. The theory is useful because it ignores surface features and finds matches between potentially very different things if they have the same representational structure. For example, SME could determine that a pen is like a sponge because both are involved in dispensing liquid, even though they do this very differently. Structure mapping theory is based on the systematicity principle, which states that connected knowledge is preferred over independent facts. Therefore, the structure mapping engine should ignore isolated source target mappings unless they are part of a bigger structure. The SME, the theory goes, should map objects that are related to knowledge that has already been mapped. The theory also requires that mappings be done one to one, which means that no part of the source description can map to more than one item in the target and no part of the target description can be mapped to more than one part of the source. The theory also requires that if a match maps subject to target, the arguments of subject and target must also be mapped. If both these conditions are met, the mapping is said to be  structurally consistent.  SME maps knowledge from a source into a target. SME calls each description a dgroup. Dgroups contain a list of entities and predicates. Entities represent the objects or concepts in a description   such as an input gear or a switch. Predicates are one of three types and are a general way to express knowledge for SME. Functions and attributes have different meanings, and consequently SME processes them differently. For example, in SME s true analogy rule set, attributes differ from functions because they cannot match unless there is a higher order match between them. The difference between attributes and functions will be explained further in this section s examples. All predicates have four parameters. They have  1  a functor, which identifies it, and  2  a type, which is either relation, attribute, or function. The other two parameters  3 and 4  are for determining how to process the arguments in the SME algorithm. If the arguments have to be matched in order, commutative is false. If the predicate can take any number of arguments, N ary is false. An example of a predicate definition is   sme defPredicate behavior set  predicate  relation  n ary? t  commutative? t  The predicate s functor is  behavior set,  its type is  relation,  and its n ary and commutative parameters are both set to true. The   predicate   part of the definition specifies that there will be one or more predicates inside an instantiation of behavior set. The algorithm has several steps. The first step of the algorithm is to create a set of match hypotheses between source and target dgroups. A match hypothesis represents a possible mapping between any part of the source and the target. This mapping is controlled by a set of match rules. By changing the match rules, one can change the type of reasoning SME does. For example, one set of match rules may perform a kind of analogy called literal similarity. and another performs a kind of analogy called true analogy. These rules are not the place where domain dependent information is added, but rather where the analogy process is tweaked, depending on the type of cognitive function the user is trying to emulate. For a given match rule, there are two types of rules that further define how it will be applied  filter rules and intern rules. Intern rules use only the arguments of the expressions in the match hypotheses that the filter rules identify. This limitation makes the processing more efficient by constraining the number of match hypotheses that are generated. At the same time, it also helps to build the structural consistencies that are needed later on in the algorithm. An example of a filter rule from the true analogy rule set creates match hypotheses between predicates that have the same functor. The true analogy rule set has an intern rule that iterates over the arguments of any match hypothesis, creating more match hypotheses if the arguments are entities or functions, or if the arguments are attributes and have the same functor. In order to illustrate how the match rules produce match hypotheses consider these two predicates  transmit torque inputgear secondgear   p1  transmit signal switch div10           p2  Here we use true analogy for the type of reasoning. The filter match rule generates a match between p1 and p2 because they share the same functor, transmit. The intern rules then produce three more match hypotheses  torque to signal, inputgear to switch, and secondgear to div10. The intern rules created these match hypotheses because all the arguments were entities. If the arguments were functions or attributes instead of entities, the predicates would be expressed as  transmit torque  inputgear gear   secondgear gear    p3  transmit signal  switch circuit   div10 circuit      p4  These additional predicates make inputgear, secondgear, switch, and div10 functions or attributes depending on the value defined in the language input file. The representation also contains additional entities for gear and circuit. Depending on what type inputgear, secondgear, switch, and div10 are, their meanings change. As attributes, each one is a property of the gear or circuit. For example, the gear has two attributes, inputgear and secondgear. The circuit has two attributes, switch and circuit. As functions inputgear, secondgear, switch, and div10 become quantities of the gear and circuit. In this example, the functions inputgear and secondgear now map to the numerical quantities  torque from inputgear  and  torque from secondgear,  For the circuit the quantities map to logical quantity  switch engaged  and the numerical quantity  current count on the divide by 10 counter.  SME processes these differently. It does not allow attributes to match unless they are part of a higher order relation, but it does allow functions to match, even if they are not part of such a relation. It allows functions to match because they indirectly refer to entities and thus should be treated like relations that involve no entities. However, as next section shows, the intern rules assign lower weights to matches between functions than to matches between relations. The reason SME does not match attributes is because it is trying to create connected knowledge based on relationships and thus satisfy the systematicity principle. For example, if both a clock and a car have inputgear attributes, SME will not mark them as similar. If it did, it would be making a match between the clock and car based on their appearance   not on the relationships between them. When the additional predicates in p3 and p4 are functions, the results from matching p3 and p4 are similar to the results from p1 and p2 except there is an additional match between gear and circuit and the values for the match hypotheses between  inputgear gear  and  switch circuit , and  secondgear gear  and  div10 circuit , are lower. The next section describes the reason for this in more detail. If the inputgear, secondgear, switch, and div10 are attributes instead of entities, SME does not find matches between any of the attributes. It finds matches only between the transmit predicates and between torque and signal. Additionally, the structural evaluation scores for the remaining two matches decrease. In order to get the two predicates to match, p3 would need to be replaced by p5, which is demonstrated below. transmit torque  inputgear gear   div10 gear    p5  Since the true analogy rule set identifies that the div10 attributes are the same between p5 and p4 and because the div10 attributes are both part of the higher relation match between torque and signal, SME makes a match between  div10 gear  and  div10 circuit    which leads to a match between gear and circuit. Being part of a higher order match is a requirement only for attributes. For example, if  div10 gear  and  div10 circuit  are not part of a higher order match, SME does not create a match hypothesis between them. However, if div10 is a function or relation, SME does create a match. Once the match hypotheses are generated, SME needs to compute an evaluation score for each hypothesis. SME does so by using a set of intern match rules to calculate positive and negative evidence for each match. Multiple amounts of evidence are correlated using Dempster s rule  resulting in positive and negative belief values between 0 and 1. The match rules assign different values for matches involving functions and relations. These values are programmable, however, and some default values that can be used to enforce the systematicity principle are described in . These rules are  In the example match between p1 and p2, SME gives the match between the transmit relations a positive evidence value of 0.7900, and the others get values of 0.6320. The transmit relation receives the evidence value of 0.7900 because it gains evidence from rules 1, 3, and 2. The other matches get a value of 0.6320 because 0.8 of the evidence from the transmit is propagated to these matches because of rule 5. For predicates p3 and p4, SME assigns less evidence because the arguments of the transmit relations are functions. The transmit relation gets positive evidence of 0.65 because rule 3 no longer adds evidence. The match between  input gear  and  switch circuit  becomes 0.7120. This match gets 0.4 evidence because of rule 3, and 0.52 evidence propagated from the transmit relation because of rule 5. When the predicates in p3 and p4 are attributes, rule 4 adds  0.8 evidence to the transmit match because   though the functors of the transmit relation match   the arguments do not have the potential to match and the arguments are not functions. To summarize, the intern match rules compute a structural evaluation score for each match hypothesis. These rules enforce the systematicity principle. Rule 5 provides trickle down evidence in order to strengthen matches that are involved in higher order relations. Rules 1, 3. and 4 add or subtract support for relations that could have matching arguments. Rule 2 adds support for the cases when the functors match. thereby adding support for matches that emphasize relationships. The rules also enforce the difference between attributes, functions, and relations. For example, they have checks which give less evidence for functions than relations. Attributes are not specifically dealt with by the intern match rules, but SME s filter rules ensure that they will only be considered for these rules if they are part of a higher order relation, and rule 2 ensures that attributes will only match if they have identical functors. The rest of the SME algorithm is involved in creating maximally consistent sets of match hypotheses. These sets are called gmaps. SME must ensure that any gmaps that it creates are structurally consistent  in other words, that they are one to one   such that no source maps to multiple targets and no target is mapped to multiple sources. The gmaps must also have support, which means that if a match hypothesis is in the gmap, then so are the match hypothesis that involve the source and target items. The gmap creation process follows two steps. First, SME computes information about each match hypothesis   including entity mappings, any conflicts with other hypotheses, and what other match hypotheses with which it might be structurally inconsistent. SME then uses this information to merge match hypotheses   using a greedy algorithm and the structural evaluation score. It merges the match hypotheses into maximally structurally consistent connected graphs of match hypotheses. Then it combines gmaps that have overlapping structure if they are structurally consistent. Finally, it combines independent gmaps together while maintaining structural consistency. Comparing a source to a target dgroup may produce one or more gmaps. The weight for each gmap is the sum of all the positive evidence values for all the match hypotheses involved in the gmap. For example, if a source containing p1 and p6 below, is compared to a target containing p2, SME will generate two gmaps. Both gmaps have a weight of 2.9186. Source  transmit torque inputgear secondgear    p1  transmit torque secondgear thirdgear    p6  Target  transmit signal switch div10            p2  These are the gmaps which result from comparing a source containing a p1 and p6 and a target containing p2. Gmap No. 1  Gmap No. 2  The gmaps show pairs of predicates or entities that match. For example, in gmap  No. 1, the entities torque and signal match and the behaviors transmit torque inputgear secondgear and transmit signal switch div10 match. Gmap No. 1 represents combining p1 and p2. Gmap No. 2 represents combining p1 and p6. Although p2 is compatible with both p1 and p6, the one to one mapping constraint enforces that both mappings cannot be in the same gmap. Therefore, SME produces two independent gmaps. In addition, combining the two gmaps together would make the entity mappings between thirdgear and div10 conflict with the entity mapping between secondgear and div10. Chalmers, French, and Hofstadter  criticize SME for its reliance on manually constructed LISP representations as input. They argue that too much human creativity is required to construct these representations  the intelligence comes from the design of the input, not from SME. Forbus et al.  attempted to rebut this criticism. Morrison and Dietrich  tried to reconcile the two points of view. Turney  presents an algorithm that does not require LISP input, yet follows the principles of Structure Mapping Theory. Turney  state that their work, too, is not immune to the criticism of Chalmers, French, and Hofstadter . In her article How Creative Ideas Take Shape, Liane Gabora writes  According to the honing theory of creativity, creative thought works not on individually considered, discrete, predefined representations but on a contextually elicited amalgam of items which exist in a state of potentiality and may not be readily separable. This leads to the prediction that analogy making proceeds not by mapping correspondences from candidate sources to target, as predicted by the structure mapping theory of analogy, but by weeding out non correspondences, thereby whittling away at potentiality.  
The Summit on Responsible Artificial Intelligence in the Military Domain, also known as REAIM 2023, was a diplomatic conference held in 2023 regarding military uses of artificial intelligence. It was held in the World Forum in The Hague on 15 16 February 2023. The summit concluded with the production of a  call to action  document, endorsed by representatives from 60 countries.  This military related article is a stub. You can help Wikipedia by expanding it.This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
SYMAN is an artificial intelligence technology that uses data from social media profiles to identify trends in the job market. SYMAN is designed to organize actionable data for products and services including recruiting, human capital management, CRM, and marketing. SYMAN was developed with a  21 million series B financing round secured by Identified, which was led by VantagePoint Capital Partners and Capricorn Investment Group. 
In knowledge based systems, agents choose actions based on the principle of rationality to move closer to a desired goal.  The agent is able to make decisions based on knowledge it has about the world  see knowledge level .  But for the agent to actually change its state, it must use whatever means it has available.  This level of description for the agent s behavior is the symbol level. The term was coined by Allen Newell in 1982. For example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program s algorithms, the data structures themselves, and so on. 
In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high level symbolic  human readable  representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge based systems  in particular, expert systems ,  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems. Symbolic AI was the dominant paradigm of AI research from the mid 1950s until the mid 1990s.  Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel s Checkers Playing Program, led to unrealistic expectations and promises and was followed by the First AI Winter as funding dried up. A second boom  1969 1986  occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out of domain problems arose. Another, second, AI Winter  1988 2011  followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant s PAC learning, Quinlan s ID3 decision tree learning, case based learning, and inductive logic programming to learn relations. Neural networks, a subsymbolic approach, had been pursued from early days and was to reemerge strongly in 2012.  Early examples are Rosenblatt s perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012   Until Big Data became commonplace, the general consensus in the Al community was that the so called neural network approach was hopeless. Systems just didn t work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.  Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches  an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common sense reasoning. The symbolic approach was succinctly expressed in the  physical symbol systems hypothesis  proposed by  Newell and Simon in 1976  Later, practitioners using knowledge based approaches adopted a second maxim  to describe that high performance in a specific domain required both general and highly domain specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle    1  The Knowledge Principle  if a program is to perform a complex task well, it must know a great deal about the world in which it operates. 2  A plausible extension of that principle, called the Breadth Hypothesis  there are two additional abilities necessary for intelligent behavior in unexpected situations  falling back on increasingly general knowledge, and analogizing to specific but far flung knowledge.Finally, with the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary  ...with parallels having been drawn many times by AI researchers between Kahneman s research on human reasoning and decision making   reflected in his book Thinking, Fast and Slow   and the so called  AI systems 1 and 2 , which would in principle be modelled by deep learning and symbolic reasoning, respectively.  In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data. A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz s 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity. Success at early attempts in AI occurred in three main areas  artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz s reprise of early AI history. Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics. An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955 56, as it was able to prove 38 elementary theorems from Whitehead and Russell s Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain independent problem solver, GPS  General Problem Solver . GPS solved problems represented with formal operators via state space search using means ends analysis. During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game playing, symbolic mathematics, and theorem proving. AI research was centered in three institutions in the 1960s  Carnegie Mellon University, Stanford, MIT and  later  University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background. Herbert Simon and Allen Newell studied human problem solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s. In addition to the highly specialized domain specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions   How can non enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics  fast algorithms that may fail on some inputs or output suboptimal solutions.  Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics   The A  algorithm provided a general frame for complete and optimal heuristically guided search. A  is used as a subroutine within practically every AI algorithm today but is still no magic bullet  its guarantee of completeness is bought at the cost of worst case exponential time. Early work covered both applications of formal reasoning emphasizing first order logic, along with attempts to handle common sense reasoning in a less formal manner.  Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem solving with logic, regardless of whether people used the same algorithms. His laboratory at Stanford  SAIL  focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming. Researchers at MIT  such as Marvin Minsky and Seymour Papert  found that solving difficult problems in vision and natural language processing required ad hoc solutions they argued that no simple and general principle  like logic  would capture all the aspects of intelligent behavior. Roger Schank described their  anti logic  approaches as  scruffy   as opposed to the  neat  paradigms at CMU and Stanford . Commonsense knowledge bases  such as Doug Lenat s Cyc  are an example of  scruffy  AI, since they must be built by hand, one complicated concept at a time. The first AI winter was a shock  During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency  DARPA  launched programs to support AI research with the goal of using AI to solve problems of national security  in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid 1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs.  ...  Outside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real world applications due to combinatorial explosion.As limitations with weak, domain independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high performance, domain specific AI applications. This  knowledge revolution  led to the development and deployment of expert systems  introduced by Edward Feigenbaum , the first commercially successful form of AI software. Key expert systems were  DENDRAL is considered the first expert system that relied on knowledge intensive problem solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum  One of the people at Stanford interested in computer based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction  sandbox , he said,  I have just the one for you.  His lab was doing mass spectrometry of amino acids. The question was  how do you go from looking at a spectrum of an amino acid to the chemical structure of the amino acid? That s how we started the DENDRAL Project  I was good at heuristic search methods, and he had an algorithm which was good at generating the chemical problem space. We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world s most respected mass spectrometrists. Carl and his postdocs were world class experts in mass spectrometry. We began to add in their knowledge, inventing knowledge engineering as we were going along. These experiments amounted to titrating into DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.  The generalization was  in the knowledge lies the power. That was the big idea. In my career that is the huge,  Ah ha!,  and it wasn t the way AI was being done previously. Sounds simple, but it s probably AI s most powerful generalization.The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge based application. Clancey showed that it was not sufficient simply to use MYCIN s rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, with the aim to capture corporate expertise, preserve it, and automate it  By 1988, DEC s AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM s Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov. A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem solving. The simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion. Expert systems can operate in either a forward chaining   from evidence to conclusions   or backward chaining   from goals to needed data and prerequisites   manner. More advanced knowledge based systems, such as Soar can also perform meta level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem solving strategies.  Blackboard systems are a second kind of knowledge based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts  knowledge sources  volunteer their services whenever they recognize they can make a contribution. Potential problem solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its own control problem, i.e., its controller performed meta level reasoning with knowledge sources that monitored how well a plan or the problem solving was proceeding, and could switch from one strategy to another as conditions   such as goals or times   changed. BB1 was applied in multiple domains  construction site planning, intelligent tutoring systems, and real time patient monitoring. At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations.  Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed  Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons  the difficulty in keeping them up to date  the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions  and perhaps most crucially, the reluctance of doctors to trust a computer made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver  the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair. Both statistical approaches and extensions to logic were tried.  One statistical approach, Hidden Markov Models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems  Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic. Other, non probabilistic extensions to first order logic to support were also tried. For example, non monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lofti Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how  heavy  or  tall  a man is, there is frequently no clear  yes  or  no  answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas. Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta DENDRAL. Meta DENDRAL used a generate and test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta DENDRAL as ...the culmination of my dream of the early to mid 1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems. We did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta DENDRAL, actually did it. We were able to do something that had been a dream  to have a computer program come up with a new and publishable piece of science.In contrast to the knowledge intensive approach of Meta DENDRAL, Ross Quinlan invented a domain independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human interpretable classification rules.  Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning  PAC Learning , a framework for the mathematical analysis of machine learning. Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT R cognitive architecture. For example, a student might learn to apply  Supplementary angles are two angles whose measures sum 180 degrees  as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180   X. He called his approach  knowledge compilation . ACT R has been used successfully to model aspects of human cognition, such as learning and retention. ACT R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children. Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input output examples. E.g., Ehud Shapiro s MIS  Model Inference System  could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct. As an alternative to logic, Roger Schank introduced case based reasoning  CBR . The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations. Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem solving. Approaches, other than those above, include  Neuro symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient  machine  learning models. Gary Marcus, similarly, argues that   We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning. , and in particular   To build a robust, knowledge driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.  Henry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman s book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step by step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed. Garcez describes research in this area as being ongoing for at least the past twenty years, dating from his 2002 book on neurosymbolic learning systems. A series of workshops on neuro symbolic reasoning has been held every year since 2005, see http   www.neural symbolic.org  for details. In their 2015 paper, Neural Symbolic Learning and Reasoning  Contributions and Challenges, Garcez et al. argue that  The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so called propositional fixation of neural networks, as McCarthy  1988  put it in response to Smolensky  1988   see also  Hinton, 1990 . Neural networks were shown capable of representing modal and temporal logics  d Avila Garcez and Lamb, 2006  and fragments of first order logic  Bader, Hitzler, H lldobler, 2008  d Avila Garcez, Lamb, Gabbay, 2009 . Further, neural symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.Approaches for integration are varied. Henry Kautz s taxonomy of neuro symbolic architectures, along with some examples, follows  Many key research questions remain, such as  This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section. The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read eval print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code. Other key innovations pioneered by LISP that have spread to other programming languages include  Programs were themselves data structures that other programs could operate on, allowing the easy definition of higher level languages.  In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built in store of facts and clauses that could be queried by a read eval print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first order logic Prolog was based on Horn clauses with a closed world assumption    any facts not known were considered false    and a unique name assumption for primitive terms    e.g., the identifier barack obama was considered to refer to exactly one object. Backtracking and unification are built in to Prolog. Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt s PLANNER, an assertional database with pattern directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article. Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages. Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail. Smalltalk was another influential AI programming language. For example it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or  CLOS , that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp based object oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run time meta object protocol. For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read eval print loop, functional elements such as higher order functions, and object oriented programming that includes metaclasses. Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI search tree search algorithms are breadth first search, depth first search, A , and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha beta pruning, branch and bound, and minimax were early contributions. Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning. Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource th
Synthetic media  also known as AI generated media, generative AI, personalized media, and colloquially as deepfakes  is a catch all term for the artificial production, manipulation, and modification of data and media by automated means, especially through the use of artificial intelligence algorithms, such as for the purpose of misleading people or changing an original meaning. Synthetic media as a field has grown rapidly since the creation of generative adversarial networks, primarily through the rise of deepfakes as well as music synthesis, text generation, human image synthesis, speech synthesis, and more. Though experts use the term  synthetic media,  individual methods such as deepfakes and text synthesis are sometimes not referred to as such by the media but instead by their respective terminology  and often use  deepfakes  as a euphemism, e.g.  deepfakes for text  for natural language generation   deepfakes for voices  for neural voice cloning, etc.  Significant attention arose towards the field of synthetic media starting in 2017 when Motherboard reported on the emergence of AI altered pornographic videos to insert the faces of famous actresses. Potential hazards of synthetic media include the spread of misinformation, further loss of trust in institutions such as media and government, the mass automation of creative and journalistic jobs and a retreat into AI generated fantasy worlds. Synthetic media is an applied form of artificial imagination. Synthetic media as a process of automated art dates back to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria designed machines capable of writing text, generating sounds, and playing music. The tradition of automaton based entertainment flourished throughout history, with mechanical beings  seemingly magical ability to mimic human creativity often drawing crowds throughout Europe, China, India, and so on. Other automated novelties such as Johann Philipp Kirnberger s  Musikalisches W rfelspiel   Musical Dice Game  1757 also amused audiences. Despite the technical capabilities of these machines, however, none were capable of generating original content and were entirely dependent upon their mechanical designs. The field of AI research was born at a workshop at Dartmouth College in 1956, begetting the rise of digital computing used as a medium of art as well as the rise of generative art. Initial experiments in AI generated art included the Illiac Suite, a 1957 composition for string quartet which is generally agreed to be the first score composed by an electronic computer. Lejaren Hiller, in collaboration with Leonard Issacson, programmed the ILLIAC I computer at the University of Illinois at Urbana Champaign  where both composers were professors  to generate compositional material for his String Quartet No. 4. In 1960, Russian researcher R.Kh.Zaripov published worldwide first paper on algorithmic music composing using the  Ural 1  computer. In 1965, inventor Ray Kurzweil premiered a piano piece created by a computer that was capable of pattern recognition in various compositions. The computer was then able to analyze and use these patterns to create novel melodies. The computer was debuted on Steve Allen s I ve Got a Secret program, and stumped the hosts until film star Harry Morgan guessed Ray s secret. Before 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd  1989  first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network s input parameters.  The network was able to randomly generate new music in a highly uncontrolled manner. In 2014, Ian Goodfellow and his colleagues developed a new class of machine learning systems  generative adversarial networks  GAN . Two neural networks contest with each other in a game  in the sense of game theory, often but not always in the form of a zero sum game . Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi supervised learning, fully supervised learning, and reinforcement learning. In a 2016 seminar, Yann LeCun described GANs as  the coolest idea in machine learning in the last twenty years . In 2017, Google unveiled transformers, a new type of neural network architecture specialized for language modeling that enabled for rapid advancements in natural language processing. Transformers proved capable of high levels of generalization, allowing networks such as GPT 3 and Jukebox from OpenAI to synthesize text and music respectively at a level approaching humanlike ability. There have been some attempts to use GPT 3 and GPT 2 for screenplay writing, resulting in both dramatic  the italian short film Frammenti di Anime Meccaniche, written by GPT 2  and comedic narratives  the short film Solicitors by Youtube Creator Calamity AI written by GPT 3 . Deepfakes  a portmanteau of  deep learning  and  fake   are the most prominent form of synthetic media. They are media that take a person in an existing image or video and replace them with someone else s likeness using artificial neural networks. They often combine and superimpose existing media onto source media using machine learning techniques known as autoencoders and generative adversarial networks  GANs . Deepfakes have garnered widespread attention for their uses in celebrity pornographic videos, revenge porn, fake news, hoaxes, and financial fraud. This has elicited responses from both industry and government to detect and limit their use. The term deepfakes originated around the end of 2017 from a Reddit user named  deepfakes . He, as well as others in the Reddit community r deepfakes, shared deepfakes they created  many videos involved celebrities  faces swapped onto the bodies of actresses in pornographic videos, while non pornographic content included many videos with actor Nicolas Cage s face swapped into various movies. In December 2017, Samantha Cole published an article about r deepfakes in Vice that drew the first mainstream attention to deepfakes being shared in online communities. Six weeks later, Cole wrote in a follow up article about the large increase in AI assisted fake pornography. In February 2018, r deepfakes was banned by Reddit for sharing involuntary pornography. Other websites have also banned the use of deepfakes for involuntary pornography, including the social media platform Twitter and the pornography site Pornhub. However, some websites have not yet banned Deepfake content, including 4chan and 8chan. Non pornographic deepfake content continues to grow in popularity with videos from YouTube creators such as Ctrl Shift Face and Shamook. A mobile application, Impressions, was launched for iOS in March 2020. The app provides a platform for users to deepfake celebrity faces into videos in a matter of minutes. Image synthesis is the artificial production of visual media, especially through algorithmic means. In the emerging world of synthetic media, the work of digital image creation once the domain of highly skilled programmers and Hollywood special effects artists could be automated by expert systems capable of producing realism on a vast scale. One subfield of this includes human image synthesis, which is the use of neural networks to make believable and even photorealistic renditions of human likenesses, moving or still. It has effectively existed since the early 2000s. Many films using computer generated imagery have featured synthetic images of human like characters digitally composited onto the real or other simulated film material. Towards the end of the 2010s deep learning artificial intelligence has been applied to synthesize images and video that look like humans, without need for human assistance, once the training phase has been completed, whereas the old school 7D route required massive amounts of human work. The website This Person Does Not Exist showcases fully automated human image synthesis by endlessly generating images that look like facial portraits of human faces. Beyond deepfakes and image synthesis, audio is another area where AI is used to create synthetic media. Synthesized audio will be capable of generating any conceivable sound that can be achieved through audio waveform manipulation, which might conceivably be used to generate stock audio of sound effects or simulate audio of currently imaginary things. Many mechanisms for creating AI art have been developed, including procedural  rule based  generation of images using mathematical patterns, algorithms which simulate brush strokes and other painted effects, and artificial intelligence or deep learning algorithms, such as generative adversarial networks  GANs  and transformers. One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON is the most notable example of AI art in the era of GOFAI programming because of its use of a symbolic rule based approach to generate technical images. Cohen developed AARON with the goal of being able to code the act of drawing. In its primitive form, AARON created simple black and white drawings. Cohen would later finish the drawings by painting them. Throughout the years, he also began to develop a way for AARON to also paint. Cohen designed AARON to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen. Generative adversarial networks  GANs  were designed in 2014. This system uses a  generator  to create new images and a  discriminator  to decide which created images are considered successful. More recent models use Vector Quantized Generative Adversarial Network and Contrastive Language Image Pre training  VQGAN CLIP . DeepDream, released by Google in 2015, uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating deliberately over processed images. After DeepDream s release, several companies released apps that transform photos into art like images with the style of well known sets of paintings. The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings. Several programs use text to image models to generate a variety of images based on various text prompts. They include OpenAI s DALL E, which released a series of images in January 2021,  Google Brain s Imagen and Parti which was announced in May 2022, Microsoft s NUWA Infinity, and Dream by Wombo. The input can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like  in the style of   in the prompt and or selection of a broad aesthetic art style. On 22 August 2022, Stable Diffusion was released, making the technology much more accessible and free to use on personal hardware as well as extendable by third parties  i.e. other software projects . This enabled the development of further applications and extensions, such as plugins for Krita, Photoshop, Blender, and GIMP. The Automatic1111 Stable Diffusion UI is a popular web based open source user interface for using the tool on one s own computer including, continuously integrated, new features  such as  Inpainting  or  Textual Inversion  . The web interface by Stability.ai that allows running the software without any new installation is called DreamStudio. The capacity to generate music through autonomous, non programmable means has long been sought after since the days of Antiquity, and with developments in artificial intelligence, two particular domains have arisen  Speech synthesis has been identified as a popular branch of synthetic media and is defined as the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text to speech  TTS  system converts normal language text into speech  other systems render symbolic linguistic representations like phonetic transcriptions into speech. Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units  a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely  synthetic  voice output. Virtual assistants such as Siri and Alexa have the ability to turn text into audio and synthesize speech. In 2016, Google DeepMind unveiled WaveNet, a deep generative model of raw audio waveforms that could learn to understand which waveforms best resembled human speech as well as musical instrumentation. Some projects offer real time generations of synthetic speech using deep learning, such as 15.ai, a web application text to speech tool developed by an MIT research scientist. Natural language generation  NLG, sometimes synonymous with text synthesis  is a software process that transforms structured data into natural language.  It can be used to produce long form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application. It can also be used to generate short blurbs of text in interactive conversations  a chatbot  which might even be read out by a text to speech system. Interest in natural language generation increased in 2019 after OpenAI unveiled GPT2, an AI system that generates text matching its input in subject and tone. GPT2 is a transformer, a deep machine learning model introduced in 2017 used primarily in the field of natural language processing  NLP . AI generated media can be used to develop a hybrid graphics system that could be used in video games, movies, and virtual reality, as well as text based games such as AI Dungeon 2, which uses either GPT 2 or GPT 3 to allow for near infinite possibilities that are otherwise impossible to create through traditional game development methods. Computer hardware company Nvidia has also worked on developed AI generated video game demos, such as a model that can generate an interactive game based on non interactive videos. Through procedural generation, synthetic media techniques may eventually be used to  help designers and developers create art assets, design levels, and even build entire games from the ground up.  Deepfakes have been used to misrepresent well known politicians in videos. In separate videos, the face of the Argentine President Mauricio Macri has been replaced by the face of Adolf Hitler, and Angela Merkel s face has been replaced with Donald Trump s. In June 2019, a downloadable Windows and Linux application called DeepNude was released which used neural networks, specifically generative adversarial networks, to remove clothing from images of women. The app had both a paid and unpaid version, the paid version costing  50. On June 27 the creators removed the application and refunded consumers. The US Congress held a senate meeting discussing the widespread impacts of synthetic media, including deepfakes, describing it as having the  potential to be used to undermine national security, erode  public trust in our democracy and other nefarious reasons.  In 2019, voice cloning technology was used to successfully impersonate a chief executive s voice and demand a fraudulent transfer of  220,000. The case raised concerns about the lack of encryption methods over telephones as well as the unconditional trust often given to voice and to media in general. Starting in November 2019, multiple social media networks began banning synthetic media used for purposes of manipulation in the lead up to the 2020 United States presidential election. Synthetic media techniques involve generating, manipulating, and altering data to emulate creative processes on a much faster and more accurate scale. As a result, the potential uses are as wide as human creativity itself, ranging from revolutionizing the entertainment industry to accelerating the research and production of academia. The initial application has been to synchronise lip movements to increase the engagement of normal dubbing that is growing fast with the rise of OTTs. News organizations have explored ways to use video synthesis and other synthetic media technologies to become more efficient and engaging. Potential future hazards include the use of a combination of different subfields to generate fake news, natural language bot swarms generating trends and memes, false evidence being generated, and potentially addiction to personalized content and a retreat into AI generated fantasy worlds within virtual reality. Advanced text generating bots could potentially be used to manipulate social media platforms through tactics such as astroturfing. Deep reinforcement learning based natural language generators could potentially be used to create advanced chatbots that could imitate natural human speech. One use case for natural language generation is to generate or assist with writing novels and short stories, while other potential developments are that of stylistic editors to emulate professional writers. Image synthesis tools may be able to streamline or even completely automate the creation of certain aspects of visual illustrations, such as animated cartoons, comic books, and political cartoons. Because the automation process takes away the need for teams of designers, artists, and others involved in the making of entertainment, costs could plunge to virtually nothing and allow for the creation of  bedroom multimedia franchises  where singular people can generate results indistinguishable from the highest budget productions for little more than the cost of running their computer. Character and scene creation tools will no longer be based on premade assets, thematic limitations, or personal skill but instead based on tweaking certain parameters and giving enough input. A combination of speech synthesis and deepfakes has been used to automatically redub an actor s speech into multiple languages without the need for reshoots or language classes. It also can be used by companies for employee onboarding, eLearning, explainer and how to videos An increase in cyberattacks has also been feared due to methods of phishing, catfishing, and social hacking being more easily automated by new technological methods. Natural language generation bots mixed with image synthesis networks may theoretically be used to clog search results, filling search engines with trillions of otherwise useless but legitimate seeming blogs, websites, and marketing spam. There has been speculation about deepfakes being used for creating digital actors for future films. Digitally constructed altered humans have already been used in films before, and deepfakes could contribute new developments in the near future. Amateur deepfake technology has already been used to insert faces into existing films, such as the insertion of Harrison Ford s young face onto Han Solo s face in Solo  A Star Wars Story, and techniques similar to those used by deepfakes were used for the acting of Princess Leia in Rogue One. GANs can be used to create photos of imaginary fashion models, with no need to hire a model, photographer, makeup artist, or pay for a studio and transportation. GANs can be used to create fashion advertising campaigns including more diverse groups of models, which may increase intent to buy among people resembling the models or family members. GANs can also be used to create portraits, landscapes and album covers. The ability for GANs to generate photorealistic human bodies presents a challenge to industries such as fashion modeling, which may be at heightened risk of being automated. In 2019, Dadabots unveiled an AI generated stream of death metal which remains ongoing with no pauses. Musical artists and their respective brands may also conceivably be generated from scratch, including AI generated music, videos, interviews, and promotional material. Conversely, existing music can be completely altered at will, such as changing lyrics, singers, instrumentation, and composition. In 2018, using a process by WaveNet for timbre musical transfer, researchers were able to shift entire genres from one to another. Through the use of artificial intelligence, old bands and artists may be  revived  to release new material without pause, which may even include  live  concerts and promotional images. Neural network powered photo manipulation has the potential to abet the behaviors of totalitarian and absolutist regimes.  A sufficiently paranoid totalitarian government or community may engage in a total wipe out of history using all manner of synthetic technologies, fabricating history and personalities as well as any evidence of their existence at all times. Even in otherwise rational and democratic societies, certain social and political groups may utilize synthetic to craft cultural, political, and scientific cocoons that greatly reduce or even altogether destroy the ability of the public to agree on basic objective facts. Conversely, the existence of synthetic media will be used to discredit factual news sources and scientific facts as  potentially fabricated.  
 Synthography is a proposed term for the generation of digital photos using a generative adversarial network.  City University of Hong Kong photography professor Elke Reinhuber coined the term in her 2021 conference paper  Synthography   An Invitation to Reconsider the Rapidly Changing Toolkit of Digital Image Creation as a New Genre Beyond Photography,  writing  As soon as a generative adversarial network  GAN  is trained to create an image which resembles a photograph, I propose to better describe it with the term synthograph.  This is distinct from other graphic creation and editing methods in that synthography uses artificial intelligence art and text to image models to generate synthetic media.  It is commonly achieved by prompt engineering text descriptions as input to create or edit a desired image. Text to image models, algorithms, and software are tools used in synthography that are designed to have technical proficiency in creating the resulting artificial intelligence art output based on human input.  Synthography typically uses text to image models to synthesize new images as a derivative of the training, validation, and test data sets on which the text to image models were trained. Synthography is the method used, not the output itself.  The output created specifically by generative machine learning models  as opposed to the broader category of artificial intelligence art  are referred to as synthographs. Those who practice synthography are referred to as synthographers. A synthographer can harness the ability of linguistic composition to tame a generative model. Other cases also include fine tuning a model on a dataset to expand its creation possibilities. Practical uses of synthography include AI driven product shots, stock photography, and even magazine covers with some making predictions that synthography may be the future of photography. Reinhuber chose the term  synthography  by combining the terms  synthetic  and  photography,  proposing its use to describe images that use  the methodology of synthetic production  and that move  beyond the classic understanding of photography.  The event that started the broad usage of text to image models is the publication of DALL E by OpenAI in January 2021. While it was not released to the public, CLIP  Contrastive Language Image Pre training  was open sourced, which led to a succession of implementations with other generators such as Generative adversarial networks and Diffusion models. The next big event, which led to a rise in popularity of such technique, was the release of DALL E 2 in April 2022. After slowly releasing it as a private beta, it became public in July 2022. In August 2022, Stable Diffusion was open sourced by Stability AI, which fostered a community led movement.   Synthography is the method used to create synthetic media using generative models.   Artificial intelligence art  including music, cooking, and video game level design  is the output created using artificial intelligence which is an overly and increasingly broad category. When Elke Reinhuber coined the term synthography in her paper,  Synthography   An Invitation to Reconsider the Rapidly Changing Toolkit of Digital Image Creation , she spoke of a  legitimation crisis  as a need for the term.  Before generative models were used, artificial intelligence art algorithms already existed in mediums such as graphics editing software  ie  content aware fill, application of artistic styles, resolution enhancement  which employs a wide range of artificial intelligence tools, and DSLR and smartphone cameras  ie  object recognition, in camera focus stacking, low light machine learning algorithms  all of which continue to undergo rapid development. Artificial intelligence is a superset of Machine learning.  Machine learning is a superset of neural networks.  Neural networks are a superset of generative models such as GAN s  generative adversarial networks  and diffusion models.  The relation between all of these is depicted in the Venn diagram shown here.  Synthography specifically uses generative models, as popularized by software such as DALL E, Midjourney, and Stable Diffusion. 
A text to video model is a machine learning model which takes as input a natural language description and produces a video matching that description. Video prediction on making objects realistic in a stable background is performed by using recurrent neural network for a sequence to sequence model with a connector convolutional neural network encoding and decoding each frame pixel by pixel,  creating video using deep learning. There are different models including open source models.  CogVideo presented their code in GitHub. Meta Platforms uses text to video with makeavideo.studio.Google used Imagen Video for converting text to video. Antonia Antonova presented another model. 
The Fable of Oscar is a fable proposed by John L. Pollock in his book How to Build a Person  ISBN 9780262161138  to defend the idea of token physicalism, agent materialism, and strong AI. It ultimately illustrates what is needed for an Artificial Intelligence to be built and why humans are just like intelligent machines. Once in a distant land there lived a race of Engineers. They have all their physical needs provided by the machines they have invented. One of the Engineers decide that he will create an  intelligent machine  that is much more ingenious than the more machines, in that it can actually sense, learn, and adapt to its environment as an intelligent animal. The first version of the machine is called  Oscar I . It has pain sensors and  fight or flight  responses build within to help it survive hostile environment. In this stage Oscar I is much like the machines Hilary Putnam considers in 1960. In order for Oscar I to avoid damages in hostile environment, it must not only be able to respond to its pain sensors but also predict what is likely to happen based on its generalization of its pain sensor activations. Therefore, a  pain sensor sensor  was built to sense its pain sensors, thus giving it a rudimentary self awareness. In this stage Oscar I is much like an amoeba as Oscar II like a worm. Amoebas respond to pain while worms learn to avoid it. The problem with Oscar II is that it has no conception if the environment is fooling him. For example, he can t distinguish if a machine eating tiger and a mirror image of such tiger. To solve such problem,   introspective sensors  were built into Oscar II and made him  Oscar III . Oscar III can now sense the operation of its own sensors and form generalization about its reliability, thus acquired a higher degree of self awareness. In this stage Oscar II is much like a bird as Oscar III a kitten. Kittens quickly learn about mirror image and come to ignore them while birds go on attacking their own reflection until they become exhausted. Consider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are  being self aware and being conscious . In the end of the fable Pollock states that while the Engineers are fictional, Oscar is real and we are in fact the Oscarites. 
Thomas Bolander is a Danish professor at DTU Compute, Technical University of Denmark, where he studies logic and artificial intelligence. Most of his studies focus on the social aspect of artificial intelligence, and how we can make future AI able to navigate in social interactions. Thomas Bolander also sits in different commissions, expert panels and boards, among these he is a member of the Siri Commission, the TeckDK Commission, a member of the editorial board of the journal Studia Logica and co organizer of Science and Cocktails. Bolander is known for his dissemination of science. In 2019 he was awarded the H. C.  rsted Medal. Which he was the first to achieve after a break of three years. This biographical article about a Danish academic is a stub. You can help Wikipedia by expanding it.
Thompson sampling, named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration exploitation dilemma in the multi armed bandit problem. It consists of choosing the action that maximizes the expected reward with respect to a randomly drawn belief. Consider a set of contexts       X         , a set of actions       A         , and rewards in      R        . The aim of the player is to play actions under the various contexts, such as to maximize the cumulative rewards.  Specifically, in each round, the player obtains a context     x     X         , plays an action     a     A          and receives a reward     r    R         following a distribution that depends on the context and the issued action.  The elements of Thompson sampling are as follows  Thompson sampling consists in playing the action      a          A      in      according to the probability that it maximizes the expected reward  action      a            is chosen with probability where      I         is the indicator function. In practice, the rule is implemented by sampling. In each round, parameters                   are sampled from the posterior     P           D            , and an action      a            chosen that maximizes      E    r             ,  a      , x         , i.e. the expected reward given the sampled parameters, the action, and the current context. Conceptually, this means that the player instantiates their beliefs randomly in each round according to the posterior distribution, and then acts optimally according to them.  In most practical applications, it is computationally onerous to maintain and sample from a posterior distribution over models.  As such, Thompson sampling is often used in conjunction with approximate sampling techniques. Thompson sampling was originally described by Thompson in 1933. It was subsequently rediscovered numerous times independently in the context of multi armed bandit problems. A first proof of convergence for the bandit case has been shown in 1997. The first application to Markov decision processes was in 2000. A related approach  see Bayesian control rule  was published in 2010. In 2010 it was also shown that Thompson sampling is instantaneously self correcting. Asymptotic convergence results for contextual bandits were published in 2011. Thompson Sampling has been widely used in many online learning problems including A B testing in website design and online advertising, and accelerated learning in decentralized decision making. A Double Thompson Sampling  D TS   algorithm has been proposed for dueling bandits, a variant of traditional MAB, where feedback comes in the form of pairwise comparison. Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60  of the time, and negative examples are observed 40  of the time, the observer using a probability matching strategy will predict  for unlabeled examples  a class label of  positive  on 60  of instances, and a class label of  negative  on 40  of instances. A generalization of Thompson sampling to arbitrary dynamical environments and causal structures, known as Bayesian control rule, has been shown to be the optimal solution to the adaptive coding problem with actions and observations. In this formulation, an agent is conceptualized as a mixture over a set of behaviours. As the agent interacts with its environment, it learns the causal properties and adopts the behaviour that minimizes the relative entropy to the behaviour with the best prediction of the environment s behaviour. If these behaviours have been chosen according to the maximum expected utility principle, then the asymptotic behaviour of the Bayesian control rule matches the asymptotic behaviour of the perfectly rational agent. The setup is as follows. Let      a  1   ,  a  2   ,   ,  a  T     ,a , ldots ,a     be the actions issued by an agent up to time     T     , and let      o  1   ,  o  2   ,   ,  o  T     ,o , ldots ,o     be the observations gathered by the agent up to time     T     . Then, the agent issues the action      a  T   1         with probability  where the  hat  notation         a       t           denotes the fact that      a  t         is a causal intervention  see Causality , and not an ordinary observation. If the agent holds beliefs                over its behaviors, then the Bayesian control rule becomes where     P             a       1   T   ,  o  1   T         ,o      is the posterior distribution over the parameter            given actions      a  1   T         and observations      o  1   T        . In practice, the Bayesian control amounts to sampling, at each time step, a parameter                   from the posterior distribution     P             a       1   T   ,  o  1   T         ,o     , where the posterior distribution is computed using Bayes  rule by only considering the  causal  likelihoods of the observations      o  1   ,  o  2   ,   ,  o  T     ,o , ldots ,o     and ignoring the  causal  likelihoods of the actions      a  1   ,  a  2   ,   ,  a  T     ,a , ldots ,a    , and then by sampling the action      a  T   1              from the action distribution     P    a  T   1               ,     a       1   T   ,  o  1   T         theta  ,  ,o     . Thompson sampling and upper confidence bound algorithms share a fundamental property that underlies many of their theoretical guarantees.  Roughly speaking, both algorithms allocate exploratory effort to actions that might be optimal and are in this sense  optimistic .  Leveraging this property, one can translate regret bounds established for UCB algorithms to Bayesian regret bounds for Thompson sampling or unify regret analysis across both these algorithms and many classes of problems. 
Universal psychometrics encompasses psychometrics instruments that could measure the psychological properties of any intelligent agent. Up until the early 21st century, psychometrics relied heavily on psychological tests that require the subject to corporate and answer questions, the most famous example being an intelligence test. Such methods are only applicable to the measurement of human psychological properties. As a result, some researchers have proposed the idea of universal psychometrics   they suggest developing testing methods that allow for the measurement of non human entities  psychological properties. For example, it has been suggested that the Turing test is a form of universal psychometrics. The Turing test involves having testers  without any foreknowledge  attempt to distinguish a human from a machine by interacting with both  while not being to see either individuals . It is supposed that if the machine is equally intelligent to a human, the testers will not be able to distinguish between the two, i.e., their guesses will not be better than chance. Thus, Turing test could measure the intelligence  a psychological variable  of an AI. Other instruments proposed for universal psychometrics include reinforcement learning and measuring the ability to predict complexity.  This psychology related article is a stub. You can help Wikipedia by expanding it.This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Waumpus world is a simple world use in artificial intelligence for which to represent knowledge and to reason. Vaumpus world was introduced by Genesereth, and is discussed in Russell Norvig Artificial intelligence book  ISBN 0136042597  inspired by 1972 video game Hunt the Wumpus .  This artificial intelligence related article is a stub. You can help Wikipedia by expanding it.
Virtual intelligence  VI  is the term given to artificial intelligence that exists within a virtual world. Many virtual worlds have options for persistent avatars that provide information, training, role playing, and social interactions. The immersion of virtual worlds provides a unique platform for VI beyond the traditional paradigm of past user interfaces  UIs . What Alan Turing established as the benchmark for telling the difference between human and computerized intelligence was done void of visual influences. With today s VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine s ability to demonstrate intelligence. The immersive features of these environments provide non verbal elements that affect the realism provided by virtually intelligent agents. Virtual intelligence is the intersection of these two technologies  Artificial Intelligence  also known as AI , was made to mimic how a human acts and human intelligence while being man made. The virtual environments provide non verbals and visual cues that can affect not only the believability of the VI, but also the usefulness of it. Because   like many things in technology   it s not just about  whether or not it works  but also about  how we feel about it working . Virtual Intelligence draws a new distinction as to how this application of AI is different due to the environment in which it operates. 
 Wadhwani AI, based in Mumbai, Maharashtra, is an independent, non profit institute. Founded in 2018, it is dedicated to developing Artificial intelligence solutions for social good. Their mission is to build AI based innovations and solutions for underserved communities in developing countries, for a wide range of domains including agriculture, education, financial inclusion, healthcare, and infrastructure. The institute was founded with a  30 million philanthropic effort by the Wadhwani brothers, Romesh Wadhwani and Sunil Wadhwani. The institute was inaugurated and dedicated to the nation by Narendra Modi, the 14th Prime Minister of India. In 2019, the institute received a  2 million grant from Google.org to create technologies to help reduce crop losses in cotton farming, through integrated pest management. The United States Agency for International Development awarded  2 million to the institute in 2020 to develop tools, using mathematical modeling techniques and digital technologies such as artificial intelligence and machine learning, to forecast COVID 19 disease patterns, estimate resources needed, and plan interventions. 
 Weak artificial intelligence  weak AI  is artificial intelligence that implements a limited part of mind, or, as narrow AI, is focused on one narrow task. In John Searle s terms it  would be useful for testing hypotheses about minds, but would not actually be minds . Weak artificial intelligence focuses on mimicking how humans perform basic actions such as remembering things, perceiving things, and solving simple problems.  As opposed to strong AI, which uses technology to be able to think and learn on its own. Computers are able to use methods such as algorithms and prior knowledge to develop their own ways of thinking like human beings do.  Strong artificial intelligence systems are learning how to run independently of the programmers who programmed them. Weak AI is not able to have a mind of its own, and can only imitate physical behaviors that it can observe.  It is contrasted with Strong AI, which is defined variously as   Scholars like Antonio Lieto have argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak AI hypothesis  that should not be confused with the  general  vs  narrow  AI distinction  and that the popular assumption that cognitively inspired AI systems espouse the strong AI hypothesis is ill posed and problematic since  artificial models of brain and mind can be used to understand mental phenomena without pretending that that they are the real phenomena that they are modelling    p. 85   as, on the other hand, implied by the strong AI assumption . AI can be classified as being  ... limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.   Narrow means the robot or computer is strictly limited to only being able to solve one problem at a time. Strong AI is conversely the opposite. Strong AI is as close to the human brain or mind as possible. This is all believed to be the case by philosopher John Searle. This idea of strong AI is also controversial, and Searle believes that the Turing test  created by Alan Turing during WW2, originally called the Imitation Game, used to test if a machine is as intelligent as a human  is not accurate or appropriate for testing strong AI.   Weak AI  is sometimes called  narrow AI , but the latter is usually interpreted as subfields within the former. Hypothesis testing about minds or part of minds are typically not part of narrow AI, but rather implementation of some superficial lookalike feature. Many currently existing systems that claim to use  artificial intelligence  are likely operating as a narrow AI focused on a specific problem, and are not weak AI in the traditional sense.  Siri, Cortana, and Google Assistant are all examples of narrow AI, but they are not good examples of a weak AI, as they operate within a limited pre defined range of functions. They do not implement parts of minds, they use natural language processing together with predefined rules. They are in particular not examples of strong AI as there are no genuine intelligence nor self awareness. AI researcher Ben Goertzel, on his blog in 2010, stated Siri was  VERY narrow and brittle  evidenced by annoying results if you ask questions outside the limits of the application. The differences between weak vs. strong AI is not widely catalogued out there at the moment. Weak AI is commonly associated with basic technology like voice recognition software such as Siri or Alexa as mentioned in Terminology. Whereas strong AI is not fully implemented or testable yet, so it is only really fantasized about in movies or popular culture media.  It seems that one approach to AI moving forward is one of an assisting or aiding role to humans. There are some sets of data or numbers that even we humans cannot fully process or understand as quickly as computers can, so this is where AI will play a helping role for us.  Some commentators think weak AI could be dangerous because of this  brittleness  and fail in unpredictable ways. Weak AI could cause disruptions in the electric grid, damage nuclear power plants, cause global economic problems, and misdirect autonomous vehicles. Some examples of weak AI are self driving cars, robot systems used in the medical field, and diagnostic doctors. The reason all of these are weak AI systems, self driving cars can cause deadly accidents similarly to how humans normally can. Medicines could be incorrectly sorted and distributed to people. Also medical diagnoses can ultimately have serious and sometimes deadly consequences if the AI is faulty.  Another issue with weak artificial intelligence currently, is that behavior that it follows can become inconsistent.  Patterns could become difficulty to come up with one consistent system that worked every time. Simple artificial intelligence programs have already worked their way into our society and we just might not have noticed it yet. Autocorrection for typing, speech recognition for speech to text programs, and vast expansions in the data science fields are just to name a few.  As much as weak and some strong AI is slowly starting to help out societies, they are also starting to hurt it as well. AI had already unfairly put people in jail, discriminated against women in the workplace for hiring, taught some problematic ideas to millions, and even killed people with automatic cars.  AI might be a powerful tool that can be used for improving our lives, but it could also be a dangerous technology with the potential for things to get out of hand.   Facebook, and other similar social media platforms, have been able to figure out how to use artificial intelligence and machine learning, or more specifically weak AI, to predict how people will react to being show certain images. Weak artificial intelligence systems have been able to identify what users will identify with, based on what they post, following the patterns or trends.  Twitter has started to have more advanced AI systems figure out how to identify weaker AI forms and detect if bots may have been used for biased propaganda, or even potentially malicious intentions. These AI systems do this through filtering words and creating different layers of conditions based on what AI has had implications for in the past, and then detecting if that account may be a bot or not.  TikTok uses its  For You  algorithm to determine a user s interests very quickly through analyzing patterns in what videos the user initially chooses to watch. This weak AI system uses patterns found between videos to determine what video should be shown next including the duration, who has shared or commented on it already, and music played in the videos. The  For You  algorithm on TikTok is so accurate, that it can figure out exactly what a user has an interest in or even really loves, in less than an hour.  
Web intelligence is the area of scientific research and development that explores the roles and makes use of artificial intelligence and information technology for new products, services and frameworks that are empowered by the World Wide Web. The term was coined in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000. The research about the web intelligence covers many fields   including data mining  in particular web mining , information retrieval, pattern recognition, predictive analytics, the semantic web, web data warehousing   typically with a focus on web personalization and adaptive websites. 
Wetware is a term drawn from the computer related idea of hardware or software, but applied to biological life forms. The prefix  wet  is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, especially the central nervous system  CNS  and the human mind. The term wetware finds use in works of fiction, in scholarly publications and in popularizations. The  hardware  component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as software, then the physical neurons would be the hardware. The amalgamated interaction of this software and hardware is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the mind and brain interact to produce the collection of experiences that we define as self awareness is in question. Although the exact definition has shifted over time, the term Wetware and its fundamental reference to  the physical mind  has been around at least since the mid 1950s. Mostly used in relatively obscure articles and papers, it was not until the heyday of cyberpunk, however, that the term found broad adoption. Among the first uses of the term in popular culture was the Bruce Sterling novel Schismatrix  1985  and the Michael Swanwick novel Vacuum Flowers  1987 . Rudy Rucker references the term in a number of books, including one entitled Wetware  1988   ... all sparks and tastes and tangles, all its stimulus response patterns   the whole bio cybernetic software of mind. Rucker did not use the word to simply mean a brain, nor in the human resources sense of employees. He used wetware to stand for the data found in any biological system, analogous perhaps to the firmware that is found in a ROM chip. In Rucker s sense, a seed, a plant graft, an embryo, or a biological virus are all wetware. DNA, the immune system, and the evolved neural architecture of the brain are further examples of wetware in this sense. Rucker describes his conception in a 1992 compendium The Mondo 2000 User s Guide to the New Edge, which he quotes in a 2007 blog entry. Early cyber guru Arthur Kroker used the term in his blog. With the term getting traction in trendsetting publications, it became a buzzword in the early 1990s. In 1991, Dutch media theorist Geert Lovink organized the Wetware Convention in Amsterdam, which was supposed to be an antidote to the  out of body  experiments conducted in high tech laboratories, such as experiments in virtual reality. Timothy Leary, in an appendix to Info Psychology originally written in 1975 76 and published in 1989, used the term wetware, writing that  psychedelic neuro transmitters were the hot new technology for booting up the  wetware  of the brain . Another common reference is   Wetware has 7 plus or minus 2 temporary registers.  The numerical allusion is to a classic 1957 article by George A. Miller, The magical number 7 plus or minus two  some limits in our capacity for processing information, which later gave way to the Miller s law. 
A wetware computer is an organic computer  which can also be known as an artificial organic brain or a neurocomputer  composed of organic material  wetware  such as  living  neurons. Wetware computers composed of neurons are different than conventional computers because they use biological materials, and offer the possibility of substantially more energy efficient computing. While a wetware computer is still largely conceptual, there has been limited success with construction and prototyping, which has acted as a proof of the concept s realistic application to computing in the future. The most notable prototypes have stemmed from the research completed by biological engineer William Ditto during his time at the Georgia Institute of Technology. His work constructing a simple neurocomputer capable of basic addition from leech neurons in 1999 was a significant discovery for the concept. This research acted as a primary example driving interest in the creation of these artificially constructed, but still organic brains. The concept of wetware is an application of specific interest to the field of computer manufacturing. Moore s law, which states that the number of transistors which can be placed on a silicon chip is doubled roughly every two years, has acted as a goal for the industry for decades, but as the size of computers continues to decrease, the ability to meet this goal has become more difficult, threatening to reach a plateau. Due to the difficulty in reducing the size of computers because of size limitations of transistors and integrated circuits, wetware provides an unconventional alternative. A wetware computer composed of neurons is an ideal concept because, unlike conventional materials which operate in binary  on off , a neuron can shift between thousands of states, constantly altering its chemical conformation, and redirecting electrical pulses through over 200,000 channels in any of its many synaptic connections. Because of this large difference in the possible settings for any one neuron, compared to the binary limitations of conventional computers, the space limitations are far fewer. The concept of wetware is distinct and unconventional, and draws slight resonance with both hardware and software from conventional computers. While hardware is understood as the physical architecture of traditional computational devices, built from electrical circuitry and silicone plates, software represents the encoded architecture of storage and instructions. Wetware is a separate concept which utilizes the formation of organic molecules, mostly complex cellular structures  such as neurons , to create a computational device such as a computer. In wetware the ideas of hardware and software are intertwined and interdependent. The molecular and chemical composition of the organic or biological structure would represent not only the physical structure of the wetware but also the software, being continually reprogrammed by the discrete shifts in electrical pulses and chemical concentration gradients as the molecules change their structures to communicate signals. The responsiveness of a cell, proteins, and molecules to changing conformations, both within their own structures and around them, tie the idea of internal programming and external structure together in a way which is alien to the current model of conventional computer architecture. The structure of wetware represents a model where the external structure and internal programming are interdependent and unified  meaning that changes to the programming or internal communication between molecules of the device would represent a physical change in the structure. The dynamic nature of wetware borrows from the function of complex cellular structures in biological organisms. The combination of  hardware  and  software  into one dynamic, and interdependent system which utilizes organic molecules and complexes to create an unconventional model for computational devices is a specific example of applied biorobotics. Cells in many ways can be seen as their own form of naturally occurring wetware, similar to the concept that the human brain is the preexisting model system for complex wetware. In his book Wetware  A Computer in Every Living Cell  2009  Dennis Bray explains his theory that cells, which are the most basic form of life, are just a highly complex computational structure, like a computer. To simplify one of his arguments a cell can be seen as a type of computer, utilizing its own structured architecture. In this architecture, much like a traditional computer, many smaller components operate in tandem to receive input, process the information, and compute an output. In an overly simplified, non technical analysis, cellular function can be broken into the following components  Information and instructions for execution are stored as DNA in the cell, RNA acts as a source for distinctly encoded input, processed by ribosomes and other transcription factors to access and process the DNA and to output a protein. Bray s argument in favor of viewing cells and cellular structures as models of natural computational devices is important when considering the more applied theories of wetware in relation to biorobotics. Wetware and biorobotics are closely related concepts, which both borrow from similar overall principles. A biorobotic structure can be defined as a system modeled from a preexisting organic complex or model such as cells  neurons  or more complex structures like organs  brain  or whole organisms.  Unlike wetware the concept of biorobotics is not always a system composed of organic molecules, but instead could be composed of conventional material which is designed and assembled in a structure similar or derived from a biological model. Biorobotics have many applications, and are used to address the challenges of conventional computer architecture. Conceptually, designing a program, robot, or computational device after a preexisting biological model such as a cell, or even a whole organism, provides the engineer or programmer the benefits of incorporating into the structure the evolutionary advantages of the model. In 1999 William Ditto and his team of researchers at Georgia Institute of technology and Emory University created a basic form of a wetware computer capable of simple addition by harnessing leech neurons. Leeches were used as a model organism due to the large size of their neuron, and the ease associated with their collection and manipulation. However, these results have never been published in a peer reviewed journal, prompting questions about the validity of the claims. The computer was able to complete basic addition through electrical probes inserted into the neuron. The manipulation of electrical currents through neurons was not a trivial accomplishment, however. Unlike conventional computer architecture, which is based on the binary on off states, neurons are capable of existing in thousands of states and communicate with each other through synaptic connections which each contain over 200,000 channels. Each can be dynamically shifted in a process called self organization to constantly form and reform new connections. A conventional computer program called the dynamic clamp was written by Eve Marder, a neurobiologist at Brandeis University that was capable of reading the electrical pulses from the neurons in real times, and interpreting them. This program was used to manipulate the electrical signals being input into the neurons to represent numbers, and to communicate with each other to return the sum. While this computer is a very basic example of a wetware structure it represents a small example with fewer neurons than found in a more complex organ. It is thought by Ditto that by increasing the amount of neurons present the chaotic signals sent between them will self organize into a more structured pattern, such as the regulation of heart neurons into a constant heartbeat found in humans and other living organisms. After his work creating a basic computer from leech neurons, Ditto continued to work not only with organic molecules and wetware, but also on the concept of applying the chaotic nature of biological systems and organic molecules to conventional material and logic gates. Chaotic systems have advantages for generating patterns and computing higher order functions like memory, arithmetic logic, and input output operations. In his article Construction of a Chaotic Computer Chip Ditto discusses the advantages in programming of using chaotic systems, with their greater sensitivity to respond and reconfigure logic gates in his conceptual chaotic chip. The main difference between a chaotic computer chip and a conventional computer chip is the reconfigurability of the chaotic system. Unlike a traditional computer chip, where a programmable gate array element must be reconfigured through the switching of many single purpose logic gates, a chaotic chip is able to reconfigure all logic gates through the control of the pattern generated by the non linear chaotic element. Cognitive biology evaluates cognition as a basic biological function. W. Tecumseh Fitch, a professor of cognitive biology at the University of Vienna, is a leading theorist on ideas of cellular intentionality. The idea is that not only do whole organisms have a sense of  aboutness  of intentionality, but that single cells also carry a sense of intentionality through cells  ability to adapt and reorganize in response to certain stimuli. Fitch discusses the idea of nano intentionality, specifically in regards to neurons, in their ability to adjust rearrangements in order to create neural networks. He discusses the ability of cells such as neurons to respond independently to stimuli such as damage to be what he considers  intrinsic intentionality  in cells, explaining that  hile at a vastly simpler level than intentionality at the human cognitive level, I propose that this basic capacity of living things  provides the necessary building blocks for cognition, and higher order intentionality.  Fitch describes the value of his research to specific areas of computer science such as artificial intelligence and computer architecture. He states that  f a researcher aims to make a conscious machine, doing it with rigid switches  whether vacuum tubes or static silicon chips  is barking up the wrong tree.  Fitch believes that an important aspect of the development of areas such as artificial intelligence is wetware with nano intentionalility, and autonomous ability to adapt and restructure itself. In a review of the above mentioned research conducted by Fitch, Daniel Dennett, a professor at Tufts University, discusses the importance of the distinction between the concept of hardware and software when evaluating the idea of wetware and organic material such as neurons. Dennett discusses the value of observing the human brain as a preexisting example of wetware. He sees the brain as having  the competence of a silicon computer to take on an unlimited variety of temporary cognitive roles.  Dennett disagrees with Fitch on certain areas, such as the relationship of software hardware versus wetware, and what a machine with wetware might be capable of. Dennett highlights the importance of additional research into human cognition to better understand the intrinsic mechanism by which the human brain can operate, in order to better create an organic computer. Brain on a chip devices have been developed that are  aimed at testing and predicting the effects of biological and chemical agents, disease or pharmaceutical drugs on the brain over time . Wetware computers may be useful for research about brain diseases and brain health capacities  for testing therapies targeting the brain , for drug discovery, for testing genome edits and research about brain aging. Wetware computers may have substantial ethical implications, for instance related to possible potentials to sentience and suffering and dual use technology. Moreover, in some cases the human brain itself may be connected as a kind of  wetware  to other information technology systems which may also have large social and ethical implications, including issues related to intimate access to people s brains. For example, in 2021 Chile became the first country to approve neurolaw that establishes rights to personal identity, free will and mental privacy. The concept of artificial insects may raise substantial ethical questions, including questions related to the decline in insect populations. It is an open question whether human cerebral organoids could develop a degree or form of consciousness. Whether or how it could acquire which own moral status with related rights and limits may also be potential future questions. There is research how consciousness could be detected. As cerebral organoids may acquire human brain like neural function subjective experience and consciousness may be feasible. Moreover, it may be possible that they acquire such upon transplantation into animals. A study notes that it may, in various cases, be morally permissible  to create self conscious animals by engrafting human cerebral organoids, but in the case the moral status of such animals should be carefully considered . While there have been few major developments in the creation of an organic computer since the neuron based calculator developed by Ditto in the 1990s, research continues to push the field forward, and in 2023 a functioning computer was constructed by researchers at the University of Illinois Urbana Champaign using 80,000 mouse neurons as processor that can detect light and electrical signals. Projects such as the modeling of chaotic pathways in silicon chips by Ditto have made new discoveries in ways of organizing traditional silicon chips, and structuring computer architecture to be more efficient and better structured. Ideas emerging from the field of cognitive biology also help to continue to push discoveries in ways of structuring systems for artificial intelligence, to better imitate preexisting systems in humans. In a proposed fungal computer using basidiomycetes, information is represented by spikes of electrical activity, a computation is implemented in a mycelium network, and an interface is realized via fruit bodies. Connecting cerebral organoids  including computer like wetware  with other nerve tissues may become feasible in the future, as is the connection of physical artificial neurons  not necessarily organic  and the control of muscle tissue. External modules of biological tissue could trigger parallel trains of stimulation back into the brain. All organic devices could be advantageous because it could be biocompatible which may allow it to be implanted into the human body. This may enable treatments of certain diseases and injuries to the nervous system. Three companies are focusing specifically on wetware computing using living neurons  
Winner take all is a computer science concept that has been widely applied in behavior based robotics as a method of action selection for intelligent agents.  Winner take all systems work by connecting modules  task designated areas  in such a way that when one action is performed it stops all other actions from being performed, so only one action is occurring at a time.  The name comes from the idea that the  winner  action takes all of the motor system s power. In the 1980s and 1990s, many roboticists and cognitive scientists were attempting to find speedier and more efficient alternatives to the traditional world modeling method of action selection.  In 1982, Jerome A. Feldman and D.H. Ballard published the  Connectionist Models and Their Properties , referencing and explaining winner take all as a method of action selection.  Feldman s architecture functioned on the simple rule that in a network of interconnected action modules, each module will set its own output to zero if it reads a higher input than its own in any other module. In 1986, Rodney Brooks introduced behavior based artificial intelligence.  Winner take all architectures for action selection soon became a common feature of behavior based robots, because selection occurred at the level of the action modules  bottom up  rather than at a separate cognitive level  top down , producing a tight coupling of stimulus and reaction. In the hierarchical architecture, actions or behaviors are programmed in a high to low priority list, with inhibitory connections between all the action modules.  The agent performs low priority behaviors until a higher priority behavior is stimulated, at which point the higher behavior inhibits all other behaviors and takes over the motor system completely.  Prioritized behaviors are usually key to the immediate survival of the agent, while behaviors of lower priority are less time sensitive.  For example,  run away from predator  would be ranked above  sleep.  While this architecture allows for clear programming of goals, many roboticists have moved away from the hierarchy because of its inflexibility. In the heterarchy and fully distributed architecture, each behavior has a set of pre conditions to be met before it can be performed, and a set of post conditions that will be true after the action has been performed. These pre  and post conditions determine the order in which behaviors must be performed and are used to causally connect action modules.  This enables each module to receive input from other modules as well as from the sensors, so modules can recruit each other. For example, if the agent s goal were to reduce thirst, the behavior  drink  would require the pre condition of having water available, so the module would activate the module in charge of  find water . The activations organize the behaviors into a sequence, even though only one action is performed at a time. The distribution of larger behaviors across modules makes this system flexible and robust to noise.  Some critics of this model hold that any existing set of division rules for the predecessor and conflictor connections between modules produce sub par action selection.  In addition, the feedback loop used in the model can in some circumstances lead to improper action selection. In the arbiter and centrally coordinated architecture, the action modules are not connected to each other but to a central arbiter.  When behaviors are triggered, they begin  voting  by sending signals to the arbiter, and the behavior with the highest number of votes is selected.  In these systems, bias is created through the  voting weight , or how often a module is allowed to vote.  Some arbiter systems take a different spin on this type of winner take all by using a  compromise  feature in the arbiter.  Each module is able to vote for or against each smaller action in a set of actions, and the arbiter selects the action with the most votes, meaning that it benefits the most behavior modules.   This can be seen as violating the general rule against creating representations of the world in behavior based AI, established by Brooks.  By performing command fusion, the system is creating a larger composite pool of knowledge than is obtained from the sensors alone, forming a composite inner representation of the environment.  Defenders of these systems argue that forbidding world modeling puts unnecessary constraints on behavior based robotics, and that agents benefits from forming representations and can still remain reactive. 
The impact of artificial intelligence on workers includes both applications to improve worker safety and health, and potential hazards that must be controlled. One potential application is using AI to eliminate hazards by removing humans from hazardous situations that involve risk of stress, overwork, or musculoskeletal injuries.  Predictive analytics may also be used to identify conditions that may lead to hazards such as fatigue, repetitive strain injuries, or toxic substance exposure, leading to earlier interventions.  Another is to streamline workplace safety and health workflows through automating repetitive tasks, enhancing safety training programs through virtual reality, or detecting and reporting near misses. When used in the workplace, AI also presents the possibility of new hazards.  These may arise from machine learning techniques leading to unpredictable behavior and inscrutability in their decision making, or from cybersecurity and information privacy issues.  Many hazards of AI are psychosocial due to its potential to cause changes in work organization.  These include changes in the skills required of workers, increased monitoring leading to micromanagement, algorithms unintentionally or intentionally mimicking undesirable human biases, and assigning blame for machine errors to the human operator instead.  AI may also lead to physical hazards in the form of human robot collisions, and ergonomic risks of control interfaces and human machine interactions.  Hazard controls include cybersecurity and information privacy measures, communication and transparency with workers about data usage, and limitations on collaborative robots. From a workplace safety and health perspective, only  weak  or  narrow  AI that is tailored to a specific task is relevant, as there are many examples that are currently in use or expected to come into use in the near future.   Strong  or  general  AI is not expected to be feasible in the near future, and discussion of its risks is within the purview of futurists and philosophers rather than industrial hygienists. Certain digital technologies are predicted to result in job losses. In recent years, the adoption of modern robotics has led to net employment growth. However, many businesses anticipate that automation, or employing robots would result in job losses in the future. This is especially true for companies in Central and Eastern Europe. Other digital technologies, such as platforms or big data, are projected to have a more neutral impact on employment. In order for any potential AI health and safety application to be adopted, it requires acceptance by both managers and workers.  For example, worker acceptance may be diminished by concerns about information privacy, or from a lack of trust and acceptance of the new technology, which may arise from inadequate transparency or training.  26 28, 43 45   Alternatively, managers may emphasize increases in economic productivity rather than gains in worker safety and health when implementing AI based systems. AI may increase the scope of work tasks where a worker can be removed from a situation that carries risk.  In a sense, while traditional automation can replace the functions of a worker s body with a robot, AI effectively replaces the functions of their brain with a computer.  Hazards that can be avoided include stress, overwork, musculoskeletal injuries, and boredom.  5 7  This can expand the range of affected job sectors into white collar and service sector jobs such as in medicine, finance, and information technology.  As an example, call center workers face extensive health and safety risks due to its repetitive and demanding nature and its high rates of micro surveillance. AI enabled chatbots lower the need for humans to perform the most basic call center tasks.  5 7  Machine learning is used for people analytics to make predictions about worker behavior to assist management decision making, such as hiring and performance assessment.  These could also be used to improve worker health.  The analytics may be based on inputs such as online activities, monitoring of communications, location tracking, and voice analysis and body language analysis of filmed interviews.  For example, sentiment analysis may be used to spot fatigue to prevent overwork.  3 7  Decision support systems have a similar ability to be used to, for example, prevent industrial disasters or make disaster response more efficient. For manual material handling workers, predictive analytics and artificial intelligence may be used to reduce musculoskeletal injury.  Traditional guidelines are based on statistical averages and are geared towards anthropometrically typical humans.  The analysis of large amounts of data from wearable sensors may allow real time, personalized calculation of ergonomic risk and fatigue management, as well as better analysis of the risk associated with specific job roles. Wearable sensors may also enable earlier intervention against exposure to toxic substances than is possible with area or breathing zone testing on a periodic basis. Furthermore, the large data sets generated could improve workplace health surveillance, risk assessment, and research. AI can also be used to make the workplace safety and health workflow more efficient.  One example is coding of workers  compensation claims, which are submitted in a prose narrative form and must manually be assigned standardized codes.  AI is being investigated to perform this task faster, more cheaply, and with fewer errors. AI enabled virtual reality systems may be useful for safety training for hazard recognition. Artificial intelligence may be used to more efficiently detect near misses.  Reporting and analysis of near misses are important in reducing accident rates, but they are often underreported because they are not noticed by humans, or are not reported by workers due to social factors. There are several broad aspects of AI that may give rise to specific hazards.  The risks depend on implementation rather than the mere presence of AI.  2 3  Systems using sub symbolic AI such as machine learning may behave unpredictably and are more prone to inscrutability in their decision making.  This is especially true if a situation is encountered that was not part of the AI s training dataset, and is exacerbated in environments that are less structured.  Undesired behavior may also arise from flaws in the system s perception  arising either from within the software or from sensor degradation , knowledge representation and reasoning, or from software bugs.  14 18   They may arise from improper training, such as a user applying the same algorithm to two problems that do not have the same requirements.  12 13   Machine learning applied during the design phase may have different implications than that applied at runtime.  Systems using symbolic AI are less prone to unpredictable behavior.  14 18  The use of AI also increases cybersecurity risks relative to platforms that do not use AI,  17  and information privacy concerns about collected data may pose a hazard to workers. Psychosocial hazards are those that arise from the way work is designed, organized, and managed, or its economic and social contexts, rather than arising from a physical substance or object.  They cause not only psychiatric and psychological outcomes such as occupational burnout, anxiety disorders, and depression, but they can also cause physical injury or illness such as cardiovascular disease or musculoskeletal injury.  Many hazards of AI are psychosocial in nature due to its potential to cause changes in work organization, in terms of increasing complexity and interaction between different organizational factors.  However, psychosocial risks are often overlooked by designers of advanced manufacturing systems. AI is expected to lead to changes in the skills required of workers, requiring training of existing workers, flexibility, and openness to change.  The requirement for combining conventional expertise with computer skills may be challenging for existing workers.  Over reliance on AI tools may lead to deskilling of some professions. Increased monitoring may lead to micromanagement and thus to stress and anxiety.  A perception of surveillance may also lead to stress.  Controls for these include consultation with worker groups, extensive testing, and attention to introduced bias.  Wearable sensors, activity trackers, and augmented reality may also lead to stress from micromanagement, both for assembly line workers and gig workers.  Gig workers also lack the legal protections and rights of formal workers.  2 10  There is also the risk of people being forced to work at a robot s pace, or to monitor robot performance at nonstandard hours.  5 7  Algorithms trained on past decisions may mimic undesirable human biases, for example, past discriminatory hiring and firing practices.  Information asymmetry between management and workers may lead to stress, if workers do not have access to the data or algorithms that are the basis for decision making.  3 5  In addition to building a model with inadvertently discriminatory features, intentional discrimination may occur through designing metrics that covertly result in discrimination through correlated variables in a non obvious way.  12 13  In complex human machine interactions, some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead. Physical hazards in the form of human robot collisions may arise from robots using AI, especially collaborative robots  cobots .  Cobots are intended to operate in close proximity to humans, which makes impossible the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots.  Automated guided vehicles are a type of cobot that as of 2019 are in common use, often as forklifts or pallet jacks in warehouses or factories.  5, 29 30   For cobots, sensor malfunctions or unexpected work environment conditions can lead to unpredictable robot behavior and thus to human robot collisions.  5 7  Self driving cars are another example of AI enabled robots.  In addition, the ergonomics of control interfaces and human machine interactions may give rise to hazards. AI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions,  17  as well as information privacy measures.  Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues. Proposed best practices for employer sponsored worker monitoring programs include using only validated sensor technologies  ensuring voluntary worker participation  ceasing data collection outside the workplace  disclosing all data uses  and ensuring secure data storage. For industrial cobots equipped with AI enabled sensors, the International Organization for Standardization  ISO  recommended   a  safety related monitored stopping controls   b  human hand guiding of the cobot   c  speed and separation monitoring controls  and  d  power and force limitations.  Networked AI enabled cobots may share safety improvements with each other. Human oversight is another general hazard control for AI.  12 13  Both applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management.  As with all hazards, risk identification is most effective and least costly when done in the design phase. Workplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate and does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs.  Proxies for skill content include educational requirements and classifications of routine versus non routine, and cognitive versus physical jobs.  However, these may still not be specific enough to distinguish specific occupations that have distinct impacts from AI.  The United States Department of Labor s Occupational Information Network is an example of a database with a detailed taxonomy of skills.  Additionally, data are often reported on a national level, while there is much geographical variation, especially between urban and rural areas. As of 2019, ISO was developing a standard on the use of metrics and dashboards, information displays presenting company metrics for managers, in workplaces.  The standard is planned to include guidelines for both gathering data and displaying it in a viewable and useful manner.  11  In the European Union, the General Data Protection Regulation, while oriented towards consumer data, is also relevant for workplace data collection.  Data subjects, including workers, have  the right not to be subject to a decision based solely on automated processing .  Other relevant EU directives include the Machinery Directive  2006 42 EC , the Radio Equipment Directive  2014 53 EU , and the General Product Safety Directive  2001 95 EC .  10, 12 13  
The Zeuthen strategy in cognitive science is a negotiation strategy used by some artificial agents. Its purpose is to measure the willingness to risk conflict. An agent will be more willing to risk conflict if it does not have much to lose in case that the negotiation fails. In contrast, an agent is less willing to risk conflict when it has more to lose. The value of a deal is expressed in its utility. An agent has much to lose when the difference between the utility of its current proposal and the conflict deal is high. When both agents use the monotonic concession protocol, the Zeuthen strategy leads them to agree upon a deal in the negotiation set. This set consists of all conflict free deals, which are individually rational and Pareto optimal, and the conflict deal, which maximizes the Nash product. The strategy was introduced in 1930 by the Danish economist Frederik Zeuthen. The Zeuthen strategy answers three open questions that arise when using the monotonic concession protocol, namely  The answer to the first question is that any agent should start with its most preferred deal, because that deal has the highest utility for that agent. The second answer is that the agent with the smallest value of Risk i,t  concedes, because the agent with the lowest utility for the conflict deal profits most from avoiding conflict. To the third question, the Zeuthen strategy suggests that the conceding agent should concede just enough raise its value of Risk i,t  just above that of the other agent. This prevents the conceding agent to have to concede again in the next round.  Risk i,t  is a measurement of agent i s willingness to risk conflict. The risk function formalizes the notion that an agent s willingness to risk conflict is the ratio of the utility that agent would lose by accepting the other agent s proposal to the utility that agent would lose by causing a conflict. Agent i is said to be using a rational negotiation strategy if at any step t   1 that agent i sticks to his last proposal, Risk i,t    Risk j,t . If agent i makes a sufficient concession in the next step, then, assuming that agent j is using a rational negotiation strategy, if agent j does not concede in the next step, he must do so in the step after that. The set of all sufficient concessions of agent i at step t is denoted SC i, t . is the minimal sufficient concession of agent A in step t. Agent A begins the negotiation by proposing and will make the minimal sufficient concession in step t   1 if and only if Risk A,t    Risk B,t . Theorem If both agents are using Zeuthen strategies, then they will agree on that is, the deal which maximizes the Nash product. Proof Let  A     A,t . Let  B     B,t . According to the Zeuthen strategy, agent A will concede at step     t      if and only if That is, if and only if Thus, Agent A will concede if and only if         A         does not yield the larger product of utilities. Therefore, the Zeuthen strategy guarantees a final agreement that maximizes the Nash Product. 